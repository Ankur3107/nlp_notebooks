{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NLP-Notebooks Collection","title":"NLP-Notebooks Collection"},{"location":"#nlp-notebooks-collection","text":"","title":"NLP-Notebooks Collection"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/","text":"BERT Fine-Tuning Tutorial with PyTorch By Ankur Singh Introduction History 2018 was a breakthrough year in NLP. Transfer learning, particularly models like Allen AI's ELMO, OpenAI's Open-GPT, and Google's BERT allowed researchers to smash multiple benchmarks with minimal task-specific fine-tuning and provided the rest of the NLP community with pretrained models that could easily (with less data and less compute time) be fine-tuned and implemented to produce state of the art results. Unfortunately, for many starting out in NLP and even for some experienced practicioners, the theory and practical application of these powerful models is still not well understood. What is BERT? BERT (Bidirectional Encoder Representations from Transformers), released in late 2018, is the model we will use in this tutorial to provide readers with a better understanding of and practical guidance for using transfer learning models in NLP. BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions. This post will explain how you can modify and fine-tune BERT to create a powerful NLP model that quickly gives you state of the art results. Advantages of Fine-Tuning In this tutorial, we will use BERT to train a text classifier. Specifically, we will take the pre-trained BERT model, add an untrained layer of neurons on the end, and train the new model for our classification task. Why do this rather than train a train a specific deep learning model (a CNN, BiLSTM, etc.) that is well suited for the specific NLP task you need? Quicker Development First, the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model - it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our classification task. In fact, the authors recommend only 2-4 epochs of training for fine-tuning BERT on a specific NLP task (compared to the hundreds of GPU hours needed to train the original BERT model or a LSTM from scratch!). Less Data In addition and perhaps just as important, because of the pre-trained weights this method allows us to fine-tune our task on a much smaller dataset than would be required in a model that is built from scratch. A major drawback of NLP models built from scratch is that we often need a prohibitively large dataset in order to train our network to reasonable accuracy, meaning a lot of time and energy had to be put into dataset creation. By fine-tuning BERT, we are now able to get away with training a model to good performance on a much smaller amount of training data. Better Results Finally, this simple fine-tuning procedure (typically adding one fully-connected layer on top of BERT and training for a few epochs) was shown to achieve state of the art results with minimal task-specific adjustments for a wide variety of tasks: classification, language inference, semantic similarity, question answering, etc. Rather than implementing custom and sometimes-obscure architetures shown to work well on a specific task, simply fine-tuning BERT is shown to be a better (or at least equal) alternative. A Shift in NLP This shift to transfer learning parallels the same shift that took place in computer vision a few years ago. Creating a good deep learning network for computer vision tasks can take millions of parameters and be very expensive to train. Researchers discovered that deep networks learn hierarchical feature representations (simple features like edges at the lowest layers with gradually more complex features at higher layers). Rather than training a new network from scratch each time, the lower layers of a trained network with generalized image features could be copied and transfered for use in another network with a different task. It soon became common practice to download a pre-trained deep network and quickly retrain it for the new task or add additional layers on top - vastly preferable to the expensive process of training a network from scratch. For many, the introduction of deep pre-trained language models in 2018 (ELMO, BERT, ULMFIT, Open-GPT, etc.) signals the same shift to transfer learning in NLP that computer vision saw. Let's get started! 1. Setup 1.1. Using Colab GPU for Training Google Colab offers free GPUs and TPUs! Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time. A GPU can be added by going to the menu and selecting: Edit \ud83e\udc52 Notebook Settings \ud83e\udc52 Hardware accelerator \ud83e\udc52 (GPU) Then run the following cell to confirm that the GPU is detected. import tensorflow as tf # Get the GPU device name. device_name = tf.test.gpu_device_name() # The device name should look like the following: if device_name == '/device:GPU:0': print('Found GPU at: {}'.format(device_name)) else: raise SystemError('GPU device not found') The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info . Found GPU at: /device:GPU:0 In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. import torch # If there's a GPU available... if torch.cuda.is_available(): # Tell PyTorch to use the GPU. device = torch.device(\"cuda\") print('There are %d GPU(s) available.' % torch.cuda.device_count()) print('We will use the GPU:', torch.cuda.get_device_name(0)) # If not... else: print('No GPU available, using the CPU instead.') device = torch.device(\"cpu\") There are 1 GPU(s) available. We will use the GPU: Tesla P100-PCIE-16GB 1.2. Installing the Hugging Face Library Next, let's install the transformers package from Hugging Face which will give us a pytorch interface for working with BERT. (This library contains interfaces for other pretrained language models like OpenAI's GPT and GPT-2.) We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!). At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task. For example, in this tutorial we will use BertForSequenceClassification . The library also includes task-specific classes for token classification, question answering, next sentence prediciton, etc. Using these pre-built classes simplifies the process of modifying BERT for your purposes. !pip install transformers Collecting transformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/d1/08/4a6768ca1a7a4fa37e5ee08077c5d02b8d83876bd36caa5fc24d98992ac2/transformers-2.2.2-py3-none-any.whl (387kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 389kB 3.4MB/s \u001b[?25hCollecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 860kB 44.9MB/s \u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0) Collecting regex \u001b[?25l Downloading https://files.pythonhosted.org/packages/91/0e/84041e5245d0bfc0f6c6431b36f8d7e1668ee0ce5f8184e0e1f9ee1082b6/regex-2019.12.19-cp36-cp36m-manylinux2010_x86_64.whl (689kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 696kB 43.6MB/s \u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36) Collecting sentencepiece \u001b[?25l Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0MB 34.0MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1) Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28) Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3) Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8) Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1) Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4) Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36) Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2) Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=224727a36ca806911eba978b9150331daecc6deae8bddf8f171029d137b31c2c Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94 Successfully built sacremoses Installing collected packages: sacremoses, regex, sentencepiece, transformers Successfully installed regex-2019.12.19 sacremoses-0.0.35 sentencepiece-0.1.85 transformers-2.2.2 The code in this notebook is actually a simplified version of the run_glue.py example script from huggingface. run_glue.py is a helpful utility which allows you to pick which GLUE benchmark task you want to run on, and which pre-trained model you want to use (you can see the list of possible models here ). It also supports using either the CPU, a single GPU, or multiple GPUs. It even supports using 16-bit precision if you want further speed up. Unfortunately, all of this configurability comes at the cost of readability . In this Notebook, we've simplified the code greatly and added plenty of comments to make it clear what's going on. 2. Loading CoLA Dataset We'll use The Corpus of Linguistic Acceptability (CoLA) dataset for single sentence classification. It's a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the \"GLUE Benchmark\" on which models like BERT are competing. 2.1. Download & Extract We'll use the wget package to download the dataset to the Colab instance's file system. !pip install wget Collecting wget Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip Building wheels for collected packages: wget Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=a6fdb3b6384387a047c5a76d75d01ecde89a1735ca0a70202174f2064688424c Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f Successfully built wget Installing collected packages: wget Successfully installed wget-3.2 The dataset is hosted on GitHub in this repo: https://nyu-mll.github.io/CoLA/ import wget import os print('Downloading dataset...') # The URL for the dataset zip file. url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip' # Download the file (if we haven't already) if not os.path.exists('./cola_public_1.1.zip'): wget.download(url, './cola_public_1.1.zip') Downloading dataset... Unzip the dataset to the file system. You can browse the file system of the Colab instance in the sidebar on the left. # Unzip the dataset (if we haven't already) if not os.path.exists('./cola_public/'): !unzip cola_public_1.1.zip Archive: cola_public_1.1.zip creating: cola_public/ inflating: cola_public/README creating: cola_public/tokenized/ inflating: cola_public/tokenized/in_domain_dev.tsv inflating: cola_public/tokenized/in_domain_train.tsv inflating: cola_public/tokenized/out_of_domain_dev.tsv creating: cola_public/raw/ inflating: cola_public/raw/in_domain_dev.tsv inflating: cola_public/raw/in_domain_train.tsv inflating: cola_public/raw/out_of_domain_dev.tsv 2.2. Parse We can see from the file names that both tokenized and raw versions of the data are available. We can't use the pre-tokenized version because, in order to apply the pre-trained BERT, we must use the tokenizer provided by the model. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words. We'll use pandas to parse the \"in-domain\" training set and look at a few of its properties and data points. import pandas as pd # Load the dataset into a pandas dataframe. df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence']) # Report the number of sentences. print('Number of training sentences: {:,}\\n'.format(df.shape[0])) # Display 10 random rows from the data. df.sample(10) Number of training sentences: 8,551 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sentence_source label label_notes sentence 1942 r-67 0 * Every student who ever goes to Europe ever has... 7611 sks13 1 NaN Is there anything to do today? 7685 sks13 0 * John convinced the rice to be cooked by Bill. 3923 ks08 1 NaN The birds devour the worm. 722 bc01 0 * Home was gone by John. 130 cj99 0 * The more John eats, the tighter keep your mout... 5387 b_73 0 * She has problem enough as it is. 4566 ks08 0 * The roof is leaked. 4480 ks08 0 * John did not leaving here. 5548 b_73 1 NaN Mary has more than two friends. The two properties we actually care about are the the sentence and its label , which is referred to as the \"acceptibility judgment\" (0=unacceptable, 1=acceptable). Here are five sentences which are labeled as not grammatically acceptible. Note how much more difficult this task is than something like sentiment analysis! df.loc[df.label == 0].sample(5)[['sentence', 'label']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sentence label 1651 Here's a knife for you to say was on the table. 0 2904 I detached the handle and the box. 0 7409 They are readable of the paper. 0 3116 Cynthia lunched peaches. 0 1155 I insist on seeing through all the students wh... 0 Let's extract the sentences and labels of our training set as numpy ndarrays. # Get the lists of sentences and their labels. sentences = df.sentence.values labels = df.label.values 3. Tokenization & Input Formatting In this section, we'll transform our dataset into the format that BERT can be trained on. 3.1. BERT Tokenizer To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here. from transformers import BertTokenizer # Load the BERT tokenizer. print('Loading BERT tokenizer...') tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) Loading BERT tokenizer... HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid\u2026 Let's apply the tokenizer to one sentence just to see the output. # Print the original sentence. print(' Original: ', sentences[0]) # Print the sentence split into tokens. print('Tokenized: ', tokenizer.tokenize(sentences[0])) # Print the sentence mapped to token ids. print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) Original: Our friends won't buy this analysis, let alone the next one we propose. Tokenized: ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.'] Token IDs: [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012] When we actually convert all of our sentences, we'll use the tokenize.encode function to handle both steps, rather than calling tokenize and convert_tokens_to_ids separately. Before we can do that, though, we need to talk about some of BERT's formatting requirements. 3.2. Required Formatting The above code left out a few required formatting steps that we'll look at here. Side Note: The input format to BERT seems \"over-specified\" to me... We are required to give it a number of pieces of information which seem redundant, or like they could easily be inferred from the data without us explicity providing it. But it is what it is, and I suspect it will make more sense once I have a deeper understanding of the BERT internals. We are required to: 1. Add special tokens to the start and end of each sentence. 2. Pad & truncate all sentences to a single constant length. 3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\". Special Tokens [SEP] At the end of every sentence, we need to append the special [SEP] token. This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). I am not certain yet why the token is still required when we have only single-sentence input, but it is! [CLS] For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence. This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!). On the output of the final (12th) transformer, only the first embedding (corresponding to the [CLS] token) is used by the classifier . \"The first token of every sequence is always a special classification token ( [CLS] ). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\" (from the BERT paper ) I'm not sure why the authors took this strategy instead of some kind of pooling of all the final vectors, but I'm sure that if pooling were better they would have gone that route. Also, because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector. Sentence Length & Attention Mask The sentences in our dataset obviously have varying lengths, so how does BERT handle this? BERT has two constraints: 1. All sentences must be padded or truncated to a single, fixed length. 2. The maximum sentence length is 512 tokens. Padding is done with a special [PAD] token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens. The \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?! Again, I don't currently know why). I've experimented with running this notebook with two different values of MAX_LEN, and it impacted both the training speed and the test set accuracy. With a Tesla K80 and: MAX_LEN = 128 --> Training epochs take ~5:28 each, score is 0.535 MAX_LEN = 64 --> Training epochs take ~2:57 each, score is 0.566 These results suggest to me that the padding tokens aren't simply skipped over--that they are in fact fed through the model and incorporated in the results (thereby impacting both model speed and accuracy). I'll have to dig into the architecture more to understand this. 3.2. Sentences to IDs The tokenizer.encode function combines multiple steps for us: 1. Split the sentence into tokens. 2. Add the special [CLS] and [SEP] tokens. 3. Map the tokens to their IDs. Oddly, this function can perform truncating for us, but doesn't handle padding. # Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = [] # For every sentence... for sent in sentences: # `encode` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. encoded_sent = tokenizer.encode( sent, # Sentence to encode. add_special_tokens = True, # Add '[CLS]' and '[SEP]' # This function also supports truncation and conversion # to pytorch tensors, but we need to do padding, so we # can't use these features :( . #max_length = 128, # Truncate all sentences. #return_tensors = 'pt', # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_sent) # Print sentence 0, now as a list of IDs. print('Original: ', sentences[0]) print('Token IDs:', input_ids[0]) Original: Our friends won't buy this analysis, let alone the next one we propose. Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102] 3.3. Padding & Truncating Pad and truncate our sequences so that they all have the same length, MAX_LEN . First, what's the maximum sentence length in our dataset? print('Max sentence length: ', max([len(sen) for sen in input_ids])) Max sentence length: 47 Given that, let's choose MAX_LEN = 64 and apply the padding. # We'll borrow the `pad_sequences` utility function to do this. from keras.preprocessing.sequence import pad_sequences # Set the maximum sequence length. # I've chosen 64 somewhat arbitrarily. It's slightly larger than the # maximum training sentence length of 47... MAX_LEN = 64 print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN) print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id)) # Pad our input tokens with value 0. # \"post\" indicates that we want to pad and truncate at the end of the sequence, # as opposed to the beginning. input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\") print('\\nDone.') Padding/truncating all sentences to 64 values... Padding token: \"[PAD]\", ID: 0 Done. Using TensorFlow backend. 3.4. Attention Masks The attention mask simply makes it explicit which tokens are actual words versus which are padding. The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it's padding, and otherwise it's a real token. # Create attention masks attention_masks = [] # For each sentence... for sent in input_ids: # Create the attention mask. # - If a token ID is 0, then it's padding, set the mask to 0. # - If a token ID is > 0, then it's a real token, set the mask to 1. att_mask = [int(token_id > 0) for token_id in sent] # Store the attention mask for this sentence. attention_masks.append(att_mask) 3.5. Training & Validation Split Divide up our training set to use 90% for training and 10% for validation. # Use train_test_split to split our data into train and validation sets for # training from sklearn.model_selection import train_test_split # Use 90% for training and 10% for validation. train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1) # Do the same for the masks. train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1) 3.6. Converting to PyTorch Data Types Our model expects PyTorch tensors rather than numpy.ndarrays, so convert all of our dataset variables. # Convert all inputs and labels into torch tensors, the required datatype # for our model. train_inputs = torch.tensor(train_inputs) validation_inputs = torch.tensor(validation_inputs) train_labels = torch.tensor(train_labels) validation_labels = torch.tensor(validation_labels) train_masks = torch.tensor(train_masks) validation_masks = torch.tensor(validation_masks) We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory. from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler # The DataLoader needs to know our batch size for training, so we specify it # here. # For fine-tuning BERT on a specific task, the authors recommend a batch size of # 16 or 32. batch_size = 32 # Create the DataLoader for our training set. train_data = TensorDataset(train_inputs, train_masks, train_labels) train_sampler = RandomSampler(train_data) train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # Create the DataLoader for our validation set. validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels) validation_sampler = SequentialSampler(validation_data) validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size) 4. Train Our Classification Model Now that our input data is properly formatted, it's time to fine tune the BERT model. 4.1. BertForSequenceClassification For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task. Here is the current list of classes provided for fine-tuning: * BertModel * BertForPreTraining * BertForMaskedLM * BertForNextSentencePrediction * BertForSequenceClassification - The one we'll use. * BertForTokenClassification * BertForQuestionAnswering The documentation for these can be found under here . We'll be using BertForSequenceClassification . This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\"). The documentation for from_pretrained can be found here , with the additional parameters defined here . from transformers import BertForSequenceClassification, AdamW, BertConfig # Load BertForSequenceClassification, the pretrained BERT model with a single # linear classification layer on top. model = BertForSequenceClassification.from_pretrained( \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab. num_labels = 2, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) # Tell pytorch to run this model on the GPU. model.cuda() HBox(children=(IntProgress(value=0, description='Downloading', max=313, style=ProgressStyle(description_width=\u2026 HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_\u2026 BertForSequenceClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=2, bias=True) ) Just for curiosity's sake, we can browse all of the model's parameters by name here. In the below cell, I've printed out the names and dimensions of the weights for: The embedding layer. The first of the twelve transformers. The output layer. # Get all of the model's parameters as a list of tuples. params = list(model.named_parameters()) print('The BERT model has {:} different named parameters.\\n'.format(len(params))) print('==== Embedding Layer ====\\n') for p in params[0:5]: print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size())))) print('\\n==== First Transformer ====\\n') for p in params[5:21]: print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size())))) print('\\n==== Output Layer ====\\n') for p in params[-4:]: print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size())))) The BERT model has 201 different named parameters. ==== Embedding Layer ==== bert.embeddings.word_embeddings.weight (30522, 768) bert.embeddings.position_embeddings.weight (512, 768) bert.embeddings.token_type_embeddings.weight (2, 768) bert.embeddings.LayerNorm.weight (768,) bert.embeddings.LayerNorm.bias (768,) ==== First Transformer ==== bert.encoder.layer.0.attention.self.query.weight (768, 768) bert.encoder.layer.0.attention.self.query.bias (768,) bert.encoder.layer.0.attention.self.key.weight (768, 768) bert.encoder.layer.0.attention.self.key.bias (768,) bert.encoder.layer.0.attention.self.value.weight (768, 768) bert.encoder.layer.0.attention.self.value.bias (768,) bert.encoder.layer.0.attention.output.dense.weight (768, 768) bert.encoder.layer.0.attention.output.dense.bias (768,) bert.encoder.layer.0.attention.output.LayerNorm.weight (768,) bert.encoder.layer.0.attention.output.LayerNorm.bias (768,) bert.encoder.layer.0.intermediate.dense.weight (3072, 768) bert.encoder.layer.0.intermediate.dense.bias (3072,) bert.encoder.layer.0.output.dense.weight (768, 3072) bert.encoder.layer.0.output.dense.bias (768,) bert.encoder.layer.0.output.LayerNorm.weight (768,) bert.encoder.layer.0.output.LayerNorm.bias (768,) ==== Output Layer ==== bert.pooler.dense.weight (768, 768) bert.pooler.dense.bias (768,) classifier.weight (2, 768) classifier.bias (2,) 4.2. Optimizer & Learning Rate Scheduler Now that we have our model loaded we need to grab the training hyperparameters from within the stored model. For the purposes of fine-tuning, the authors recommend choosing from the following values: - Batch size: 16, 32 (We chose 32 when creating our DataLoaders). - Learning rate (Adam): 5e-5, 3e-5, 2e-5 (We'll use 2e-5). - Number of epochs: 2, 3, 4 (We'll use 4). The epsilon parameter eps = 1e-8 is \"a very small number to prevent any division by zero in the implementation\" (from here ). You can find the creation of the AdamW optimizer in run_glue.py here . # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW(model.parameters(), lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) from transformers import get_linear_schedule_with_warmup # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Total number of training steps is number of batches * number of epochs. total_steps = len(train_dataloader) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, # Default value in run_glue.py num_training_steps = total_steps) 4.3. Training Loop Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. At each pass we need to: Training loop: - Unpack our data inputs and labels - Load data onto the GPU for acceleration - Clear out the gradients calculated in the previous pass. - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out. - Forward pass (feed input data through the network) - Backward pass (backpropagation) - Tell the network to update parameters with optimizer.step() - Track variables for monitoring progress Evalution loop: - Unpack our data inputs and labels - Load data onto the GPU for acceleration - Forward pass (feed input data through the network) - Compute loss on our validation data and track variables for monitoring progress So please read carefully through the comments to get an understanding of what's happening. If you're unfamiliar with pytorch a quick look at some of their beginner tutorials will help show you that training loops really involve only a few simple steps; the rest is usually just decoration and logging. Define a helper function for calculating accuracy. import numpy as np # Function to calculate the accuracy of our predictions vs labels def flat_accuracy(preds, labels): pred_flat = np.argmax(preds, axis=1).flatten() labels_flat = labels.flatten() return np.sum(pred_flat == labels_flat) / len(labels_flat) Helper function for formatting elapsed times. import time import datetime def format_time(elapsed): ''' Takes a time in seconds and returns a string hh:mm:ss ''' # Round to the nearest second. elapsed_rounded = int(round((elapsed))) # Format as hh:mm:ss return str(datetime.timedelta(seconds=elapsed_rounded)) We're ready to kick off the training! import random # This training code is based on the `run_glue.py` script here: # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128 # Set the seed value all over the place to make this reproducible. seed_val = 42 random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val) # Store the average loss after each epoch so we can plot them. loss_values = [] # For each epoch... for epoch_i in range(0, epochs): # ======================================== # Training # ======================================== # Perform one full pass over the training set. print(\"\") print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs)) print('Training...') # Measure how long the training epoch takes. t0 = time.time() # Reset the total loss for this epoch. total_loss = 0 # Put the model into training mode. Don't be mislead--the call to # `train` just changes the *mode*, it doesn't *perform* the training. # `dropout` and `batchnorm` layers behave differently during training # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch) model.train() # For each batch of training data... for step, batch in enumerate(train_dataloader): # Progress update every 40 batches. if step % 40 == 0 and not step == 0: # Calculate elapsed time in minutes. elapsed = format_time(time.time() - t0) # Report progress. print(' Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed)) # Unpack this training batch from our dataloader. # # As we unpack the batch, we'll also copy each tensor to the GPU using the # `to` method. # # `batch` contains three pytorch tensors: # [0]: input ids # [1]: attention masks # [2]: labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) # Always clear any previously calculated gradients before performing a # backward pass. PyTorch doesn't do this automatically because # accumulating the gradients is \"convenient while training RNNs\". # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch) model.zero_grad() # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple. loss = outputs[0] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss.item() # Perform a backward pass to calculate the gradients. loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over the training data. avg_train_loss = total_loss / len(train_dataloader) # Store the loss value for plotting the learning curve. loss_values.append(avg_train_loss) print(\"\") print(\" Average training loss: {0:.2f}\".format(avg_train_loss)) print(\" Training epcoh took: {:}\".format(format_time(time.time() - t0))) # ======================================== # Validation # ======================================== # After the completion of each training epoch, measure our performance on # our validation set. print(\"\") print(\"Running Validation...\") t0 = time.time() # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model.eval() # Tracking variables eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 # Evaluate data for one epoch for batch in validation_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch.no_grad(): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # Get the \"logits\" output by the model. The \"logits\" are the output # values prior to applying an activation function like the softmax. logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to('cpu').numpy() # Calculate the accuracy for this batch of test sentences. tmp_eval_accuracy = flat_accuracy(logits, label_ids) # Accumulate the total accuracy. eval_accuracy += tmp_eval_accuracy # Track the number of batches nb_eval_steps += 1 # Report the final accuracy for this validation run. print(\" Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps)) print(\" Validation took: {:}\".format(format_time(time.time() - t0))) print(\"\") print(\"Training complete!\") ======== Epoch 1 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:11. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:31. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.50 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.79 Validation took: 0:00:02 ======== Epoch 2 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:11. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:32. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.32 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.82 Validation took: 0:00:02 ======== Epoch 3 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:11. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:32. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.20 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.82 Validation took: 0:00:02 ======== Epoch 4 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:10. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:31. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.14 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.82 Validation took: 0:00:02 Training complete! Let's take a look at our training loss over all batches: import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns # Use plot styling from seaborn. sns.set(style='darkgrid') # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (12,6) # Plot the learning curve. plt.plot(loss_values, 'b-o') # Label the plot. plt.title(\"Training loss\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show() 5. Performance On Test Set Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using Matthew's correlation coefficient because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task. 5.1. Data Preparation We'll need to apply all of the same steps that we did for the training data to prepare our test data set. import pandas as pd # Load the dataset into a pandas dataframe. df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence']) # Report the number of sentences. print('Number of test sentences: {:,}\\n'.format(df.shape[0])) # Create sentence and label lists sentences = df.sentence.values labels = df.label.values # Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = [] # For every sentence... for sent in sentences: # `encode` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. encoded_sent = tokenizer.encode( sent, # Sentence to encode. add_special_tokens = True, # Add '[CLS]' and '[SEP]' ) input_ids.append(encoded_sent) # Pad our input tokens input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") # Create attention masks attention_masks = [] # Create a mask of 1s for each token followed by 0s for padding for seq in input_ids: seq_mask = [float(i>0) for i in seq] attention_masks.append(seq_mask) # Convert to tensors. prediction_inputs = torch.tensor(input_ids) prediction_masks = torch.tensor(attention_masks) prediction_labels = torch.tensor(labels) # Set the batch size. batch_size = 32 # Create the DataLoader. prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels) prediction_sampler = SequentialSampler(prediction_data) prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) Number of test sentences: 516 5.2. Evaluate on Test Set With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set. # Prediction on test set print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs))) # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in prediction_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch # Telling the model not to compute or store gradients, saving memory and # speeding up prediction with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to('cpu').numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(' DONE.') Predicting labels for 516 test sentences... DONE. Accuracy on the CoLA benchmark is measured using the \" Matthews correlation coefficient \" (MCC). We use MCC here because the classes are imbalanced: print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0))) Positive samples: 354 of 516 (68.60%) from sklearn.metrics import matthews_corrcoef matthews_set = [] # Evaluate each test batch using Matthew's correlation coefficient print('Calculating Matthews Corr. Coef. for each batch...') # For each input batch... for i in range(len(true_labels)): # The predictions for this batch are a 2-column ndarray (one column for \"0\" # and one column for \"1\"). Pick the label with the highest value and turn this # in to a list of 0s and 1s. pred_labels_i = np.argmax(predictions[i], axis=1).flatten() # Calculate and store the coef for this batch. matthews = matthews_corrcoef(true_labels[i], pred_labels_i) matthews_set.append(matthews) Calculating Matthews Corr. Coef. for each batch... /usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp) The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it. matthews_set [0.049286405809014416, -0.21684543705982773, 0.4040950971038548, 0.41179801403140964, 0.25365601296401685, 0.6777932975034471, 0.4879500364742666, 0.0, 0.8320502943378436, 0.8246211251235321, 0.9229582069908973, 0.647150228929434, 0.8150678894028793, 0.7141684885491869, 0.3268228676411533, 0.5844155844155844, 0.0] # Combine the predictions for each batch into a single list of 0s and 1s. flat_predictions = [item for sublist in predictions for item in sublist] flat_predictions = np.argmax(flat_predictions, axis=1).flatten() # Combine the correct labels for each batch into a single list. flat_true_labels = [item for sublist in true_labels for item in sublist] # Calculate the MCC mcc = matthews_corrcoef(flat_true_labels, flat_predictions) print('MCC: %.3f' % mcc) MCC: 0.529 Cool! In about half an hour and without doing any hyperparameter tuning (adjusting the learning rate, epochs, batch size, ADAM properties, etc.) we are able to get a good score. I should also mention we didn't train on the entire training dataset, but set aside a portion of it as our validation set for legibililty of code. The library documents the expected accuracy for this benchmark here . You can also look at the official leaderboard here . Note that (due to the small dataset size?) the accuracy can vary significantly with different random seeds. Conclusion This post demonstrates that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in. Appendix A1. Saving & Loading Fine-Tuned Model This first cell (taken from run_glue.py here ) writes the model and tokenizer out to disk. import os # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained() output_dir = './model_save/' # Create output directory if needed if not os.path.exists(output_dir): os.makedirs(output_dir) print(\"Saving model to %s\" % output_dir) # Save a trained model, configuration and tokenizer using `save_pretrained()`. # They can then be reloaded using `from_pretrained()` model_to_save = model.module if hasattr(model, 'module') else model # Take care of distributed/parallel training model_to_save.save_pretrained(output_dir) tokenizer.save_pretrained(output_dir) # Good practice: save your training arguments together with the trained model # torch.save(args, os.path.join(output_dir, 'training_args.bin')) Saving model to ./model_save/ ('./model_save/vocab.txt', './model_save/special_tokens_map.json', './model_save/added_tokens.json') Let's check out the file sizes, out of curiosity. !ls -l --block-size=K ./model_save/ total 427964K -rw-r--r-- 1 root root 1K Dec 19 17:33 added_tokens.json -rw-r--r-- 1 root root 1K Dec 19 17:33 config.json -rw-r--r-- 1 root root 427719K Dec 19 17:33 pytorch_model.bin -rw-r--r-- 1 root root 1K Dec 19 17:33 special_tokens_map.json -rw-r--r-- 1 root root 1K Dec 19 17:33 tokenizer_config.json -rw-r--r-- 1 root root 227K Dec 19 17:33 vocab.txt The largest file is the model weights, at around 418 megabytes. !ls -l --block-size=M ./model_save/pytorch_model.bin -rw-r--r-- 1 root root 418M Dec 19 17:33 ./model_save/pytorch_model.bin To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive. # Mount Google Drive to this Notebook instance. from google.colab import drive drive.mount('/content/drive') # Copy the model files to a directory in your Google Drive. !cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\" The following functions will load the model back from disk. # Load a trained model and vocabulary that you have fine-tuned model = model_class.from_pretrained(output_dir) tokenizer = tokenizer_class.from_pretrained(output_dir) # Copy the model to the GPU. model.to(device) A.2. Weight Decay The huggingface example includes the following code block for enabling weight decay, but the default decay rate is \"0.0\", so I moved this to the appendix. This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99. # This code is taken from: # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102 # Don't apply weight decay to any parameters whose names include these tokens. # (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms) no_decay = ['bias', 'LayerNorm.weight'] # Separate the `weight` parameters from the `bias` parameters. # - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. # - For the `bias` parameters, the 'weight_decay_rate' is 0.0. optimizer_grouped_parameters = [ # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'. {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.1}, # Filter for parameters which *do* include those. {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0} ] # Note - `optimizer_grouped_parameters` only includes the parameter values, not # the names.","title":"BERT Fine Tuning Sentence Classification v2"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#bert-fine-tuning-tutorial-with-pytorch","text":"By Ankur Singh","title":"BERT Fine-Tuning Tutorial with PyTorch"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#introduction","text":"","title":"Introduction"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#history","text":"2018 was a breakthrough year in NLP. Transfer learning, particularly models like Allen AI's ELMO, OpenAI's Open-GPT, and Google's BERT allowed researchers to smash multiple benchmarks with minimal task-specific fine-tuning and provided the rest of the NLP community with pretrained models that could easily (with less data and less compute time) be fine-tuned and implemented to produce state of the art results. Unfortunately, for many starting out in NLP and even for some experienced practicioners, the theory and practical application of these powerful models is still not well understood.","title":"History"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#what-is-bert","text":"BERT (Bidirectional Encoder Representations from Transformers), released in late 2018, is the model we will use in this tutorial to provide readers with a better understanding of and practical guidance for using transfer learning models in NLP. BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions. This post will explain how you can modify and fine-tune BERT to create a powerful NLP model that quickly gives you state of the art results.","title":"What is BERT?"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#advantages-of-fine-tuning","text":"In this tutorial, we will use BERT to train a text classifier. Specifically, we will take the pre-trained BERT model, add an untrained layer of neurons on the end, and train the new model for our classification task. Why do this rather than train a train a specific deep learning model (a CNN, BiLSTM, etc.) that is well suited for the specific NLP task you need? Quicker Development First, the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model - it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our classification task. In fact, the authors recommend only 2-4 epochs of training for fine-tuning BERT on a specific NLP task (compared to the hundreds of GPU hours needed to train the original BERT model or a LSTM from scratch!). Less Data In addition and perhaps just as important, because of the pre-trained weights this method allows us to fine-tune our task on a much smaller dataset than would be required in a model that is built from scratch. A major drawback of NLP models built from scratch is that we often need a prohibitively large dataset in order to train our network to reasonable accuracy, meaning a lot of time and energy had to be put into dataset creation. By fine-tuning BERT, we are now able to get away with training a model to good performance on a much smaller amount of training data. Better Results Finally, this simple fine-tuning procedure (typically adding one fully-connected layer on top of BERT and training for a few epochs) was shown to achieve state of the art results with minimal task-specific adjustments for a wide variety of tasks: classification, language inference, semantic similarity, question answering, etc. Rather than implementing custom and sometimes-obscure architetures shown to work well on a specific task, simply fine-tuning BERT is shown to be a better (or at least equal) alternative.","title":"Advantages of Fine-Tuning"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#a-shift-in-nlp","text":"This shift to transfer learning parallels the same shift that took place in computer vision a few years ago. Creating a good deep learning network for computer vision tasks can take millions of parameters and be very expensive to train. Researchers discovered that deep networks learn hierarchical feature representations (simple features like edges at the lowest layers with gradually more complex features at higher layers). Rather than training a new network from scratch each time, the lower layers of a trained network with generalized image features could be copied and transfered for use in another network with a different task. It soon became common practice to download a pre-trained deep network and quickly retrain it for the new task or add additional layers on top - vastly preferable to the expensive process of training a network from scratch. For many, the introduction of deep pre-trained language models in 2018 (ELMO, BERT, ULMFIT, Open-GPT, etc.) signals the same shift to transfer learning in NLP that computer vision saw. Let's get started!","title":"A Shift in NLP"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#1-setup","text":"","title":"1. Setup"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#11-using-colab-gpu-for-training","text":"Google Colab offers free GPUs and TPUs! Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time. A GPU can be added by going to the menu and selecting: Edit \ud83e\udc52 Notebook Settings \ud83e\udc52 Hardware accelerator \ud83e\udc52 (GPU) Then run the following cell to confirm that the GPU is detected. import tensorflow as tf # Get the GPU device name. device_name = tf.test.gpu_device_name() # The device name should look like the following: if device_name == '/device:GPU:0': print('Found GPU at: {}'.format(device_name)) else: raise SystemError('GPU device not found') The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info . Found GPU at: /device:GPU:0 In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. import torch # If there's a GPU available... if torch.cuda.is_available(): # Tell PyTorch to use the GPU. device = torch.device(\"cuda\") print('There are %d GPU(s) available.' % torch.cuda.device_count()) print('We will use the GPU:', torch.cuda.get_device_name(0)) # If not... else: print('No GPU available, using the CPU instead.') device = torch.device(\"cpu\") There are 1 GPU(s) available. We will use the GPU: Tesla P100-PCIE-16GB","title":"1.1. Using Colab GPU for Training"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#12-installing-the-hugging-face-library","text":"Next, let's install the transformers package from Hugging Face which will give us a pytorch interface for working with BERT. (This library contains interfaces for other pretrained language models like OpenAI's GPT and GPT-2.) We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!). At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task. For example, in this tutorial we will use BertForSequenceClassification . The library also includes task-specific classes for token classification, question answering, next sentence prediciton, etc. Using these pre-built classes simplifies the process of modifying BERT for your purposes. !pip install transformers Collecting transformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/d1/08/4a6768ca1a7a4fa37e5ee08077c5d02b8d83876bd36caa5fc24d98992ac2/transformers-2.2.2-py3-none-any.whl (387kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 389kB 3.4MB/s \u001b[?25hCollecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 860kB 44.9MB/s \u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0) Collecting regex \u001b[?25l Downloading https://files.pythonhosted.org/packages/91/0e/84041e5245d0bfc0f6c6431b36f8d7e1668ee0ce5f8184e0e1f9ee1082b6/regex-2019.12.19-cp36-cp36m-manylinux2010_x86_64.whl (689kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 696kB 43.6MB/s \u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36) Collecting sentencepiece \u001b[?25l Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0MB 34.0MB/s \u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1) Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28) Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3) Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8) Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1) Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4) Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36) Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2) Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=224727a36ca806911eba978b9150331daecc6deae8bddf8f171029d137b31c2c Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94 Successfully built sacremoses Installing collected packages: sacremoses, regex, sentencepiece, transformers Successfully installed regex-2019.12.19 sacremoses-0.0.35 sentencepiece-0.1.85 transformers-2.2.2 The code in this notebook is actually a simplified version of the run_glue.py example script from huggingface. run_glue.py is a helpful utility which allows you to pick which GLUE benchmark task you want to run on, and which pre-trained model you want to use (you can see the list of possible models here ). It also supports using either the CPU, a single GPU, or multiple GPUs. It even supports using 16-bit precision if you want further speed up. Unfortunately, all of this configurability comes at the cost of readability . In this Notebook, we've simplified the code greatly and added plenty of comments to make it clear what's going on.","title":"1.2. Installing the Hugging Face Library"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#2-loading-cola-dataset","text":"We'll use The Corpus of Linguistic Acceptability (CoLA) dataset for single sentence classification. It's a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the \"GLUE Benchmark\" on which models like BERT are competing.","title":"2. Loading CoLA Dataset"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#21-download-extract","text":"We'll use the wget package to download the dataset to the Colab instance's file system. !pip install wget Collecting wget Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip Building wheels for collected packages: wget Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=a6fdb3b6384387a047c5a76d75d01ecde89a1735ca0a70202174f2064688424c Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f Successfully built wget Installing collected packages: wget Successfully installed wget-3.2 The dataset is hosted on GitHub in this repo: https://nyu-mll.github.io/CoLA/ import wget import os print('Downloading dataset...') # The URL for the dataset zip file. url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip' # Download the file (if we haven't already) if not os.path.exists('./cola_public_1.1.zip'): wget.download(url, './cola_public_1.1.zip') Downloading dataset... Unzip the dataset to the file system. You can browse the file system of the Colab instance in the sidebar on the left. # Unzip the dataset (if we haven't already) if not os.path.exists('./cola_public/'): !unzip cola_public_1.1.zip Archive: cola_public_1.1.zip creating: cola_public/ inflating: cola_public/README creating: cola_public/tokenized/ inflating: cola_public/tokenized/in_domain_dev.tsv inflating: cola_public/tokenized/in_domain_train.tsv inflating: cola_public/tokenized/out_of_domain_dev.tsv creating: cola_public/raw/ inflating: cola_public/raw/in_domain_dev.tsv inflating: cola_public/raw/in_domain_train.tsv inflating: cola_public/raw/out_of_domain_dev.tsv","title":"2.1. Download &amp; Extract"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#22-parse","text":"We can see from the file names that both tokenized and raw versions of the data are available. We can't use the pre-tokenized version because, in order to apply the pre-trained BERT, we must use the tokenizer provided by the model. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words. We'll use pandas to parse the \"in-domain\" training set and look at a few of its properties and data points. import pandas as pd # Load the dataset into a pandas dataframe. df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence']) # Report the number of sentences. print('Number of training sentences: {:,}\\n'.format(df.shape[0])) # Display 10 random rows from the data. df.sample(10) Number of training sentences: 8,551 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sentence_source label label_notes sentence 1942 r-67 0 * Every student who ever goes to Europe ever has... 7611 sks13 1 NaN Is there anything to do today? 7685 sks13 0 * John convinced the rice to be cooked by Bill. 3923 ks08 1 NaN The birds devour the worm. 722 bc01 0 * Home was gone by John. 130 cj99 0 * The more John eats, the tighter keep your mout... 5387 b_73 0 * She has problem enough as it is. 4566 ks08 0 * The roof is leaked. 4480 ks08 0 * John did not leaving here. 5548 b_73 1 NaN Mary has more than two friends. The two properties we actually care about are the the sentence and its label , which is referred to as the \"acceptibility judgment\" (0=unacceptable, 1=acceptable). Here are five sentences which are labeled as not grammatically acceptible. Note how much more difficult this task is than something like sentiment analysis! df.loc[df.label == 0].sample(5)[['sentence', 'label']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sentence label 1651 Here's a knife for you to say was on the table. 0 2904 I detached the handle and the box. 0 7409 They are readable of the paper. 0 3116 Cynthia lunched peaches. 0 1155 I insist on seeing through all the students wh... 0 Let's extract the sentences and labels of our training set as numpy ndarrays. # Get the lists of sentences and their labels. sentences = df.sentence.values labels = df.label.values","title":"2.2. Parse"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#3-tokenization-input-formatting","text":"In this section, we'll transform our dataset into the format that BERT can be trained on.","title":"3. Tokenization &amp; Input Formatting"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#31-bert-tokenizer","text":"To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here. from transformers import BertTokenizer # Load the BERT tokenizer. print('Loading BERT tokenizer...') tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) Loading BERT tokenizer... HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid\u2026 Let's apply the tokenizer to one sentence just to see the output. # Print the original sentence. print(' Original: ', sentences[0]) # Print the sentence split into tokens. print('Tokenized: ', tokenizer.tokenize(sentences[0])) # Print the sentence mapped to token ids. print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) Original: Our friends won't buy this analysis, let alone the next one we propose. Tokenized: ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.'] Token IDs: [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012] When we actually convert all of our sentences, we'll use the tokenize.encode function to handle both steps, rather than calling tokenize and convert_tokens_to_ids separately. Before we can do that, though, we need to talk about some of BERT's formatting requirements.","title":"3.1. BERT Tokenizer"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#32-required-formatting","text":"The above code left out a few required formatting steps that we'll look at here. Side Note: The input format to BERT seems \"over-specified\" to me... We are required to give it a number of pieces of information which seem redundant, or like they could easily be inferred from the data without us explicity providing it. But it is what it is, and I suspect it will make more sense once I have a deeper understanding of the BERT internals. We are required to: 1. Add special tokens to the start and end of each sentence. 2. Pad & truncate all sentences to a single constant length. 3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".","title":"3.2. Required Formatting"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#special-tokens","text":"[SEP] At the end of every sentence, we need to append the special [SEP] token. This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). I am not certain yet why the token is still required when we have only single-sentence input, but it is! [CLS] For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence. This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!). On the output of the final (12th) transformer, only the first embedding (corresponding to the [CLS] token) is used by the classifier . \"The first token of every sequence is always a special classification token ( [CLS] ). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\" (from the BERT paper ) I'm not sure why the authors took this strategy instead of some kind of pooling of all the final vectors, but I'm sure that if pooling were better they would have gone that route. Also, because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector.","title":"Special Tokens"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#sentence-length-attention-mask","text":"The sentences in our dataset obviously have varying lengths, so how does BERT handle this? BERT has two constraints: 1. All sentences must be padded or truncated to a single, fixed length. 2. The maximum sentence length is 512 tokens. Padding is done with a special [PAD] token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens. The \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?! Again, I don't currently know why). I've experimented with running this notebook with two different values of MAX_LEN, and it impacted both the training speed and the test set accuracy. With a Tesla K80 and: MAX_LEN = 128 --> Training epochs take ~5:28 each, score is 0.535 MAX_LEN = 64 --> Training epochs take ~2:57 each, score is 0.566 These results suggest to me that the padding tokens aren't simply skipped over--that they are in fact fed through the model and incorporated in the results (thereby impacting both model speed and accuracy). I'll have to dig into the architecture more to understand this.","title":"Sentence Length &amp; Attention Mask"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#32-sentences-to-ids","text":"The tokenizer.encode function combines multiple steps for us: 1. Split the sentence into tokens. 2. Add the special [CLS] and [SEP] tokens. 3. Map the tokens to their IDs. Oddly, this function can perform truncating for us, but doesn't handle padding. # Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = [] # For every sentence... for sent in sentences: # `encode` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. encoded_sent = tokenizer.encode( sent, # Sentence to encode. add_special_tokens = True, # Add '[CLS]' and '[SEP]' # This function also supports truncation and conversion # to pytorch tensors, but we need to do padding, so we # can't use these features :( . #max_length = 128, # Truncate all sentences. #return_tensors = 'pt', # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_sent) # Print sentence 0, now as a list of IDs. print('Original: ', sentences[0]) print('Token IDs:', input_ids[0]) Original: Our friends won't buy this analysis, let alone the next one we propose. Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]","title":"3.2. Sentences to IDs"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#33-padding-truncating","text":"Pad and truncate our sequences so that they all have the same length, MAX_LEN . First, what's the maximum sentence length in our dataset? print('Max sentence length: ', max([len(sen) for sen in input_ids])) Max sentence length: 47 Given that, let's choose MAX_LEN = 64 and apply the padding. # We'll borrow the `pad_sequences` utility function to do this. from keras.preprocessing.sequence import pad_sequences # Set the maximum sequence length. # I've chosen 64 somewhat arbitrarily. It's slightly larger than the # maximum training sentence length of 47... MAX_LEN = 64 print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN) print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id)) # Pad our input tokens with value 0. # \"post\" indicates that we want to pad and truncate at the end of the sequence, # as opposed to the beginning. input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\") print('\\nDone.') Padding/truncating all sentences to 64 values... Padding token: \"[PAD]\", ID: 0 Done. Using TensorFlow backend.","title":"3.3. Padding &amp; Truncating"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#34-attention-masks","text":"The attention mask simply makes it explicit which tokens are actual words versus which are padding. The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it's padding, and otherwise it's a real token. # Create attention masks attention_masks = [] # For each sentence... for sent in input_ids: # Create the attention mask. # - If a token ID is 0, then it's padding, set the mask to 0. # - If a token ID is > 0, then it's a real token, set the mask to 1. att_mask = [int(token_id > 0) for token_id in sent] # Store the attention mask for this sentence. attention_masks.append(att_mask)","title":"3.4. Attention Masks"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#35-training-validation-split","text":"Divide up our training set to use 90% for training and 10% for validation. # Use train_test_split to split our data into train and validation sets for # training from sklearn.model_selection import train_test_split # Use 90% for training and 10% for validation. train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1) # Do the same for the masks. train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)","title":"3.5. Training &amp; Validation Split"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#36-converting-to-pytorch-data-types","text":"Our model expects PyTorch tensors rather than numpy.ndarrays, so convert all of our dataset variables. # Convert all inputs and labels into torch tensors, the required datatype # for our model. train_inputs = torch.tensor(train_inputs) validation_inputs = torch.tensor(validation_inputs) train_labels = torch.tensor(train_labels) validation_labels = torch.tensor(validation_labels) train_masks = torch.tensor(train_masks) validation_masks = torch.tensor(validation_masks) We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory. from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler # The DataLoader needs to know our batch size for training, so we specify it # here. # For fine-tuning BERT on a specific task, the authors recommend a batch size of # 16 or 32. batch_size = 32 # Create the DataLoader for our training set. train_data = TensorDataset(train_inputs, train_masks, train_labels) train_sampler = RandomSampler(train_data) train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # Create the DataLoader for our validation set. validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels) validation_sampler = SequentialSampler(validation_data) validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","title":"3.6. Converting to PyTorch Data Types"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#4-train-our-classification-model","text":"Now that our input data is properly formatted, it's time to fine tune the BERT model.","title":"4. Train Our Classification Model"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#41-bertforsequenceclassification","text":"For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task. Here is the current list of classes provided for fine-tuning: * BertModel * BertForPreTraining * BertForMaskedLM * BertForNextSentencePrediction * BertForSequenceClassification - The one we'll use. * BertForTokenClassification * BertForQuestionAnswering The documentation for these can be found under here . We'll be using BertForSequenceClassification . This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\"). The documentation for from_pretrained can be found here , with the additional parameters defined here . from transformers import BertForSequenceClassification, AdamW, BertConfig # Load BertForSequenceClassification, the pretrained BERT model with a single # linear classification layer on top. model = BertForSequenceClassification.from_pretrained( \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab. num_labels = 2, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) # Tell pytorch to run this model on the GPU. model.cuda() HBox(children=(IntProgress(value=0, description='Downloading', max=313, style=ProgressStyle(description_width=\u2026 HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_\u2026 BertForSequenceClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=2, bias=True) ) Just for curiosity's sake, we can browse all of the model's parameters by name here. In the below cell, I've printed out the names and dimensions of the weights for: The embedding layer. The first of the twelve transformers. The output layer. # Get all of the model's parameters as a list of tuples. params = list(model.named_parameters()) print('The BERT model has {:} different named parameters.\\n'.format(len(params))) print('==== Embedding Layer ====\\n') for p in params[0:5]: print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size())))) print('\\n==== First Transformer ====\\n') for p in params[5:21]: print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size())))) print('\\n==== Output Layer ====\\n') for p in params[-4:]: print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size())))) The BERT model has 201 different named parameters. ==== Embedding Layer ==== bert.embeddings.word_embeddings.weight (30522, 768) bert.embeddings.position_embeddings.weight (512, 768) bert.embeddings.token_type_embeddings.weight (2, 768) bert.embeddings.LayerNorm.weight (768,) bert.embeddings.LayerNorm.bias (768,) ==== First Transformer ==== bert.encoder.layer.0.attention.self.query.weight (768, 768) bert.encoder.layer.0.attention.self.query.bias (768,) bert.encoder.layer.0.attention.self.key.weight (768, 768) bert.encoder.layer.0.attention.self.key.bias (768,) bert.encoder.layer.0.attention.self.value.weight (768, 768) bert.encoder.layer.0.attention.self.value.bias (768,) bert.encoder.layer.0.attention.output.dense.weight (768, 768) bert.encoder.layer.0.attention.output.dense.bias (768,) bert.encoder.layer.0.attention.output.LayerNorm.weight (768,) bert.encoder.layer.0.attention.output.LayerNorm.bias (768,) bert.encoder.layer.0.intermediate.dense.weight (3072, 768) bert.encoder.layer.0.intermediate.dense.bias (3072,) bert.encoder.layer.0.output.dense.weight (768, 3072) bert.encoder.layer.0.output.dense.bias (768,) bert.encoder.layer.0.output.LayerNorm.weight (768,) bert.encoder.layer.0.output.LayerNorm.bias (768,) ==== Output Layer ==== bert.pooler.dense.weight (768, 768) bert.pooler.dense.bias (768,) classifier.weight (2, 768) classifier.bias (2,)","title":"4.1. BertForSequenceClassification"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#42-optimizer-learning-rate-scheduler","text":"Now that we have our model loaded we need to grab the training hyperparameters from within the stored model. For the purposes of fine-tuning, the authors recommend choosing from the following values: - Batch size: 16, 32 (We chose 32 when creating our DataLoaders). - Learning rate (Adam): 5e-5, 3e-5, 2e-5 (We'll use 2e-5). - Number of epochs: 2, 3, 4 (We'll use 4). The epsilon parameter eps = 1e-8 is \"a very small number to prevent any division by zero in the implementation\" (from here ). You can find the creation of the AdamW optimizer in run_glue.py here . # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW(model.parameters(), lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) from transformers import get_linear_schedule_with_warmup # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Total number of training steps is number of batches * number of epochs. total_steps = len(train_dataloader) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, # Default value in run_glue.py num_training_steps = total_steps)","title":"4.2. Optimizer &amp; Learning Rate Scheduler"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#43-training-loop","text":"Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. At each pass we need to: Training loop: - Unpack our data inputs and labels - Load data onto the GPU for acceleration - Clear out the gradients calculated in the previous pass. - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out. - Forward pass (feed input data through the network) - Backward pass (backpropagation) - Tell the network to update parameters with optimizer.step() - Track variables for monitoring progress Evalution loop: - Unpack our data inputs and labels - Load data onto the GPU for acceleration - Forward pass (feed input data through the network) - Compute loss on our validation data and track variables for monitoring progress So please read carefully through the comments to get an understanding of what's happening. If you're unfamiliar with pytorch a quick look at some of their beginner tutorials will help show you that training loops really involve only a few simple steps; the rest is usually just decoration and logging. Define a helper function for calculating accuracy. import numpy as np # Function to calculate the accuracy of our predictions vs labels def flat_accuracy(preds, labels): pred_flat = np.argmax(preds, axis=1).flatten() labels_flat = labels.flatten() return np.sum(pred_flat == labels_flat) / len(labels_flat) Helper function for formatting elapsed times. import time import datetime def format_time(elapsed): ''' Takes a time in seconds and returns a string hh:mm:ss ''' # Round to the nearest second. elapsed_rounded = int(round((elapsed))) # Format as hh:mm:ss return str(datetime.timedelta(seconds=elapsed_rounded)) We're ready to kick off the training! import random # This training code is based on the `run_glue.py` script here: # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128 # Set the seed value all over the place to make this reproducible. seed_val = 42 random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val) # Store the average loss after each epoch so we can plot them. loss_values = [] # For each epoch... for epoch_i in range(0, epochs): # ======================================== # Training # ======================================== # Perform one full pass over the training set. print(\"\") print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs)) print('Training...') # Measure how long the training epoch takes. t0 = time.time() # Reset the total loss for this epoch. total_loss = 0 # Put the model into training mode. Don't be mislead--the call to # `train` just changes the *mode*, it doesn't *perform* the training. # `dropout` and `batchnorm` layers behave differently during training # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch) model.train() # For each batch of training data... for step, batch in enumerate(train_dataloader): # Progress update every 40 batches. if step % 40 == 0 and not step == 0: # Calculate elapsed time in minutes. elapsed = format_time(time.time() - t0) # Report progress. print(' Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed)) # Unpack this training batch from our dataloader. # # As we unpack the batch, we'll also copy each tensor to the GPU using the # `to` method. # # `batch` contains three pytorch tensors: # [0]: input ids # [1]: attention masks # [2]: labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) # Always clear any previously calculated gradients before performing a # backward pass. PyTorch doesn't do this automatically because # accumulating the gradients is \"convenient while training RNNs\". # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch) model.zero_grad() # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple. loss = outputs[0] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss.item() # Perform a backward pass to calculate the gradients. loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over the training data. avg_train_loss = total_loss / len(train_dataloader) # Store the loss value for plotting the learning curve. loss_values.append(avg_train_loss) print(\"\") print(\" Average training loss: {0:.2f}\".format(avg_train_loss)) print(\" Training epcoh took: {:}\".format(format_time(time.time() - t0))) # ======================================== # Validation # ======================================== # After the completion of each training epoch, measure our performance on # our validation set. print(\"\") print(\"Running Validation...\") t0 = time.time() # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model.eval() # Tracking variables eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 # Evaluate data for one epoch for batch in validation_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch.no_grad(): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # Get the \"logits\" output by the model. The \"logits\" are the output # values prior to applying an activation function like the softmax. logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to('cpu').numpy() # Calculate the accuracy for this batch of test sentences. tmp_eval_accuracy = flat_accuracy(logits, label_ids) # Accumulate the total accuracy. eval_accuracy += tmp_eval_accuracy # Track the number of batches nb_eval_steps += 1 # Report the final accuracy for this validation run. print(\" Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps)) print(\" Validation took: {:}\".format(format_time(time.time() - t0))) print(\"\") print(\"Training complete!\") ======== Epoch 1 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:11. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:31. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.50 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.79 Validation took: 0:00:02 ======== Epoch 2 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:11. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:32. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.32 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.82 Validation took: 0:00:02 ======== Epoch 3 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:11. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:32. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.20 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.82 Validation took: 0:00:02 ======== Epoch 4 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:10. Batch 80 of 241. Elapsed: 0:00:21. Batch 120 of 241. Elapsed: 0:00:31. Batch 160 of 241. Elapsed: 0:00:42. Batch 200 of 241. Elapsed: 0:00:52. Batch 240 of 241. Elapsed: 0:01:03. Average training loss: 0.14 Training epcoh took: 0:01:03 Running Validation... Accuracy: 0.82 Validation took: 0:00:02 Training complete! Let's take a look at our training loss over all batches: import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns # Use plot styling from seaborn. sns.set(style='darkgrid') # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (12,6) # Plot the learning curve. plt.plot(loss_values, 'b-o') # Label the plot. plt.title(\"Training loss\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show()","title":"4.3. Training Loop"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#5-performance-on-test-set","text":"Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using Matthew's correlation coefficient because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task.","title":"5. Performance On Test Set"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#51-data-preparation","text":"We'll need to apply all of the same steps that we did for the training data to prepare our test data set. import pandas as pd # Load the dataset into a pandas dataframe. df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence']) # Report the number of sentences. print('Number of test sentences: {:,}\\n'.format(df.shape[0])) # Create sentence and label lists sentences = df.sentence.values labels = df.label.values # Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = [] # For every sentence... for sent in sentences: # `encode` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. encoded_sent = tokenizer.encode( sent, # Sentence to encode. add_special_tokens = True, # Add '[CLS]' and '[SEP]' ) input_ids.append(encoded_sent) # Pad our input tokens input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") # Create attention masks attention_masks = [] # Create a mask of 1s for each token followed by 0s for padding for seq in input_ids: seq_mask = [float(i>0) for i in seq] attention_masks.append(seq_mask) # Convert to tensors. prediction_inputs = torch.tensor(input_ids) prediction_masks = torch.tensor(attention_masks) prediction_labels = torch.tensor(labels) # Set the batch size. batch_size = 32 # Create the DataLoader. prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels) prediction_sampler = SequentialSampler(prediction_data) prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) Number of test sentences: 516","title":"5.1. Data Preparation"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#52-evaluate-on-test-set","text":"With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set. # Prediction on test set print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs))) # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in prediction_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch # Telling the model not to compute or store gradients, saving memory and # speeding up prediction with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to('cpu').numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(' DONE.') Predicting labels for 516 test sentences... DONE. Accuracy on the CoLA benchmark is measured using the \" Matthews correlation coefficient \" (MCC). We use MCC here because the classes are imbalanced: print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0))) Positive samples: 354 of 516 (68.60%) from sklearn.metrics import matthews_corrcoef matthews_set = [] # Evaluate each test batch using Matthew's correlation coefficient print('Calculating Matthews Corr. Coef. for each batch...') # For each input batch... for i in range(len(true_labels)): # The predictions for this batch are a 2-column ndarray (one column for \"0\" # and one column for \"1\"). Pick the label with the highest value and turn this # in to a list of 0s and 1s. pred_labels_i = np.argmax(predictions[i], axis=1).flatten() # Calculate and store the coef for this batch. matthews = matthews_corrcoef(true_labels[i], pred_labels_i) matthews_set.append(matthews) Calculating Matthews Corr. Coef. for each batch... /usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp) The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it. matthews_set [0.049286405809014416, -0.21684543705982773, 0.4040950971038548, 0.41179801403140964, 0.25365601296401685, 0.6777932975034471, 0.4879500364742666, 0.0, 0.8320502943378436, 0.8246211251235321, 0.9229582069908973, 0.647150228929434, 0.8150678894028793, 0.7141684885491869, 0.3268228676411533, 0.5844155844155844, 0.0] # Combine the predictions for each batch into a single list of 0s and 1s. flat_predictions = [item for sublist in predictions for item in sublist] flat_predictions = np.argmax(flat_predictions, axis=1).flatten() # Combine the correct labels for each batch into a single list. flat_true_labels = [item for sublist in true_labels for item in sublist] # Calculate the MCC mcc = matthews_corrcoef(flat_true_labels, flat_predictions) print('MCC: %.3f' % mcc) MCC: 0.529 Cool! In about half an hour and without doing any hyperparameter tuning (adjusting the learning rate, epochs, batch size, ADAM properties, etc.) we are able to get a good score. I should also mention we didn't train on the entire training dataset, but set aside a portion of it as our validation set for legibililty of code. The library documents the expected accuracy for this benchmark here . You can also look at the official leaderboard here . Note that (due to the small dataset size?) the accuracy can vary significantly with different random seeds.","title":"5.2. Evaluate on Test Set"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#conclusion","text":"This post demonstrates that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in.","title":"Conclusion"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#appendix","text":"","title":"Appendix"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#a1-saving-loading-fine-tuned-model","text":"This first cell (taken from run_glue.py here ) writes the model and tokenizer out to disk. import os # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained() output_dir = './model_save/' # Create output directory if needed if not os.path.exists(output_dir): os.makedirs(output_dir) print(\"Saving model to %s\" % output_dir) # Save a trained model, configuration and tokenizer using `save_pretrained()`. # They can then be reloaded using `from_pretrained()` model_to_save = model.module if hasattr(model, 'module') else model # Take care of distributed/parallel training model_to_save.save_pretrained(output_dir) tokenizer.save_pretrained(output_dir) # Good practice: save your training arguments together with the trained model # torch.save(args, os.path.join(output_dir, 'training_args.bin')) Saving model to ./model_save/ ('./model_save/vocab.txt', './model_save/special_tokens_map.json', './model_save/added_tokens.json') Let's check out the file sizes, out of curiosity. !ls -l --block-size=K ./model_save/ total 427964K -rw-r--r-- 1 root root 1K Dec 19 17:33 added_tokens.json -rw-r--r-- 1 root root 1K Dec 19 17:33 config.json -rw-r--r-- 1 root root 427719K Dec 19 17:33 pytorch_model.bin -rw-r--r-- 1 root root 1K Dec 19 17:33 special_tokens_map.json -rw-r--r-- 1 root root 1K Dec 19 17:33 tokenizer_config.json -rw-r--r-- 1 root root 227K Dec 19 17:33 vocab.txt The largest file is the model weights, at around 418 megabytes. !ls -l --block-size=M ./model_save/pytorch_model.bin -rw-r--r-- 1 root root 418M Dec 19 17:33 ./model_save/pytorch_model.bin To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive. # Mount Google Drive to this Notebook instance. from google.colab import drive drive.mount('/content/drive') # Copy the model files to a directory in your Google Drive. !cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\" The following functions will load the model back from disk. # Load a trained model and vocabulary that you have fine-tuned model = model_class.from_pretrained(output_dir) tokenizer = tokenizer_class.from_pretrained(output_dir) # Copy the model to the GPU. model.to(device)","title":"A1. Saving &amp; Loading Fine-Tuned Model"},{"location":"BERT_Fine_Tuning_Sentence_Classification_v2/#a2-weight-decay","text":"The huggingface example includes the following code block for enabling weight decay, but the default decay rate is \"0.0\", so I moved this to the appendix. This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99. # This code is taken from: # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102 # Don't apply weight decay to any parameters whose names include these tokens. # (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms) no_decay = ['bias', 'LayerNorm.weight'] # Separate the `weight` parameters from the `bias` parameters. # - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. # - For the `bias` parameters, the 'weight_decay_rate' is 0.0. optimizer_grouped_parameters = [ # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'. {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.1}, # Filter for parameters which *do* include those. {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0} ] # Note - `optimizer_grouped_parameters` only includes the parameter values, not # the names.","title":"A.2. Weight Decay"},{"location":"Bert_Classification_Pt/","text":"!wget https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv --2020-08-25 15:35:11-- https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 23677025 (23M) [text/plain] Saving to: \u2018imdb_tr.csv\u2019 imdb_tr.csv 100%[===================>] 22.58M 40.2MB/s in 0.6s 2020-08-25 15:35:12 (40.2 MB/s) - \u2018imdb_tr.csv\u2019 saved [23677025/23677025] !pip install transformers Collecting transformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 778kB 3.5MB/s \u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Collecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 16.4MB/s \u001b[?25hCollecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 20.7MB/s \u001b[?25hCollecting tokenizers==0.8.1.rc1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 32.8MB/s \u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0) Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=2a9fd2beb4fbd53b26145bf94bd8620825bcc7e90e60157fd5279762b00f2801 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2 from collections import defaultdict import numpy as np import torch.nn.functional as F import torch from torch.utils.data import Dataset, DataLoader import torch.nn as nn from transformers import * from sklearn.model_selection import train_test_split import logging import os, pandas as pd Data Preparation !head imdb_tr.csv row_Number,text,polarity 2148,\"first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!\",1 23577,\"Put aside Dr. House repeat missed, Desperate Housewives (new) watch one. don't know exactly plagued movie. never thought I'd say this, want 15 minutes fame back.<br /><br />Script, Direction, can't say. recognized stable actors (the usual suspects), thought Herbert Marshall class addition sat good cheesy flick. Boy, wrong. Dullsville.<br /><br />My favorite parts: \"\"office girl\"\" makes 029 keypunch puts cards 087 sorter. LOL @ \"\"the computer\"\". I'd like someone identify next device - 477 ? It's even dinosaur's time.<br /><br />And dinosaurs don't much time waste.\",0 1319,\"big fan Stephen King's work, film made even greater fan King. Pet Sematary Creed family. moved new house, seem happy. pet cemetery behind house. Creed's new neighbor Jud (played Fred Gwyne) explains burial ground behind pet cemetery. burial ground pure evil. Jud tells Louis Creed bury human (or kind pet) burial ground, would come back life. problem, come back, person, they're evil. Soon Jud explains everything Pet Sematary, everything starts go hell. wont explain anymore don't want give away main parts film. acting Pet Sematary pretty good, needed little bit work. story one main parts movie, mainly original gripping. film features lots make-up effects make movie way eerie, frightening. One basic reasons movie sent chills back, fact make-up effects. one character film truly freaky. character \"\"Zelda.\"\" particular character pops film three times precise. Zelda Rachel Creed's sister passed away years before, Rachel still haunted her. first time Zelda appears movie isn't generally scary isn't talking anything, second time worst, honest, second time scares living **** me. absolutely nothing wrong movie, almost perfect. Pet Sematary delivers great scares, pretty good acting, first rate plot, mesmerizing make-up. truly one favorite horror films time. 10 10.\",1 13358,\"watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. <br /><br />The characters composed one lame stereo-type another, obvious attempt creating another \"\"Bad News Bears\"\" embarrassing say least.<br /><br />I seen prized turkeys time, reason list since \"\"Numero Uno\"\".<br /><br />Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\",0 9495,\"truly enjoyed film. acting terrific plot. Jeff Combs talent recognized for. part flick would change ending. death creature far gruesome Sci Fi Channel.<br /><br />There interesting religious messages film. Jeff Combs obviously played Messiah figure creature (or shark prefer) represented anti-Chirst. particularly frightening scenes 'end world feel'. noticed third viewing classic creature feature. know many people won't get references Christianity, watch close you'll get it.\",1 2154,\"memory \"\"The Last Hunt\"\" stuck since saw 1956 13. movie far ahead others time addressed treatment natives, environment, ever present contrast short long term effects greed. relevant today 1956, cinemagraphic discussion utmost depth relevance. top setting beautiful cinematography excellent. memory movie end days.\",1 19880,\"Shakespeare fan, appreciate Ken Branagh done bring Shakespeare back new generation viewers. However, movie falls short conveying overall intentions play ridiculous musical sequences. Add Alicia Silverstone's stumbling dialogue (reminiscent Keanu Reeves Much Ado Nothing) poorly cast roles, equals excruciating endurance viewing.\",0 2073,\"privilege watching Scarface big screen beautifully restored 35mm print honor 20th anniversary films release. great see big screen much lost television sets overall largesse project cannot emphasized enough. <br /><br />Scarface remake classic rags riches depths hell story featuring Al Pacino Cuban drug lord Tony Montana. version, Tony comes America Cuban boat people immigration wave late 1970s, early 1980s. Tony cohorts quickly get green cards offing political figure Tent City brief stay Cuban restaurant; Tony launched horrific path towards total destruction. <br /><br />Many characters movie played skilled manner enjoyable watch forgot little film last twenty years. Robert Loggia Tony's patron, Frank Lopez wonderful. character flawed trusting, Tony quickly figures out, soft. Lopez's right hand, Omar Suarez portrayed one greatest actors, F. Murray Abraham (Amadeus.) Suarez ultimate toady anything Frank; like mind own. Tony quickly sees constantly battles Suarez, really sees minor problem get way top. character always comes back played perfectly Mel Bernstein, audaciously corrupt Miami Narcotics detective played Harris Yulin (Training Day.) Mel, without guilt extorts great sums money form sides drug industry. plays Tony Frank catches scene marks exit film Frank Mel. priceless hear Frank asking Mel intercede, Tony kill hear Mel reply, `It's tree Frank, you're sitting it.' man Frank paying protection!<br /><br />Tony's rise meteoric matched speed intensity quick crash burn. offing Frank taking wife business Tony's greed takes never seem get enough. Tony plunges deeper world drugs, greed inability trust eventually kills best friend sister fallen love married. sets ending Tony's compound stormed army supplier feels betrayed Tony would go political assassination ordered. stems form compassionate moment Tony refused accomplice murder would involved victim's wife children.<br /><br />All great depiction 1980s excess cocaine culture. DePalma nice job holding together one fastest moving three hour movies around. violence extremely graphic contains scenes forever etched viewers mind, particularly gruesome chainsaw seen, two point blank shots head entire bloody melee ends movie. highly recommended stylistically done film squeamish, need upbeat endings potential sequels; DePalma let fly right here.\",1 12001,\"real classic. shipload sailors trying get towns daughters fathers go extremes deter sailors attempts. maidens cry aid results dispatch \"\"Rape Squad\"\". cult film waiting happen!\",1 data = pd.read_csv('imdb_tr.csv', encoding = \"ISO-8859-1\") del data['row_Number'] data.columns = [\"text\", \"label\"] def get_train_text_df(data): train_lines, test_lines, train_labels, test_labels = train_test_split(data['text'], data['label'], test_size=0.20, random_state=3107, stratify=data['label']) train_df = pd.DataFrame({'text':train_lines.values, 'label':train_labels.values}) eval_df = pd.DataFrame({'text':test_lines.values, 'label':test_labels.values}) return train_df, eval_df train_df, eval_df = get_train_text_df(data) train_df.shape, eval_df.shape ((20000, 2), (5000, 2)) MAX_LEN = 256 class TextLabelDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_len): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.texts) def __getitem__(self, item): text = str(self.texts[item]) label = self.labels[item] encoding = self.tokenizer.encode_plus( text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True ) return { 'texts': text, 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'targets': torch.tensor(label, dtype=torch.long) } def create_data_loader(df, tokenizer, max_len, batch_size): ds = TextLabelDataset( texts=df.text.to_numpy(), labels=df.label.to_numpy(), tokenizer=tokenizer, max_len=max_len ) return DataLoader( ds, batch_size=batch_size, num_workers=4 ) tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti\u2026 BATCH_SIZE =32 train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE) val_data_loader = create_data_loader(eval_df, tokenizer, MAX_LEN, BATCH_SIZE) test_data_loader = create_data_loader(eval_df, tokenizer, MAX_LEN, BATCH_SIZE) data = next(iter(train_data_loader)) data.keys() dict_keys(['texts', 'input_ids', 'attention_mask', 'targets']) print(data['input_ids'].shape) print(data['attention_mask'].shape) print(data['targets'].shape) torch.Size([32, 256]) torch.Size([32, 256]) torch.Size([32]) Model Building device = 'cuda' model = BertModel.from_pretrained('bert-base-uncased') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_\u2026 HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri\u2026 class Classifier(nn.Module): def __init__(self, n_classes): super(Classifier, self).__init__() self.bert = model self.drop = nn.Dropout(p=0.3) self.out = nn.Linear(self.bert.config.hidden_size, n_classes) def forward(self, input_ids, attention_mask): _, pooled_output = self.bert( input_ids=input_ids, attention_mask=attention_mask ) output = self.drop(pooled_output) return self.out(output) all_labels = ['negative','positive'] classifier_model = Classifier(len(all_labels)) classifier_model = classifier_model.to(device) classifier_model Classifier( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (drop): Dropout(p=0.3, inplace=False) (out): Linear(in_features=768, out_features=2, bias=True) ) EPOCHS = 1 optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False) total_steps = len(train_data_loader) * EPOCHS scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=total_steps ) loss_fn = nn.CrossEntropyLoss().to(device) Model Training def train_epoch( model, data_loader, loss_fn, optimizer, device, scheduler, n_examples ): model = model.train() losses = [] correct_predictions = 0 for d in data_loader: input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() scheduler.step() optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) def eval_model(model, data_loader, loss_fn, device, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for d in data_loader: input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) %%time history = defaultdict(list) best_accuracy = 0 for epoch in range(EPOCHS): print(f'Epoch {epoch + 1}/{EPOCHS}') print('-' * 10) train_acc, train_loss = train_epoch( classifier_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(train_df) ) print(f'Train loss {train_loss} accuracy {train_acc}') val_acc, val_loss = eval_model( classifier_model, val_data_loader, loss_fn, device, len(eval_df) ) print(f'Val loss {val_loss} accuracy {val_acc}') print() history['train_acc'].append(train_acc) history['train_loss'].append(train_loss) history['val_acc'].append(val_acc) history['val_loss'].append(val_loss) if val_acc > best_accuracy: torch.save(classifier_model.state_dict(), 'best_model_state.bin') best_accuracy = val_acc Epoch 1/1 ---------- --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-25-b0099fccb50e> in <module>() ----> 1 get_ipython().run_cell_magic('time', '', \"\\nhistory = defaultdict(list)\\nbest_accuracy = 0\\n\\nfor epoch in range(EPOCHS):\\n\\n print(f'Epoch {epoch + 1}/{EPOCHS}')\\n print('-' * 10)\\n\\n train_acc, train_loss = train_epoch(\\n classifier_model,\\n train_data_loader, \\n loss_fn, \\n optimizer, \\n device, \\n scheduler, \\n len(train_df)\\n )\\n\\n print(f'Train loss {train_loss} accuracy {train_acc}')\\n\\n val_acc, val_loss = eval_model(\\n classifier_model,\\n val_data_loader,\\n loss_fn, \\n device, \\n len(eval_df)\\n )\\n\\n print(f'Val loss {val_loss} accuracy {val_acc}')\\n print()\\n\\n history['train_acc'].append(train_acc)\\n history['train_loss'].append(train_loss)\\n history['val_acc'].append(val_acc)\\n history['val_loss'].append(val_loss)\\n\\n if val_acc > best_accuracy:\\n torch.save(classifier_model.state_dict(), 'best_model_state.bin')\\n best_accuracy = val_acc\") /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell) 2115 magic_arg_s = self.var_expand(line, stack_depth) 2116 with self.builtin_trap: -> 2117 result = fn(magic_arg_s, cell) 2118 return result 2119 <decorator-gen-60> in time(self, line, cell, local_ns) /usr/local/lib/python3.6/dist-packages/IPython/core/magic.py in <lambda>(f, *a, **k) 186 # but it's overkill for just that one bit of state. 187 def magic_deco(arg): --> 188 call = lambda f, *a, **k: f(*a, **k) 189 190 if callable(arg): /usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns) 1191 else: 1192 st = clock2() -> 1193 exec(code, glob, local_ns) 1194 end = clock2() 1195 out = None <timed exec> in <module>() <ipython-input-23-46992c3ae043> in train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples) 29 losses.append(loss.item()) 30 ---> 31 loss.backward() 32 nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) 33 optimizer.step() /usr/local/lib/python3.6/dist-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph) 183 products. Defaults to ``False``. 184 \"\"\" --> 185 torch.autograd.backward(self, gradient, retain_graph, create_graph) 186 187 def register_hook(self, hook): /usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables) 125 Variable._execution_engine.run_backward( 126 tensors, grad_tensors, retain_graph, create_graph, --> 127 allow_unreachable=True) # allow_unreachable flag 128 129 KeyboardInterrupt: import matplotlib.pyplot as plt plt.plot(history['train_acc'], label='train accuracy') plt.plot(history['val_acc'], label='validation accuracy') plt.title('Training history') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.legend() plt.ylim([0, 1]); Model Evaluation def get_predictions(model, data_loader): model = model.eval() input_texts = [] predictions = [] prediction_probs = [] real_values = [] with torch.no_grad(): for d in data_loader: texts = d[\"texts\"] input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) probs = F.softmax(outputs, dim=1) input_texts.extend(texts) predictions.extend(preds) prediction_probs.extend(probs) real_values.extend(targets) predictions = torch.stack(predictions).cpu() prediction_probs = torch.stack(prediction_probs).cpu() real_values = torch.stack(real_values).cpu() return input_texts, predictions, prediction_probs, real_values y_texts, y_pred, y_pred_probs, y_test = get_predictions( classifier_model, test_data_loader ) from sklearn.metrics import classification_report print(classification_report(y_test, y_pred, target_names=all_labels)) precision recall f1-score support negative 0.83 0.91 0.87 2500 positive 0.90 0.81 0.86 2500 accuracy 0.86 5000 macro avg 0.87 0.86 0.86 5000 weighted avg 0.87 0.86 0.86 5000","title":"Bert Classification Pt"},{"location":"Bert_Classification_Pt/#data-preparation","text":"!head imdb_tr.csv row_Number,text,polarity 2148,\"first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!\",1 23577,\"Put aside Dr. House repeat missed, Desperate Housewives (new) watch one. don't know exactly plagued movie. never thought I'd say this, want 15 minutes fame back.<br /><br />Script, Direction, can't say. recognized stable actors (the usual suspects), thought Herbert Marshall class addition sat good cheesy flick. Boy, wrong. Dullsville.<br /><br />My favorite parts: \"\"office girl\"\" makes 029 keypunch puts cards 087 sorter. LOL @ \"\"the computer\"\". I'd like someone identify next device - 477 ? It's even dinosaur's time.<br /><br />And dinosaurs don't much time waste.\",0 1319,\"big fan Stephen King's work, film made even greater fan King. Pet Sematary Creed family. moved new house, seem happy. pet cemetery behind house. Creed's new neighbor Jud (played Fred Gwyne) explains burial ground behind pet cemetery. burial ground pure evil. Jud tells Louis Creed bury human (or kind pet) burial ground, would come back life. problem, come back, person, they're evil. Soon Jud explains everything Pet Sematary, everything starts go hell. wont explain anymore don't want give away main parts film. acting Pet Sematary pretty good, needed little bit work. story one main parts movie, mainly original gripping. film features lots make-up effects make movie way eerie, frightening. One basic reasons movie sent chills back, fact make-up effects. one character film truly freaky. character \"\"Zelda.\"\" particular character pops film three times precise. Zelda Rachel Creed's sister passed away years before, Rachel still haunted her. first time Zelda appears movie isn't generally scary isn't talking anything, second time worst, honest, second time scares living **** me. absolutely nothing wrong movie, almost perfect. Pet Sematary delivers great scares, pretty good acting, first rate plot, mesmerizing make-up. truly one favorite horror films time. 10 10.\",1 13358,\"watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. <br /><br />The characters composed one lame stereo-type another, obvious attempt creating another \"\"Bad News Bears\"\" embarrassing say least.<br /><br />I seen prized turkeys time, reason list since \"\"Numero Uno\"\".<br /><br />Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\",0 9495,\"truly enjoyed film. acting terrific plot. Jeff Combs talent recognized for. part flick would change ending. death creature far gruesome Sci Fi Channel.<br /><br />There interesting religious messages film. Jeff Combs obviously played Messiah figure creature (or shark prefer) represented anti-Chirst. particularly frightening scenes 'end world feel'. noticed third viewing classic creature feature. know many people won't get references Christianity, watch close you'll get it.\",1 2154,\"memory \"\"The Last Hunt\"\" stuck since saw 1956 13. movie far ahead others time addressed treatment natives, environment, ever present contrast short long term effects greed. relevant today 1956, cinemagraphic discussion utmost depth relevance. top setting beautiful cinematography excellent. memory movie end days.\",1 19880,\"Shakespeare fan, appreciate Ken Branagh done bring Shakespeare back new generation viewers. However, movie falls short conveying overall intentions play ridiculous musical sequences. Add Alicia Silverstone's stumbling dialogue (reminiscent Keanu Reeves Much Ado Nothing) poorly cast roles, equals excruciating endurance viewing.\",0 2073,\"privilege watching Scarface big screen beautifully restored 35mm print honor 20th anniversary films release. great see big screen much lost television sets overall largesse project cannot emphasized enough. <br /><br />Scarface remake classic rags riches depths hell story featuring Al Pacino Cuban drug lord Tony Montana. version, Tony comes America Cuban boat people immigration wave late 1970s, early 1980s. Tony cohorts quickly get green cards offing political figure Tent City brief stay Cuban restaurant; Tony launched horrific path towards total destruction. <br /><br />Many characters movie played skilled manner enjoyable watch forgot little film last twenty years. Robert Loggia Tony's patron, Frank Lopez wonderful. character flawed trusting, Tony quickly figures out, soft. Lopez's right hand, Omar Suarez portrayed one greatest actors, F. Murray Abraham (Amadeus.) Suarez ultimate toady anything Frank; like mind own. Tony quickly sees constantly battles Suarez, really sees minor problem get way top. character always comes back played perfectly Mel Bernstein, audaciously corrupt Miami Narcotics detective played Harris Yulin (Training Day.) Mel, without guilt extorts great sums money form sides drug industry. plays Tony Frank catches scene marks exit film Frank Mel. priceless hear Frank asking Mel intercede, Tony kill hear Mel reply, `It's tree Frank, you're sitting it.' man Frank paying protection!<br /><br />Tony's rise meteoric matched speed intensity quick crash burn. offing Frank taking wife business Tony's greed takes never seem get enough. Tony plunges deeper world drugs, greed inability trust eventually kills best friend sister fallen love married. sets ending Tony's compound stormed army supplier feels betrayed Tony would go political assassination ordered. stems form compassionate moment Tony refused accomplice murder would involved victim's wife children.<br /><br />All great depiction 1980s excess cocaine culture. DePalma nice job holding together one fastest moving three hour movies around. violence extremely graphic contains scenes forever etched viewers mind, particularly gruesome chainsaw seen, two point blank shots head entire bloody melee ends movie. highly recommended stylistically done film squeamish, need upbeat endings potential sequels; DePalma let fly right here.\",1 12001,\"real classic. shipload sailors trying get towns daughters fathers go extremes deter sailors attempts. maidens cry aid results dispatch \"\"Rape Squad\"\". cult film waiting happen!\",1 data = pd.read_csv('imdb_tr.csv', encoding = \"ISO-8859-1\") del data['row_Number'] data.columns = [\"text\", \"label\"] def get_train_text_df(data): train_lines, test_lines, train_labels, test_labels = train_test_split(data['text'], data['label'], test_size=0.20, random_state=3107, stratify=data['label']) train_df = pd.DataFrame({'text':train_lines.values, 'label':train_labels.values}) eval_df = pd.DataFrame({'text':test_lines.values, 'label':test_labels.values}) return train_df, eval_df train_df, eval_df = get_train_text_df(data) train_df.shape, eval_df.shape ((20000, 2), (5000, 2)) MAX_LEN = 256 class TextLabelDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_len): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.texts) def __getitem__(self, item): text = str(self.texts[item]) label = self.labels[item] encoding = self.tokenizer.encode_plus( text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True ) return { 'texts': text, 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'targets': torch.tensor(label, dtype=torch.long) } def create_data_loader(df, tokenizer, max_len, batch_size): ds = TextLabelDataset( texts=df.text.to_numpy(), labels=df.label.to_numpy(), tokenizer=tokenizer, max_len=max_len ) return DataLoader( ds, batch_size=batch_size, num_workers=4 ) tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti\u2026 BATCH_SIZE =32 train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE) val_data_loader = create_data_loader(eval_df, tokenizer, MAX_LEN, BATCH_SIZE) test_data_loader = create_data_loader(eval_df, tokenizer, MAX_LEN, BATCH_SIZE) data = next(iter(train_data_loader)) data.keys() dict_keys(['texts', 'input_ids', 'attention_mask', 'targets']) print(data['input_ids'].shape) print(data['attention_mask'].shape) print(data['targets'].shape) torch.Size([32, 256]) torch.Size([32, 256]) torch.Size([32])","title":"Data Preparation"},{"location":"Bert_Classification_Pt/#model-building","text":"device = 'cuda' model = BertModel.from_pretrained('bert-base-uncased') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_\u2026 HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri\u2026 class Classifier(nn.Module): def __init__(self, n_classes): super(Classifier, self).__init__() self.bert = model self.drop = nn.Dropout(p=0.3) self.out = nn.Linear(self.bert.config.hidden_size, n_classes) def forward(self, input_ids, attention_mask): _, pooled_output = self.bert( input_ids=input_ids, attention_mask=attention_mask ) output = self.drop(pooled_output) return self.out(output) all_labels = ['negative','positive'] classifier_model = Classifier(len(all_labels)) classifier_model = classifier_model.to(device) classifier_model Classifier( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (drop): Dropout(p=0.3, inplace=False) (out): Linear(in_features=768, out_features=2, bias=True) ) EPOCHS = 1 optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False) total_steps = len(train_data_loader) * EPOCHS scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=total_steps ) loss_fn = nn.CrossEntropyLoss().to(device)","title":"Model Building"},{"location":"Bert_Classification_Pt/#model-training","text":"def train_epoch( model, data_loader, loss_fn, optimizer, device, scheduler, n_examples ): model = model.train() losses = [] correct_predictions = 0 for d in data_loader: input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() scheduler.step() optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) def eval_model(model, data_loader, loss_fn, device, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): for d in data_loader: input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) %%time history = defaultdict(list) best_accuracy = 0 for epoch in range(EPOCHS): print(f'Epoch {epoch + 1}/{EPOCHS}') print('-' * 10) train_acc, train_loss = train_epoch( classifier_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(train_df) ) print(f'Train loss {train_loss} accuracy {train_acc}') val_acc, val_loss = eval_model( classifier_model, val_data_loader, loss_fn, device, len(eval_df) ) print(f'Val loss {val_loss} accuracy {val_acc}') print() history['train_acc'].append(train_acc) history['train_loss'].append(train_loss) history['val_acc'].append(val_acc) history['val_loss'].append(val_loss) if val_acc > best_accuracy: torch.save(classifier_model.state_dict(), 'best_model_state.bin') best_accuracy = val_acc Epoch 1/1 ---------- --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-25-b0099fccb50e> in <module>() ----> 1 get_ipython().run_cell_magic('time', '', \"\\nhistory = defaultdict(list)\\nbest_accuracy = 0\\n\\nfor epoch in range(EPOCHS):\\n\\n print(f'Epoch {epoch + 1}/{EPOCHS}')\\n print('-' * 10)\\n\\n train_acc, train_loss = train_epoch(\\n classifier_model,\\n train_data_loader, \\n loss_fn, \\n optimizer, \\n device, \\n scheduler, \\n len(train_df)\\n )\\n\\n print(f'Train loss {train_loss} accuracy {train_acc}')\\n\\n val_acc, val_loss = eval_model(\\n classifier_model,\\n val_data_loader,\\n loss_fn, \\n device, \\n len(eval_df)\\n )\\n\\n print(f'Val loss {val_loss} accuracy {val_acc}')\\n print()\\n\\n history['train_acc'].append(train_acc)\\n history['train_loss'].append(train_loss)\\n history['val_acc'].append(val_acc)\\n history['val_loss'].append(val_loss)\\n\\n if val_acc > best_accuracy:\\n torch.save(classifier_model.state_dict(), 'best_model_state.bin')\\n best_accuracy = val_acc\") /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell) 2115 magic_arg_s = self.var_expand(line, stack_depth) 2116 with self.builtin_trap: -> 2117 result = fn(magic_arg_s, cell) 2118 return result 2119 <decorator-gen-60> in time(self, line, cell, local_ns) /usr/local/lib/python3.6/dist-packages/IPython/core/magic.py in <lambda>(f, *a, **k) 186 # but it's overkill for just that one bit of state. 187 def magic_deco(arg): --> 188 call = lambda f, *a, **k: f(*a, **k) 189 190 if callable(arg): /usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns) 1191 else: 1192 st = clock2() -> 1193 exec(code, glob, local_ns) 1194 end = clock2() 1195 out = None <timed exec> in <module>() <ipython-input-23-46992c3ae043> in train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples) 29 losses.append(loss.item()) 30 ---> 31 loss.backward() 32 nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) 33 optimizer.step() /usr/local/lib/python3.6/dist-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph) 183 products. Defaults to ``False``. 184 \"\"\" --> 185 torch.autograd.backward(self, gradient, retain_graph, create_graph) 186 187 def register_hook(self, hook): /usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables) 125 Variable._execution_engine.run_backward( 126 tensors, grad_tensors, retain_graph, create_graph, --> 127 allow_unreachable=True) # allow_unreachable flag 128 129 KeyboardInterrupt: import matplotlib.pyplot as plt plt.plot(history['train_acc'], label='train accuracy') plt.plot(history['val_acc'], label='validation accuracy') plt.title('Training history') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.legend() plt.ylim([0, 1]);","title":"Model Training"},{"location":"Bert_Classification_Pt/#model-evaluation","text":"def get_predictions(model, data_loader): model = model.eval() input_texts = [] predictions = [] prediction_probs = [] real_values = [] with torch.no_grad(): for d in data_loader: texts = d[\"texts\"] input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) probs = F.softmax(outputs, dim=1) input_texts.extend(texts) predictions.extend(preds) prediction_probs.extend(probs) real_values.extend(targets) predictions = torch.stack(predictions).cpu() prediction_probs = torch.stack(prediction_probs).cpu() real_values = torch.stack(real_values).cpu() return input_texts, predictions, prediction_probs, real_values y_texts, y_pred, y_pred_probs, y_test = get_predictions( classifier_model, test_data_loader ) from sklearn.metrics import classification_report print(classification_report(y_test, y_pred, target_names=all_labels)) precision recall f1-score support negative 0.83 0.91 0.87 2500 positive 0.90 0.81 0.86 2500 accuracy 0.86 5000 macro avg 0.87 0.86 0.86 5000 weighted avg 0.87 0.86 0.86 5000","title":"Model Evaluation"},{"location":"Bert_Pre_Training/","text":"!pip install transformers Collecting transformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 3.4MB/s \u001b[?25hCollecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 16.3MB/s \u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7) Collecting tokenizers==0.8.1.rc2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 25.3MB/s \u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Collecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 44.6MB/s \u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4) Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b4d9f604b99e77f4dc2b8892460fc931726fa7d37c4fc51dfcb98c01d6d08797 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.1.0 MAX_LEN = 128 BATCH_SIZE = 16 # per TPU core TOTAL_STEPS = 2000 # thats approx 4 epochs EVALUATE_EVERY = 200 LR = 1e-5 PRETRAINED_MODEL = 'bert-base-uncased' import os import numpy as np import pandas as pd import tensorflow as tf print(tf.__version__) from tensorflow.keras.optimizers import Adam import transformers from transformers import TFAutoModelWithLMHead, AutoTokenizer import logging AUTO = tf.data.experimental.AUTOTUNE 2.3.0 def connect_to_TPU(): \"\"\"Detect hardware, return appropriate distribution strategy\"\"\" try: # TPU detection. No parameters necessary if TPU_NAME environment variable is # set: this is always the case on Kaggle. tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print('Running on TPU ', tpu.master()) except ValueError: tpu = None if tpu: tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) else: # Default distribution strategy in Tensorflow. Works on CPU and single GPU. strategy = tf.distribute.get_strategy() global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync return tpu, strategy, global_batch_size tpu, strategy, global_batch_size = connect_to_TPU() print(\"REPLICAS: \", strategy.num_replicas_in_sync) INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0 Running on TPU grpc://10.19.232.114:8470 INFO:tensorflow:Initializing the TPU system: grpc://10.19.232.114:8470 INFO:tensorflow:Initializing the TPU system: grpc://10.19.232.114:8470 INFO:tensorflow:Clearing out eager caches INFO:tensorflow:Clearing out eager caches INFO:tensorflow:Finished initializing TPU system. INFO:tensorflow:Finished initializing TPU system. WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use the non experimental symbol `tf.distribute.TPUStrategy` instead. INFO:tensorflow:Found TPU system: INFO:tensorflow:Found TPU system: INFO:tensorflow:*** Num TPU Cores: 8 INFO:tensorflow:*** Num TPU Cores: 8 INFO:tensorflow:*** Num TPU Workers: 1 INFO:tensorflow:*** Num TPU Workers: 1 INFO:tensorflow:*** Num TPU Cores Per Worker: 8 INFO:tensorflow:*** Num TPU Cores Per Worker: 8 INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0) REPLICAS: 8 !wget https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv --2020-09-02 10:33:57-- https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 23677025 (23M) [text/plain] Saving to: \u2018imdb_tr.csv\u2019 imdb_tr.csv 100%[===================>] 22.58M 49.2MB/s in 0.5s 2020-09-02 10:33:58 (49.2 MB/s) - \u2018imdb_tr.csv\u2019 saved [23677025/23677025] data = pd.read_csv('imdb_tr.csv', encoding = \"ISO-8859-1\") data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } row_Number text polarity 0 2148 first think another Disney movie, might good, ... 1 1 23577 Put aside Dr. House repeat missed, Desperate H... 0 2 1319 big fan Stephen King's work, film made even gr... 1 3 13358 watched horrid thing TV. Needless say one movi... 0 4 9495 truly enjoyed film. acting terrific plot. Jeff... 1 #data = data.sample(1000) %%time def regular_encode(texts, tokenizer, maxlen=512): enc_di = tokenizer.batch_encode_plus( texts, return_attention_mask=False, return_token_type_ids=False, pad_to_max_length=True, max_length=maxlen, truncation=True ) return np.array(enc_di['input_ids']) tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL) X_data = regular_encode(data.text.values, tokenizer, maxlen=MAX_LEN) HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_\u2026 HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti\u2026 /usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, CPU times: user 1min 4s, sys: 233 ms, total: 1min 4s Wall time: 1min 5s def prepare_mlm_input_and_labels(X): # 15% BERT masking inp_mask = np.random.rand(*X.shape)<0.15 # do not mask special tokens inp_mask[X<=2] = False # set targets to -1 by default, it means ignore labels = -1 * np.ones(X.shape, dtype=int) # set labels for masked tokens labels[inp_mask] = X[inp_mask] # prepare input X_mlm = np.copy(X) # set input to [MASK] which is the last token for the 90% of tokens # this means leaving 10% unchanged inp_mask_2mask = inp_mask & (np.random.rand(*X.shape)<0.90) X_mlm[inp_mask_2mask] = tokenizer.mask_token_id # mask token is the last in the dict # set 10% to a random token inp_mask_2random = inp_mask_2mask & (np.random.rand(*X.shape) < 1/9) X_mlm[inp_mask_2random] = np.random.randint(3, tokenizer.mask_token_id, inp_mask_2random.sum()) return X_mlm, labels # use validation and test data for mlm X_train_mlm = np.vstack(X_data) # masks and labels X_train_mlm, y_train_mlm = prepare_mlm_input_and_labels(X_train_mlm) def create_dist_dataset(X, y=None, training=False): dataset = tf.data.Dataset.from_tensor_slices(X) ### Add y if present ### if y is not None: dataset_y = tf.data.Dataset.from_tensor_slices(y) dataset = tf.data.Dataset.zip((dataset, dataset_y)) ### Repeat if training ### if training: dataset = dataset.shuffle(len(X)).repeat() dataset = dataset.batch(global_batch_size).prefetch(AUTO) ### make it distributed ### dist_dataset = strategy.experimental_distribute_dataset(dataset) return dist_dataset train_dist_dataset = create_dist_dataset(X_train_mlm, y_train_mlm, True) %%time def create_mlm_model_and_optimizer(): with strategy.scope(): model = TFAutoModelWithLMHead.from_pretrained(PRETRAINED_MODEL) optimizer = tf.keras.optimizers.Adam(learning_rate=LR) return model, optimizer mlm_model, optimizer = create_mlm_model_and_optimizer() mlm_model.summary() /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_auto.py:788: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models. FutureWarning, HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri\u2026 Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForMaskedLM: ['nsp___cls'] - This IS expected if you are initializing TFBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing TFBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). All the weights of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training. Model: \"tf_bert_for_masked_lm\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= bert (TFBertMainLayer) multiple 109482240 _________________________________________________________________ mlm___cls (TFBertMLMHead) multiple 24459834 ================================================================= Total params: 110,104,890 Trainable params: 110,104,890 Non-trainable params: 0 _________________________________________________________________ CPU times: user 14.5 s, sys: 15 s, total: 29.5 s Wall time: 58.1 s def define_mlm_loss_and_metrics(): with strategy.scope(): mlm_loss_object = masked_sparse_categorical_crossentropy def compute_mlm_loss(labels, predictions): per_example_loss = mlm_loss_object(labels, predictions) loss = tf.nn.compute_average_loss( per_example_loss, global_batch_size = global_batch_size) return loss train_mlm_loss_metric = tf.keras.metrics.Mean() return compute_mlm_loss, train_mlm_loss_metric def masked_sparse_categorical_crossentropy(y_true, y_pred): y_true_masked = tf.boolean_mask(y_true, tf.not_equal(y_true, -1)) y_pred_masked = tf.boolean_mask(y_pred, tf.not_equal(y_true, -1)) loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_masked, y_pred_masked, from_logits=True) return loss def train_mlm(train_dist_dataset, total_steps=2000, evaluate_every=200): step = 0 ### Training lopp ### for tensor in train_dist_dataset: distributed_mlm_train_step(tensor) step+=1 if (step % evaluate_every == 0): ### Print train metrics ### train_metric = train_mlm_loss_metric.result().numpy() print(\"Step %d, train loss: %.2f\" % (step, train_metric)) ### Reset metrics ### train_mlm_loss_metric.reset_states() if step == total_steps: break @tf.function def distributed_mlm_train_step(data): strategy.experimental_run_v2(mlm_train_step, args=(data,)) @tf.function def mlm_train_step(inputs): features, labels = inputs with tf.GradientTape() as tape: predictions = mlm_model(features, training=True)[0] loss = compute_mlm_loss(labels, predictions) gradients = tape.gradient(loss, mlm_model.trainable_variables) optimizer.apply_gradients(zip(gradients, mlm_model.trainable_variables)) train_mlm_loss_metric.update_state(loss) compute_mlm_loss, train_mlm_loss_metric = define_mlm_loss_and_metrics() %%time train_mlm(train_dist_dataset, TOTAL_STEPS, EVALUATE_EVERY) WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.data.Iterator.get_next_as_optional()` instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.data.Iterator.get_next_as_optional()` instead. WARNING:tensorflow:From <ipython-input-12-d78fc23ea715>:47: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version. Instructions for updating: renamed to `run` WARNING:tensorflow:From <ipython-input-12-d78fc23ea715>:47: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version. Instructions for updating: renamed to `run` WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss. WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss. WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss. WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss. Step 200, train loss: 8.89 Step 400, train loss: 8.03 Step 600, train loss: 7.68 Step 800, train loss: 7.43 Step 1000, train loss: 7.22 Step 1200, train loss: 7.00 Step 1400, train loss: 6.86 Step 1600, train loss: 6.68 Step 1800, train loss: 6.54 Step 2000, train loss: 6.38 CPU times: user 1min 23s, sys: 13.4 s, total: 1min 37s Wall time: 9min 3s mlm_model.save_pretrained('imdb_bert_uncased') Load and Test from transformers import * from pprint import pprint pretrained_model = TFAutoModelWithLMHead.from_pretrained(PRETRAINED_MODEL) nlp = pipeline(\"fill-mask\",model=pretrained_model, tokenizer=tokenizer ,framework='tf') pprint(nlp(f\"I watched {nlp.tokenizer.mask_token} and that was awesome\")) /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_auto.py:788: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models. FutureWarning, Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForMaskedLM: ['nsp___cls'] - This IS expected if you are initializing TFBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing TFBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). All the weights of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training. [{'score': 0.31239137053489685, 'sequence': '[CLS] i watched him and that was awesome [SEP]', 'token': 2032, 'token_str': 'him'}, {'score': 0.1729636937379837, 'sequence': '[CLS] i watched her and that was awesome [SEP]', 'token': 2014, 'token_str': 'her'}, {'score': 0.13816313445568085, 'sequence': '[CLS] i watched it and that was awesome [SEP]', 'token': 2009, 'token_str': 'it'}, {'score': 0.08374697715044022, 'sequence': '[CLS] i watched, and that was awesome [SEP]', 'token': 1010, 'token_str': ','}, {'score': 0.06438492983579636, 'sequence': '[CLS] i watched them and that was awesome [SEP]', 'token': 2068, 'token_str': 'them'}] movie_mlm_model = TFAutoModelWithLMHead.from_pretrained('imdb_bert_uncased') nlp = pipeline(\"fill-mask\",model=movie_mlm_model, tokenizer=tokenizer ,framework='tf') pprint(nlp(f\"I watched {nlp.tokenizer.mask_token} and that was awesome\")) /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_auto.py:788: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models. FutureWarning, All model checkpoint weights were used when initializing TFBertForMaskedLM. All the weights of TFBertForMaskedLM were initialized from the model checkpoint at imdb_bert_uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training. [{'score': 0.4467789828777313, 'sequence': '[CLS] i watched it and that was awesome [SEP]', 'token': 2009, 'token_str': 'it'}, {'score': 0.06318594515323639, 'sequence': '[CLS] i watched movie and that was awesome [SEP]', 'token': 3185, 'token_str': 'movie'}, {'score': 0.056345004588365555, 'sequence': '[CLS] i watched, and that was awesome [SEP]', 'token': 1010, 'token_str': ','}, {'score': 0.013144557364284992, 'sequence': '[CLS] i watched this and that was awesome [SEP]', 'token': 2023, 'token_str': 'this'}, {'score': 0.012886741198599339, 'sequence': '[CLS] i watched one and that was awesome [SEP]', 'token': 2028, 'token_str': 'one'}]","title":"Bert Pre Training"},{"location":"Bert_Pre_Training/#load-and-test","text":"from transformers import * from pprint import pprint pretrained_model = TFAutoModelWithLMHead.from_pretrained(PRETRAINED_MODEL) nlp = pipeline(\"fill-mask\",model=pretrained_model, tokenizer=tokenizer ,framework='tf') pprint(nlp(f\"I watched {nlp.tokenizer.mask_token} and that was awesome\")) /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_auto.py:788: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models. FutureWarning, Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForMaskedLM: ['nsp___cls'] - This IS expected if you are initializing TFBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing TFBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). All the weights of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training. [{'score': 0.31239137053489685, 'sequence': '[CLS] i watched him and that was awesome [SEP]', 'token': 2032, 'token_str': 'him'}, {'score': 0.1729636937379837, 'sequence': '[CLS] i watched her and that was awesome [SEP]', 'token': 2014, 'token_str': 'her'}, {'score': 0.13816313445568085, 'sequence': '[CLS] i watched it and that was awesome [SEP]', 'token': 2009, 'token_str': 'it'}, {'score': 0.08374697715044022, 'sequence': '[CLS] i watched, and that was awesome [SEP]', 'token': 1010, 'token_str': ','}, {'score': 0.06438492983579636, 'sequence': '[CLS] i watched them and that was awesome [SEP]', 'token': 2068, 'token_str': 'them'}] movie_mlm_model = TFAutoModelWithLMHead.from_pretrained('imdb_bert_uncased') nlp = pipeline(\"fill-mask\",model=movie_mlm_model, tokenizer=tokenizer ,framework='tf') pprint(nlp(f\"I watched {nlp.tokenizer.mask_token} and that was awesome\")) /usr/local/lib/python3.6/dist-packages/transformers/modeling_tf_auto.py:788: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models. FutureWarning, All model checkpoint weights were used when initializing TFBertForMaskedLM. All the weights of TFBertForMaskedLM were initialized from the model checkpoint at imdb_bert_uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training. [{'score': 0.4467789828777313, 'sequence': '[CLS] i watched it and that was awesome [SEP]', 'token': 2009, 'token_str': 'it'}, {'score': 0.06318594515323639, 'sequence': '[CLS] i watched movie and that was awesome [SEP]', 'token': 3185, 'token_str': 'movie'}, {'score': 0.056345004588365555, 'sequence': '[CLS] i watched, and that was awesome [SEP]', 'token': 1010, 'token_str': ','}, {'score': 0.013144557364284992, 'sequence': '[CLS] i watched this and that was awesome [SEP]', 'token': 2023, 'token_str': 'this'}, {'score': 0.012886741198599339, 'sequence': '[CLS] i watched one and that was awesome [SEP]', 'token': 2028, 'token_str': 'one'}]","title":"Load and Test"},{"location":"Doc_Visual_QA_and_Bill_extraction_demo/","text":"!wget --no-check-certificate https://datasets.cvc.uab.es/rrc/DocVQA/train.tar.gz --2022-05-01 13:20:26-- https://datasets.cvc.uab.es/rrc/DocVQA/train.tar.gz Resolving datasets.cvc.uab.es (datasets.cvc.uab.es)... 158.109.8.18 Connecting to datasets.cvc.uab.es (datasets.cvc.uab.es)|158.109.8.18|:443... connected. WARNING: cannot verify datasets.cvc.uab.es's certificate, issued by \u2018CN=GEANT OV RSA CA 4,O=GEANT Vereniging,C=NL\u2019: Unable to locally verify the issuer's authority. HTTP request sent, awaiting response... 200 OK Length: 7122739200 (6.6G) [application/x-gzip] Saving to: \u2018train.tar.gz.1\u2019 train.tar.gz.1 0%[ ] 39.45M 638KB/s eta 2h 26m !wget --no-check-certificate https://datasets.cvc.uab.es/rrc/DocVQA/val.tar.gz !wget --no-check-certificate https://datasets.cvc.uab.es/rrc/DocVQA/test.tar.gz Install Packages !pip install -q transformers \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0 MB 5.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 895 kB 45.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.6 MB 31.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 4.5 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596 kB 43.9 MB/s \u001b[?25h !pip install pyyaml==5.1 # workaround: install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9 (issue: https://github.com/facebookresearch/detectron2/issues/3158) !pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html # install detectron2 that matches pytorch 1.8 # See https://detectron2.readthedocs.io/tutorials/install.html for instructions !pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html # exit(0) # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime Collecting pyyaml==5.1 Downloading PyYAML-5.1.tar.gz (274 kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 274 kB 5.3 MB/s \u001b[?25hBuilding wheels for collected packages: pyyaml Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=923c6817c78b049bf1912c6f36e26c890af93770cb6627b50efc1b77ac4eeeae Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9 Successfully built pyyaml Installing collected packages: pyyaml Attempting uninstall: pyyaml Found existing installation: PyYAML 6.0 Uninstalling PyYAML-6.0: Successfully uninstalled PyYAML-6.0 Successfully installed pyyaml-5.1 Looking in links: https://download.pytorch.org/whl/torch_stable.html Collecting torch==1.8.0+cu101 Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 763.5 MB 16 kB/s \u001b[?25hCollecting torchvision==0.9.0+cu101 Downloading https://download.pytorch.org/whl/cu101/torchvision-0.9.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (17.3 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.3 MB 795 kB/s \u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.21.6) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (4.2.0) Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu101) (7.1.2) Installing collected packages: torch, torchvision Attempting uninstall: torch Found existing installation: torch 1.11.0+cu113 Uninstalling torch-1.11.0+cu113: Successfully uninstalled torch-1.11.0+cu113 Attempting uninstall: torchvision Found existing installation: torchvision 0.12.0+cu113 Uninstalling torchvision-0.12.0+cu113: Successfully uninstalled torchvision-0.12.0+cu113 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.8.0+cu101 which is incompatible. torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m Successfully installed torch-1.8.0+cu101 torchvision-0.9.0+cu101 \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.3 MB 897 kB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 74 kB 2.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50 kB 5.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 147 kB 10.7 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 130 kB 33.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 749 kB 42.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 843 kB 34.7 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112 kB 46.3 MB/s \u001b[?25h Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone !pip install -q datasets \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 325 kB 5.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212 kB 43.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 136 kB 43.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1 MB 35.9 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127 kB 45.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 144 kB 46.5 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94 kB 2.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 271 kB 47.1 MB/s \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m \u001b[?25h !sudo apt install tesseract-ocr !pip install -q pytesseract Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: libnvidia-common-460 nsight-compute-2020.2.0 Use 'sudo apt autoremove' to remove them. The following additional packages will be installed: tesseract-ocr-eng tesseract-ocr-osd The following NEW packages will be installed: tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd 0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded. Need to get 4,795 kB of archives. After this operation, 15.8 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB] Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB] Fetched 4,795 kB in 1s (3,752 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package tesseract-ocr-eng. (Reading database ... 155202 files and directories currently installed.) Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ... Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ... Selecting previously unselected package tesseract-ocr-osd. Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ... Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ... Selecting previously unselected package tesseract-ocr. Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ... Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ... Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ... Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ... Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3 MB 5.4 MB/s \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m \u001b[?25h !pip install Pillow==9.0.0 Collecting Pillow==9.0.0 Downloading Pillow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3 MB 5.3 MB/s \u001b[?25hInstalling collected packages: Pillow Attempting uninstall: Pillow Found existing installation: Pillow 9.1.0 Uninstalling Pillow-9.1.0: Successfully uninstalled Pillow-9.1.0 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m Successfully installed Pillow-9.0.0 DocVQA Demo from transformers import LayoutLMv2Processor processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\") Downloading: 0%| | 0.00/135 [00:00<?, ?B/s] Downloading: 0%| | 0.00/226k [00:00<?, ?B/s] Downloading: 0%| | 0.00/707 [00:00<?, ?B/s] from transformers import AutoModelForQuestionAnswering model = AutoModelForQuestionAnswering.from_pretrained(\"tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\") Downloading: 0%| | 0.00/2.69k [00:00<?, ?B/s] Downloading: 0%| | 0.00/765M [00:00<?, ?B/s] import torch device = torch.device(\"cuda\") model.to(device) def run_qa(image, question): # step 1: encoding encoding = processor(image, question, return_tensors=\"pt\", truncation=True) # step 2: forward pass for k,v in encoding.items(): encoding[k] = v.to(model.device) outputs = model(**encoding) # step 3: get start_logits and end_logits start_logits = outputs.start_logits end_logits = outputs.end_logits # step 4: get largest logit for both predicted_start_idx = start_logits.argmax(-1).item() predicted_end_idx = end_logits.argmax(-1).item() print(\"Predicted start idx:\", predicted_start_idx) print(\"Predicted end idx:\", predicted_end_idx) # step 5: decode the predicted answer return processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx:predicted_end_idx+1]) from PIL import Image image = Image.open(\"image-2.jpeg\").convert(\"RGB\") image question = \"Where to call?\" run_qa(image, question) Predicted start idx: 77 Predicted end idx: 82 'ext. 7240.' Bill Information extraction Demo import numpy as np from transformers import LayoutLMv2Processor, LayoutLMv2ForTokenClassification from datasets import load_dataset from PIL import Image, ImageDraw, ImageFont processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\") model = LayoutLMv2ForTokenClassification.from_pretrained(\"Theivaprakasham/layoutlmv2-finetuned-sroie\") Downloading: 0%| | 0.00/3.07k [00:00<?, ?B/s] Downloading: 0%| | 0.00/765M [00:00<?, ?B/s] dataset = load_dataset(\"darentang/sroie\", split=\"test\") Downloading builder script: 0%| | 0.00/4.25k [00:00<?, ?B/s] Downloading and preparing dataset sroie/sroie to /root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/26ed9374c9a15a1d2f44fd8886f679076e1a1fd7da2d53726d6e58a99436c506... Downloading data files: 0%| | 0/1 [00:00<?, ?it/s] Downloading data: 0%| | 0.00/456M [00:00<?, ?B/s] Extracting data files: 0%| | 0/1 [00:00<?, ?it/s] Generating train split: 0 examples [00:00, ? examples/s] Generating test split: 0 examples [00:00, ? examples/s] Dataset sroie downloaded and prepared to /root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/26ed9374c9a15a1d2f44fd8886f679076e1a1fd7da2d53726d6e58a99436c506. Subsequent calls will reuse this data. Image.open(dataset[50][\"image_path\"]).convert(\"RGB\").save(\"example1.png\") ls example1.png image-2.jpeg \u001b[0m\u001b[01;34msample_data\u001b[0m/ train.tar.gz train.tar.gz.1 Image.open(dataset[100][\"image_path\"]).convert(\"RGB\").save(\"example2.png\") # define id2label, label2color labels = dataset.features['ner_tags'].feature.names id2label = {v: k for v, k in enumerate(labels)} label2color = {'B-ADDRESS': 'blue', 'B-COMPANY': 'green', 'B-DATE': 'red', 'B-TOTAL': 'red', 'I-ADDRESS': \"blue\", 'I-COMPANY': 'green', 'I-DATE': 'red', 'I-TOTAL': 'red', 'O': 'green'} label2color = dict((k.lower(), v.lower()) for k,v in label2color.items()) def unnormalize_box(bbox, width, height): return [ width * (bbox[0] / 1000), height * (bbox[1] / 1000), width * (bbox[2] / 1000), height * (bbox[3] / 1000), ] def iob_to_label(label): return label image = Image.open(\"example2.png\").convert(\"RGB\") image width, height = image.size width, height (932, 2216) encoding = processor(image, truncation=True, return_offsets_mapping=True, return_tensors=\"pt\") offset_mapping = encoding.pop('offset_mapping') outputs = model(**encoding) # get predictions predictions = outputs.logits.argmax(-1).squeeze().tolist() token_boxes = encoding.bbox.squeeze().tolist() # only keep non-subword predictions is_subword = np.array(offset_mapping.squeeze().tolist())[:,0] != 0 true_predictions = [id2label[pred] for idx, pred in enumerate(predictions) if not is_subword[idx]] true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(token_boxes) if not is_subword[idx]] true_boxes [[0.0, 0.0, 0.0, 0.0], [430.584, 26.592000000000002, 523.7840000000001, 90.85600000000001], [680.36, 33.24, 722.3000000000001, 79.776], [163.1, 259.272, 296.376, 294.728], [315.94800000000004, 261.488, 462.272, 294.728], [480.91200000000003, 261.488, 581.568, 296.944], [596.48, 261.488, 752.124, 296.944], [335.52, 301.37600000000003, 582.5, 334.616], [258.16400000000004, 352.344, 319.67600000000004, 378.93600000000004], [332.724, 352.344, 386.78, 378.93600000000004], [398.896, 354.56, 499.552, 378.93600000000004], [514.464, 354.56, 660.788, 381.152], [229.272, 398.88, 323.404, 427.688], [335.52, 398.88, 480.91200000000003, 427.688], [501.41600000000005, 401.096, 685.952, 429.904], [144.46, 474.224, 203.176, 500.81600000000003], [217.156, 480.872, 219.952, 483.088], [233.0, 474.224, 433.38, 503.03200000000004], [481.844, 476.44, 543.356, 503.03200000000004], [556.404, 500.81600000000003, 558.268, 500.81600000000003], [571.316, 476.44, 772.6279999999999, 505.248], [275.872, 525.192, 342.976, 551.784], [355.092, 525.192, 385.84799999999996, 554.0], [398.896, 531.84, 400.76, 551.784], [414.74, 525.192, 641.2159999999999, 554.0], [32.620000000000005, 582.808, 876.0799999999999, 622.696], [46.6, 624.9119999999999, 83.88, 651.504], [95.996, 627.1279999999999, 147.256, 653.7199999999999], [45.668, 675.88, 121.16000000000001, 704.688], [132.344, 678.096, 227.408, 704.688], [239.524, 678.096, 329.928, 706.904], [340.18, 678.096, 553.608, 706.904], [45.668, 720.2, 93.2, 749.008], [108.11200000000001, 722.416, 117.432, 749.008], [133.27599999999998, 722.416, 234.864, 749.008], [247.912, 722.416, 341.11199999999997, 751.224], [354.16, 724.6320000000001, 370.93600000000004, 749.008], [44.736000000000004, 764.52, 158.44, 791.112], [172.42, 764.52, 259.096, 791.112], [271.212, 766.736, 364.41200000000003, 793.328], [45.668, 813.2719999999999, 189.19600000000003, 839.864], [202.244, 813.2719999999999, 387.712, 842.08], [44.736000000000004, 859.808, 244.184, 888.6160000000001], [46.6, 899.696, 169.624, 932.9359999999999], [285.192, 901.9119999999999, 483.708, 932.9359999999999], [602.072, 908.56, 677.564, 935.1519999999999], [703.66, 904.1279999999999, 875.1479999999999, 935.1519999999999], [45.668, 952.88, 161.236, 981.688], [259.096, 979.472, 261.89200000000005, 981.688], [289.852, 957.312, 378.39200000000005, 983.904], [598.344, 957.312, 675.6999999999999, 983.904], [703.66, 959.528, 833.208, 986.12], [47.532, 1003.8480000000001, 230.204, 1037.088], [259.096, 1014.928, 261.89200000000005, 1032.656], [610.46, 1008.2800000000001, 677.564, 1034.872], [58.716, 1054.816, 131.41199999999998, 1059.248], [527.512, 1076.9759999999999, 607.664, 1108.0], [700.864, 1059.248, 832.2760000000001, 1108.0], [32.620000000000005, 1110.216, 188.264, 1143.4560000000001], [189.19600000000003, 1112.432, 367.208, 1145.672], [369.072, 1112.432, 481.844, 1145.672], [520.988, 1110.216, 615.12, 1136.808], [0.0, 0.0, 932.0, 2216.0], [674.768, 1112.432, 779.1519999999999, 1136.808], [830.412, 1116.864, 877.944, 1150.104], [47.532, 1150.104, 139.79999999999998, 1174.48], [305.696, 1156.752, 312.22, 1176.6960000000001], [399.828, 1154.536, 470.66, 1178.912], [531.24, 1154.536, 602.072, 1178.912], [711.116, 1156.752, 782.88, 1181.1280000000002], [841.596, 1156.752, 877.944, 1181.1280000000002], [46.6, 1198.856, 127.68400000000001, 1223.2320000000002], [140.732, 1198.856, 230.204, 1223.2320000000002], [241.388, 1198.856, 280.532, 1227.6640000000002], [49.396, 1252.04, 138.868, 1274.1999999999998], [303.832, 1252.04, 315.94800000000004, 1276.416], [408.216, 1254.2559999999999, 462.272, 1276.416], [540.56, 1254.2559999999999, 595.548, 1278.6319999999998], [711.116, 1256.472, 740.94, 1280.848], [752.124, 1256.472, 781.948, 1280.848], [842.528, 1258.6879999999999, 877.944, 1280.848], [45.668, 1298.576, 118.364, 1322.952], [122.092, 1298.576, 194.78799999999998, 1322.952], [207.836, 1300.792, 259.096, 1325.168], [31.688000000000002, 1338.464, 59.648, 1342.896], [62.444, 1340.68, 166.828, 1373.92], [165.896, 1340.68, 229.272, 1380.568], [238.592, 1325.168, 876.0799999999999, 1378.352], [165.896, 1424.8880000000001, 235.796, 1451.48], [247.912, 1427.104, 327.132, 1453.6960000000001], [339.248, 1427.104, 488.368, 1462.5600000000002], [500.48400000000004, 1429.32, 578.772, 1460.344], [607.664, 1435.968, 610.46, 1455.912], [703.66, 1429.32, 786.608, 1455.912], [451.08799999999997, 1486.9360000000001, 579.704, 1513.528], [607.664, 1493.584, 610.46, 1513.528], [722.3000000000001, 1486.9360000000001, 786.608, 1515.7440000000001], [431.516, 1544.552, 501.41600000000005, 1571.144], [513.532, 1546.7679999999998, 579.704, 1571.144], [607.664, 1553.416, 610.46, 1571.144], [722.3000000000001, 1548.984, 785.6759999999999, 1573.36], [442.7, 1599.952, 578.772, 1635.408], [607.664, 1608.816, 610.46, 1628.76], [722.3000000000001, 1602.168, 785.6759999999999, 1628.76], [99.724, 1657.568, 179.876, 1684.16], [192.92399999999998, 1657.568, 279.59999999999997, 1684.16], [290.784, 1659.784, 448.292, 1690.808], [459.476, 1662.0, 490.232, 1686.376], [501.41600000000005, 1662.0, 580.636, 1695.24], [607.664, 1668.648, 611.392, 1688.592], [702.728, 1662.0, 785.6759999999999, 1690.808], [484.64000000000004, 1719.616, 554.54, 1746.208], [558.268, 1719.616, 611.392, 1746.208], [702.728, 1721.832, 785.6759999999999, 1748.424], [453.884, 1763.9360000000001, 558.268, 1797.1760000000002], [562.928, 1770.584, 611.392, 1790.528], [721.368, 1763.9360000000001, 785.6759999999999, 1792.7440000000001], [101.588, 1854.792, 169.624, 1890.248], [181.74, 1863.656, 355.092, 1892.464], [528.444, 1859.224, 594.616, 1863.656], [107.18, 1919.056, 169.624, 1950.08], [174.284, 1919.056, 258.16400000000004, 1952.296], [355.092, 1921.272, 381.188, 1954.512], [471.592, 1923.488, 620.712, 1956.728], [0.0, 0.0, 932.0, 2216.0], [697.136, 1923.488, 839.732, 1958.944], [110.908, 1963.376, 153.78, 1989.968], [359.752, 1965.592, 375.596, 1989.968], [537.764, 1967.808, 619.7800000000001, 1992.184], [775.424, 1967.808, 838.8000000000001, 1994.4], [298.24, 2012.1280000000002, 378.39200000000005, 2040.9360000000001], [393.304, 2020.992, 397.964, 2040.9360000000001], [534.968, 2014.344, 616.984, 2040.9360000000001], [775.424, 2016.5600000000002, 839.732, 2043.152], [94.132, 2098.5519999999997, 202.244, 2122.928], [214.36, 2098.5519999999997, 290.784, 2122.928], [300.104, 2098.5519999999997, 358.82, 2122.928], [369.072, 2100.768, 427.788, 2122.928], [439.904, 2100.768, 633.76, 2127.36], [645.876, 2102.984, 738.144, 2125.144], [749.3280000000001, 2102.984, 816.432, 2125.144], [932.0, 2216.0, 932.0, 2216.0]] true_predictions ['O', 'O', 'O', 'B-COMPANY', 'I-COMPANY', 'I-COMPANY', 'I-COMPANY', 'O', 'B-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOTAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] # draw predictions over the image draw = ImageDraw.Draw(image) font = ImageFont.load_default() for prediction, box in zip(true_predictions, true_boxes): predicted_label = iob_to_label(prediction).lower() draw.rectangle(box, outline=label2color[predicted_label]) draw.text((box[0]+10, box[1]-10), text=predicted_label, fill=label2color[predicted_label], font=font) image","title":"Doc Visual QA and Bill extraction demo"},{"location":"Doc_Visual_QA_and_Bill_extraction_demo/#install-packages","text":"!pip install -q transformers \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0 MB 5.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 895 kB 45.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.6 MB 31.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 4.5 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596 kB 43.9 MB/s \u001b[?25h !pip install pyyaml==5.1 # workaround: install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9 (issue: https://github.com/facebookresearch/detectron2/issues/3158) !pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html # install detectron2 that matches pytorch 1.8 # See https://detectron2.readthedocs.io/tutorials/install.html for instructions !pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html # exit(0) # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime Collecting pyyaml==5.1 Downloading PyYAML-5.1.tar.gz (274 kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 274 kB 5.3 MB/s \u001b[?25hBuilding wheels for collected packages: pyyaml Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=923c6817c78b049bf1912c6f36e26c890af93770cb6627b50efc1b77ac4eeeae Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9 Successfully built pyyaml Installing collected packages: pyyaml Attempting uninstall: pyyaml Found existing installation: PyYAML 6.0 Uninstalling PyYAML-6.0: Successfully uninstalled PyYAML-6.0 Successfully installed pyyaml-5.1 Looking in links: https://download.pytorch.org/whl/torch_stable.html Collecting torch==1.8.0+cu101 Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 763.5 MB 16 kB/s \u001b[?25hCollecting torchvision==0.9.0+cu101 Downloading https://download.pytorch.org/whl/cu101/torchvision-0.9.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (17.3 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.3 MB 795 kB/s \u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.21.6) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (4.2.0) Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu101) (7.1.2) Installing collected packages: torch, torchvision Attempting uninstall: torch Found existing installation: torch 1.11.0+cu113 Uninstalling torch-1.11.0+cu113: Successfully uninstalled torch-1.11.0+cu113 Attempting uninstall: torchvision Found existing installation: torchvision 0.12.0+cu113 Uninstalling torchvision-0.12.0+cu113: Successfully uninstalled torchvision-0.12.0+cu113 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.8.0+cu101 which is incompatible. torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m Successfully installed torch-1.8.0+cu101 torchvision-0.9.0+cu101 \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.3 MB 897 kB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 74 kB 2.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50 kB 5.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 147 kB 10.7 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 130 kB 33.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 749 kB 42.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 843 kB 34.7 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112 kB 46.3 MB/s \u001b[?25h Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone !pip install -q datasets \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 325 kB 5.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212 kB 43.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 136 kB 43.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1 MB 35.9 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 127 kB 45.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 144 kB 46.5 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94 kB 2.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 271 kB 47.1 MB/s \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m \u001b[?25h !sudo apt install tesseract-ocr !pip install -q pytesseract Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: libnvidia-common-460 nsight-compute-2020.2.0 Use 'sudo apt autoremove' to remove them. The following additional packages will be installed: tesseract-ocr-eng tesseract-ocr-osd The following NEW packages will be installed: tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd 0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded. Need to get 4,795 kB of archives. After this operation, 15.8 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB] Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB] Fetched 4,795 kB in 1s (3,752 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package tesseract-ocr-eng. (Reading database ... 155202 files and directories currently installed.) Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ... Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ... Selecting previously unselected package tesseract-ocr-osd. Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ... Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ... Selecting previously unselected package tesseract-ocr. Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ... Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ... Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ... Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ... Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3 MB 5.4 MB/s \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m \u001b[?25h !pip install Pillow==9.0.0 Collecting Pillow==9.0.0 Downloading Pillow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3 MB 5.3 MB/s \u001b[?25hInstalling collected packages: Pillow Attempting uninstall: Pillow Found existing installation: Pillow 9.1.0 Uninstalling Pillow-9.1.0: Successfully uninstalled Pillow-9.1.0 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m Successfully installed Pillow-9.0.0","title":"Install Packages"},{"location":"Doc_Visual_QA_and_Bill_extraction_demo/#docvqa-demo","text":"from transformers import LayoutLMv2Processor processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\") Downloading: 0%| | 0.00/135 [00:00<?, ?B/s] Downloading: 0%| | 0.00/226k [00:00<?, ?B/s] Downloading: 0%| | 0.00/707 [00:00<?, ?B/s] from transformers import AutoModelForQuestionAnswering model = AutoModelForQuestionAnswering.from_pretrained(\"tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa\") Downloading: 0%| | 0.00/2.69k [00:00<?, ?B/s] Downloading: 0%| | 0.00/765M [00:00<?, ?B/s] import torch device = torch.device(\"cuda\") model.to(device) def run_qa(image, question): # step 1: encoding encoding = processor(image, question, return_tensors=\"pt\", truncation=True) # step 2: forward pass for k,v in encoding.items(): encoding[k] = v.to(model.device) outputs = model(**encoding) # step 3: get start_logits and end_logits start_logits = outputs.start_logits end_logits = outputs.end_logits # step 4: get largest logit for both predicted_start_idx = start_logits.argmax(-1).item() predicted_end_idx = end_logits.argmax(-1).item() print(\"Predicted start idx:\", predicted_start_idx) print(\"Predicted end idx:\", predicted_end_idx) # step 5: decode the predicted answer return processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx:predicted_end_idx+1]) from PIL import Image image = Image.open(\"image-2.jpeg\").convert(\"RGB\") image question = \"Where to call?\" run_qa(image, question) Predicted start idx: 77 Predicted end idx: 82 'ext. 7240.'","title":"DocVQA Demo"},{"location":"Doc_Visual_QA_and_Bill_extraction_demo/#bill-information-extraction-demo","text":"import numpy as np from transformers import LayoutLMv2Processor, LayoutLMv2ForTokenClassification from datasets import load_dataset from PIL import Image, ImageDraw, ImageFont processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\") model = LayoutLMv2ForTokenClassification.from_pretrained(\"Theivaprakasham/layoutlmv2-finetuned-sroie\") Downloading: 0%| | 0.00/3.07k [00:00<?, ?B/s] Downloading: 0%| | 0.00/765M [00:00<?, ?B/s] dataset = load_dataset(\"darentang/sroie\", split=\"test\") Downloading builder script: 0%| | 0.00/4.25k [00:00<?, ?B/s] Downloading and preparing dataset sroie/sroie to /root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/26ed9374c9a15a1d2f44fd8886f679076e1a1fd7da2d53726d6e58a99436c506... Downloading data files: 0%| | 0/1 [00:00<?, ?it/s] Downloading data: 0%| | 0.00/456M [00:00<?, ?B/s] Extracting data files: 0%| | 0/1 [00:00<?, ?it/s] Generating train split: 0 examples [00:00, ? examples/s] Generating test split: 0 examples [00:00, ? examples/s] Dataset sroie downloaded and prepared to /root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/26ed9374c9a15a1d2f44fd8886f679076e1a1fd7da2d53726d6e58a99436c506. Subsequent calls will reuse this data. Image.open(dataset[50][\"image_path\"]).convert(\"RGB\").save(\"example1.png\") ls example1.png image-2.jpeg \u001b[0m\u001b[01;34msample_data\u001b[0m/ train.tar.gz train.tar.gz.1 Image.open(dataset[100][\"image_path\"]).convert(\"RGB\").save(\"example2.png\") # define id2label, label2color labels = dataset.features['ner_tags'].feature.names id2label = {v: k for v, k in enumerate(labels)} label2color = {'B-ADDRESS': 'blue', 'B-COMPANY': 'green', 'B-DATE': 'red', 'B-TOTAL': 'red', 'I-ADDRESS': \"blue\", 'I-COMPANY': 'green', 'I-DATE': 'red', 'I-TOTAL': 'red', 'O': 'green'} label2color = dict((k.lower(), v.lower()) for k,v in label2color.items()) def unnormalize_box(bbox, width, height): return [ width * (bbox[0] / 1000), height * (bbox[1] / 1000), width * (bbox[2] / 1000), height * (bbox[3] / 1000), ] def iob_to_label(label): return label image = Image.open(\"example2.png\").convert(\"RGB\") image width, height = image.size width, height (932, 2216) encoding = processor(image, truncation=True, return_offsets_mapping=True, return_tensors=\"pt\") offset_mapping = encoding.pop('offset_mapping') outputs = model(**encoding) # get predictions predictions = outputs.logits.argmax(-1).squeeze().tolist() token_boxes = encoding.bbox.squeeze().tolist() # only keep non-subword predictions is_subword = np.array(offset_mapping.squeeze().tolist())[:,0] != 0 true_predictions = [id2label[pred] for idx, pred in enumerate(predictions) if not is_subword[idx]] true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(token_boxes) if not is_subword[idx]] true_boxes [[0.0, 0.0, 0.0, 0.0], [430.584, 26.592000000000002, 523.7840000000001, 90.85600000000001], [680.36, 33.24, 722.3000000000001, 79.776], [163.1, 259.272, 296.376, 294.728], [315.94800000000004, 261.488, 462.272, 294.728], [480.91200000000003, 261.488, 581.568, 296.944], [596.48, 261.488, 752.124, 296.944], [335.52, 301.37600000000003, 582.5, 334.616], [258.16400000000004, 352.344, 319.67600000000004, 378.93600000000004], [332.724, 352.344, 386.78, 378.93600000000004], [398.896, 354.56, 499.552, 378.93600000000004], [514.464, 354.56, 660.788, 381.152], [229.272, 398.88, 323.404, 427.688], [335.52, 398.88, 480.91200000000003, 427.688], [501.41600000000005, 401.096, 685.952, 429.904], [144.46, 474.224, 203.176, 500.81600000000003], [217.156, 480.872, 219.952, 483.088], [233.0, 474.224, 433.38, 503.03200000000004], [481.844, 476.44, 543.356, 503.03200000000004], [556.404, 500.81600000000003, 558.268, 500.81600000000003], [571.316, 476.44, 772.6279999999999, 505.248], [275.872, 525.192, 342.976, 551.784], [355.092, 525.192, 385.84799999999996, 554.0], [398.896, 531.84, 400.76, 551.784], [414.74, 525.192, 641.2159999999999, 554.0], [32.620000000000005, 582.808, 876.0799999999999, 622.696], [46.6, 624.9119999999999, 83.88, 651.504], [95.996, 627.1279999999999, 147.256, 653.7199999999999], [45.668, 675.88, 121.16000000000001, 704.688], [132.344, 678.096, 227.408, 704.688], [239.524, 678.096, 329.928, 706.904], [340.18, 678.096, 553.608, 706.904], [45.668, 720.2, 93.2, 749.008], [108.11200000000001, 722.416, 117.432, 749.008], [133.27599999999998, 722.416, 234.864, 749.008], [247.912, 722.416, 341.11199999999997, 751.224], [354.16, 724.6320000000001, 370.93600000000004, 749.008], [44.736000000000004, 764.52, 158.44, 791.112], [172.42, 764.52, 259.096, 791.112], [271.212, 766.736, 364.41200000000003, 793.328], [45.668, 813.2719999999999, 189.19600000000003, 839.864], [202.244, 813.2719999999999, 387.712, 842.08], [44.736000000000004, 859.808, 244.184, 888.6160000000001], [46.6, 899.696, 169.624, 932.9359999999999], [285.192, 901.9119999999999, 483.708, 932.9359999999999], [602.072, 908.56, 677.564, 935.1519999999999], [703.66, 904.1279999999999, 875.1479999999999, 935.1519999999999], [45.668, 952.88, 161.236, 981.688], [259.096, 979.472, 261.89200000000005, 981.688], [289.852, 957.312, 378.39200000000005, 983.904], [598.344, 957.312, 675.6999999999999, 983.904], [703.66, 959.528, 833.208, 986.12], [47.532, 1003.8480000000001, 230.204, 1037.088], [259.096, 1014.928, 261.89200000000005, 1032.656], [610.46, 1008.2800000000001, 677.564, 1034.872], [58.716, 1054.816, 131.41199999999998, 1059.248], [527.512, 1076.9759999999999, 607.664, 1108.0], [700.864, 1059.248, 832.2760000000001, 1108.0], [32.620000000000005, 1110.216, 188.264, 1143.4560000000001], [189.19600000000003, 1112.432, 367.208, 1145.672], [369.072, 1112.432, 481.844, 1145.672], [520.988, 1110.216, 615.12, 1136.808], [0.0, 0.0, 932.0, 2216.0], [674.768, 1112.432, 779.1519999999999, 1136.808], [830.412, 1116.864, 877.944, 1150.104], [47.532, 1150.104, 139.79999999999998, 1174.48], [305.696, 1156.752, 312.22, 1176.6960000000001], [399.828, 1154.536, 470.66, 1178.912], [531.24, 1154.536, 602.072, 1178.912], [711.116, 1156.752, 782.88, 1181.1280000000002], [841.596, 1156.752, 877.944, 1181.1280000000002], [46.6, 1198.856, 127.68400000000001, 1223.2320000000002], [140.732, 1198.856, 230.204, 1223.2320000000002], [241.388, 1198.856, 280.532, 1227.6640000000002], [49.396, 1252.04, 138.868, 1274.1999999999998], [303.832, 1252.04, 315.94800000000004, 1276.416], [408.216, 1254.2559999999999, 462.272, 1276.416], [540.56, 1254.2559999999999, 595.548, 1278.6319999999998], [711.116, 1256.472, 740.94, 1280.848], [752.124, 1256.472, 781.948, 1280.848], [842.528, 1258.6879999999999, 877.944, 1280.848], [45.668, 1298.576, 118.364, 1322.952], [122.092, 1298.576, 194.78799999999998, 1322.952], [207.836, 1300.792, 259.096, 1325.168], [31.688000000000002, 1338.464, 59.648, 1342.896], [62.444, 1340.68, 166.828, 1373.92], [165.896, 1340.68, 229.272, 1380.568], [238.592, 1325.168, 876.0799999999999, 1378.352], [165.896, 1424.8880000000001, 235.796, 1451.48], [247.912, 1427.104, 327.132, 1453.6960000000001], [339.248, 1427.104, 488.368, 1462.5600000000002], [500.48400000000004, 1429.32, 578.772, 1460.344], [607.664, 1435.968, 610.46, 1455.912], [703.66, 1429.32, 786.608, 1455.912], [451.08799999999997, 1486.9360000000001, 579.704, 1513.528], [607.664, 1493.584, 610.46, 1513.528], [722.3000000000001, 1486.9360000000001, 786.608, 1515.7440000000001], [431.516, 1544.552, 501.41600000000005, 1571.144], [513.532, 1546.7679999999998, 579.704, 1571.144], [607.664, 1553.416, 610.46, 1571.144], [722.3000000000001, 1548.984, 785.6759999999999, 1573.36], [442.7, 1599.952, 578.772, 1635.408], [607.664, 1608.816, 610.46, 1628.76], [722.3000000000001, 1602.168, 785.6759999999999, 1628.76], [99.724, 1657.568, 179.876, 1684.16], [192.92399999999998, 1657.568, 279.59999999999997, 1684.16], [290.784, 1659.784, 448.292, 1690.808], [459.476, 1662.0, 490.232, 1686.376], [501.41600000000005, 1662.0, 580.636, 1695.24], [607.664, 1668.648, 611.392, 1688.592], [702.728, 1662.0, 785.6759999999999, 1690.808], [484.64000000000004, 1719.616, 554.54, 1746.208], [558.268, 1719.616, 611.392, 1746.208], [702.728, 1721.832, 785.6759999999999, 1748.424], [453.884, 1763.9360000000001, 558.268, 1797.1760000000002], [562.928, 1770.584, 611.392, 1790.528], [721.368, 1763.9360000000001, 785.6759999999999, 1792.7440000000001], [101.588, 1854.792, 169.624, 1890.248], [181.74, 1863.656, 355.092, 1892.464], [528.444, 1859.224, 594.616, 1863.656], [107.18, 1919.056, 169.624, 1950.08], [174.284, 1919.056, 258.16400000000004, 1952.296], [355.092, 1921.272, 381.188, 1954.512], [471.592, 1923.488, 620.712, 1956.728], [0.0, 0.0, 932.0, 2216.0], [697.136, 1923.488, 839.732, 1958.944], [110.908, 1963.376, 153.78, 1989.968], [359.752, 1965.592, 375.596, 1989.968], [537.764, 1967.808, 619.7800000000001, 1992.184], [775.424, 1967.808, 838.8000000000001, 1994.4], [298.24, 2012.1280000000002, 378.39200000000005, 2040.9360000000001], [393.304, 2020.992, 397.964, 2040.9360000000001], [534.968, 2014.344, 616.984, 2040.9360000000001], [775.424, 2016.5600000000002, 839.732, 2043.152], [94.132, 2098.5519999999997, 202.244, 2122.928], [214.36, 2098.5519999999997, 290.784, 2122.928], [300.104, 2098.5519999999997, 358.82, 2122.928], [369.072, 2100.768, 427.788, 2122.928], [439.904, 2100.768, 633.76, 2127.36], [645.876, 2102.984, 738.144, 2125.144], [749.3280000000001, 2102.984, 816.432, 2125.144], [932.0, 2216.0, 932.0, 2216.0]] true_predictions ['O', 'O', 'O', 'B-COMPANY', 'I-COMPANY', 'I-COMPANY', 'I-COMPANY', 'O', 'B-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'I-ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TOTAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] # draw predictions over the image draw = ImageDraw.Draw(image) font = ImageFont.load_default() for prediction, box in zip(true_predictions, true_boxes): predicted_label = iob_to_label(prediction).lower() draw.rectangle(box, outline=label2color[predicted_label]) draw.text((box[0]+10, box[1]-10), text=predicted_label, fill=label2color[predicted_label], font=font) image","title":"Bill Information extraction Demo"},{"location":"GPT_2_on_Onnx_CPU/","text":"!pip install onnxruntime==1.8.1 onnx==1.9.0 onnxconverter_common==1.8.1 transformers==4.8.2 psutil pytz pandas py-cpuinfo py3nvml Uninstalling transformers-4.14.1: Successfully uninstalled transformers-4.14.1 Attempting uninstall: onnxruntime Found existing installation: onnxruntime 1.10.0 Uninstalling onnxruntime-1.10.0: Successfully uninstalled onnxruntime-1.10.0 Attempting uninstall: onnxconverter-common Found existing installation: onnxconverter-common 1.9.0 Uninstalling onnxconverter-common-1.9.0: Successfully uninstalled onnxconverter-common-1.9.0 Successfully installed huggingface-hub-0.0.12 onnx-1.9.0 onnxconverter-common-1.8.1 onnxruntime-1.8.1 transformers-4.8.2 import os # Create a cache directory to store pretrained model. cache_dir = os.path.join(\".\", \"cache_models\") if not os.path.exists(cache_dir): os.makedirs(cache_dir) !lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 2 On-line CPU(s) list: 0,1 Thread(s) per core: 2 Core(s) per socket: 1 Socket(s): 1 NUMA node(s): 1 Vendor ID: GenuineIntel CPU family: 6 Model: 79 Model name: Intel(R) Xeon(R) CPU @ 2.20GHz Stepping: 0 CPU MHz: 2199.998 BogoMIPS: 4399.99 Hypervisor vendor: KVM Virtualization type: full L1d cache: 32K L1i cache: 32K L2 cache: 256K L3 cache: 56320K NUMA node0 CPU(s): 0,1 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities !pip install coloredlogs Requirement already satisfied: coloredlogs in /usr/local/lib/python3.7/dist-packages (15.0.1) Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.7/dist-packages (from coloredlogs) (10.0) from onnxruntime.transformers.gpt2_beamsearch_helper import Gpt2BeamSearchHelper, GPT2LMHeadModel_BeamSearchStep from transformers import AutoConfig import torch model_name_or_path = \"gpt2\" config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir) model = GPT2LMHeadModel_BeamSearchStep.from_pretrained(model_name_or_path, config=config, batch_size=1, beam_size=4, cache_dir=cache_dir) device = torch.device(\"cpu\") model.eval().to(device) print(model.config) num_attention_heads = model.config.n_head hidden_size = model.config.n_embd num_layer = model.config.n_layer GPT2Config { \"_name_or_path\": \"gpt2\", \"activation_function\": \"gelu_new\", \"architectures\": [ \"GPT2LMHeadModel\" ], \"attn_pdrop\": 0.1, \"batch_size\": 1, \"beam_size\": 4, \"bos_token_id\": 50256, \"embd_pdrop\": 0.1, \"eos_token_id\": 50256, \"gradient_checkpointing\": false, \"initializer_range\": 0.02, \"layer_norm_epsilon\": 1e-05, \"model_type\": \"gpt2\", \"n_ctx\": 1024, \"n_embd\": 768, \"n_head\": 12, \"n_inner\": null, \"n_layer\": 12, \"n_positions\": 1024, \"resid_pdrop\": 0.1, \"scale_attn_weights\": true, \"summary_activation\": null, \"summary_first_dropout\": 0.1, \"summary_proj_to_labels\": true, \"summary_type\": \"cls_index\", \"summary_use_proj\": true, \"task_specific_params\": { \"text-generation\": { \"do_sample\": true, \"max_length\": 50 } }, \"transformers_version\": \"4.8.2\", \"use_cache\": true, \"vocab_size\": 50257 } onnx_model_path = \"gpt2_one_step_search.onnx\" Gpt2BeamSearchHelper.export_onnx(model, device, onnx_model_path) # add parameter use_external_data_format=True when model size > 2 GB /usr/local/lib/python3.7/dist-packages/onnxruntime/transformers/gpt2_beamsearch_helper.py:91: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). selected_input_seq = selected_index_flat // self.config.beam_size /usr/local/lib/python3.7/dist-packages/torch/onnx/utils.py:100: UserWarning: `example_outputs' is deprecated and ignored. Will be removed in next PyTorch release. warnings.warn(\"`example_outputs' is deprecated and ignored. Will be removed in \" /usr/local/lib/python3.7/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers. warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \" /usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:698: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! assert batch_size > 0, \"batch_size has to be defined and > 0\" /usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:249: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results). past_key, past_value = layer_past /usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:181: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! attn_weights = attn_weights / (float(value.size(-1)) ** 0.5) import onnxruntime import numpy from transformers import AutoTokenizer EXAMPLE_Text = ['best hotel in bay area.'] def get_tokenizer(model_name_or_path, cache_dir): tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir) tokenizer.padding_side = \"left\" tokenizer.pad_token = tokenizer.eos_token #okenizer.add_special_tokens({'pad_token': '[PAD]'}) return tokenizer def get_example_inputs(prompt_text=EXAMPLE_Text): tokenizer = get_tokenizer(model_name_or_path, cache_dir) encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True) input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64) attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32) position_ids = (attention_mask.long().cumsum(-1) - 1) position_ids.masked_fill_(position_ids < 0, 0) #Empty Past State for generating first word empty_past = [] batch_size = input_ids.size(0) sequence_length = input_ids.size(1) past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads] for i in range(num_layer): empty_past.append(torch.empty(past_shape).type(torch.float32).to(device)) return input_ids, attention_mask, position_ids, empty_past input_ids, attention_mask, position_ids, empty_past = get_example_inputs() beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long() input_log_probs = torch.zeros([input_ids.shape[0], 1]) input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool) prev_step_scores = torch.zeros([input_ids.shape[0], 1]) onnx_model_path = \"gpt2_one_step_search.onnx\" session = onnxruntime.InferenceSession(onnx_model_path) ort_inputs = { 'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()), 'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()), 'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()), 'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()), 'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()), 'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()), 'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()), 'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()), } for i, past_i in enumerate(empty_past): ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy()) ort_outputs = session.run(None, ort_inputs) def inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores, step, context_len): output_shapes = Gpt2BeamSearchHelper.get_output_shapes(batch_size=1, context_len=context_len, past_sequence_length=past[0].size(3), sequence_length=input_ids.size(1), beam_size=4, step=step, config=config, model_class=\"GPT2LMHeadModel_BeamSearchStep\") output_buffers = Gpt2BeamSearchHelper.get_output_buffers(output_shapes, device) io_binding = Gpt2BeamSearchHelper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores) session.run_with_iobinding(io_binding) outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes, return_numpy=False) return outputs input_ids, attention_mask, position_ids, empty_past = get_example_inputs() beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long() input_log_probs = torch.zeros([input_ids.shape[0], 1]) input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool) prev_step_scores = torch.zeros([input_ids.shape[0], 1]) outputs = inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, empty_past, beam_select_idx, input_log_probs, input_unfinished_sents, input_ids, prev_step_scores, 0, input_ids.shape[-1]) assert torch.eq(outputs[-2], torch.from_numpy(ort_outputs[-2])).all() print(\"IO Binding result is good\") IO Binding result is good def update(output, step, batch_size, beam_size, context_length, prev_attention_mask, device): \"\"\" Update the inputs for next inference. \"\"\" last_state = (torch.from_numpy(output[0]).to(device) if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu()) input_ids = last_state.view(batch_size * beam_size, -1).to(device) input_unfinished_sents_id = -3 prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray) else output[-2].clone().detach().to(device)) position_ids = (torch.tensor([context_length + step - 1 ]).unsqueeze(0).repeat(batch_size * beam_size, 1).to(device)) if prev_attention_mask.shape[0] != (batch_size * beam_size): prev_attention_mask = prev_attention_mask.repeat(batch_size * beam_size, 1) attention_mask = torch.cat( [ prev_attention_mask, torch.ones([batch_size * beam_size, 1]).type_as(prev_attention_mask), ], 1, ).to(device) beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance( output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id - 2].clone().detach().to(device)) input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance( output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id - 1].clone().detach().to(device)) input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance( output[input_unfinished_sents_id], numpy.ndarray) else output[input_unfinished_sents_id].clone().detach().to(device)) prev_step_scores = (torch.from_numpy(output[-1]).to(device) if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device)) past = [] if isinstance(output[1], tuple): # past in torch output is tuple past = list(output[1]) else: for i in range(model.config.n_layer): past_i = (torch.from_numpy(output[i + 1]) if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach()) past.append(past_i.to(device)) inputs = { 'input_ids': input_ids, 'attention_mask' : attention_mask, 'position_ids': position_ids, 'beam_select_idx': beam_select_idx, 'input_log_probs': input_log_probs, 'input_unfinished_sents': input_unfinished_sents, 'prev_step_results': prev_step_results, 'prev_step_scores': prev_step_scores, } ort_inputs = { 'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()), 'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()), 'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()), 'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()), 'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()), 'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()), 'prev_step_results': numpy.ascontiguousarray(prev_step_results.cpu().numpy()), 'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()), } for i, past_i in enumerate(past): ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy()) return inputs, ort_inputs, past def test_generation(tokenizer, input_text, use_onnxruntime_io, ort_session = None, num_tokens_to_produce = 30): print(\"Text generation using\", \"OnnxRuntime with IO binding\" if use_onnxruntime_io else \"OnnxRuntime\", \"...\") input_ids, attention_mask, position_ids, past = get_example_inputs(input_text) beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long() input_log_probs = torch.zeros([input_ids.shape[0], 1]) input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool) prev_step_scores = torch.zeros([input_ids.shape[0], 1]) inputs = { 'input_ids': input_ids, 'attention_mask' : attention_mask, 'position_ids': position_ids, 'beam_select_idx': beam_select_idx, 'input_log_probs': input_log_probs, 'input_unfinished_sents': input_unfinished_sents, 'prev_step_results': input_ids, 'prev_step_scores': prev_step_scores, } ort_inputs = { 'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()), 'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()), 'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()), 'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()), 'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()), 'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()), 'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()), 'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()), } for i, past_i in enumerate(past): ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy()) batch_size = input_ids.size(0) beam_size = 4 context_length = input_ids.size(-1) for step in range(num_tokens_to_produce): if use_onnxruntime_io: outputs = inference_with_io_binding(ort_session, config, inputs['input_ids'], inputs['position_ids'], inputs['attention_mask'], past, inputs['beam_select_idx'], inputs['input_log_probs'], inputs['input_unfinished_sents'], inputs['prev_step_results'], inputs['prev_step_scores'], step, context_length) else: outputs = ort_session.run(None, ort_inputs) inputs, ort_inputs, past = update(outputs, step, batch_size, beam_size, context_length, inputs['attention_mask'], device) if not inputs['input_unfinished_sents'].any(): break print(\"------------\") print(tokenizer.decode(inputs['prev_step_results'][0], skip_special_tokens=True)) tokenizer = get_tokenizer(model_name_or_path, cache_dir) input_text = EXAMPLE_Text test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session) Text generation using OnnxRuntime ... ------------ best hotel in bay area. \"It's a great place to stay,\" he said. test_generation(tokenizer, input_text, use_onnxruntime_io=True, ort_session=session) Text generation using OnnxRuntime with IO binding ... ------------ best hotel in bay area. \"It's a great place to stay,\" he said.","title":"GPT 2 on Onnx CPU"},{"location":"Generic_Transformer_Classification/","text":"!pip install transformers Collecting transformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 778kB 2.8MB/s \u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Collecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 13.3MB/s \u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7) Collecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 19.7MB/s \u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Collecting tokenizers==0.8.1.rc1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 20.6MB/s \u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0) Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=6bda503b3bbdbf7626ff38a3e3235c3a20e948d97c40c78a681f5c87b90ed237 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2 import os, pandas as pd from sklearn.model_selection import train_test_split import logging from transformers import * import torch from torch.utils.data import Dataset, DataLoader import torch.nn as nn from tqdm.autonotebook import tqdm 1. Set Configuration class Config: train_file = './data.csv' eval_file = './eval.csv' max_seq_len = 128 batch_size = 32 epochs = 5 model_name = 'bert-base-uncased' learning_rate = 2e-5 n_classes = 3 device = 'cpu' flags = Config 2. Build Dataset Pipeline class TextLabelDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_len): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.texts) def __getitem__(self, item): text = str(self.texts[item]) label = self.labels[item] encoding = self.tokenizer.encode_plus( text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True ) return { 'texts': text, 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'targets': torch.tensor(label, dtype=torch.long) } def create_data_loader(df, tokenizer, max_len, batch_size, is_prediction=False): if isinstance(df, str): df = pd.read_csv(df) else: pass if is_prediction: ds = TextLabelDataset( texts=df.text.to_numpy(), labels=np.array([-1]*len(df.text.values)), tokenizer=tokenizer, max_len=max_len ) else: ds = TextLabelDataset( texts=df.text.to_numpy(), labels=df.labels.to_numpy(), tokenizer=tokenizer, max_len=max_len ) return DataLoader( ds, batch_size=batch_size, num_workers=4 ) 3. Build Model class Classifier(nn.Module): def __init__(self, model_name, n_classes): super(Classifier, self).__init__() self.bert = AutoModel.from_pretrained(model_name) self.drop = nn.Dropout(p=0.3) self.out = nn.Linear(self.bert.config.hidden_size, n_classes) def forward(self, input_ids, attention_mask): _, pooled_output = self.bert( input_ids=input_ids, attention_mask=attention_mask ) output = self.drop(pooled_output) return self.out(output) class ClassificationModel: def __init__(self, flags): self.flags = flags self.tokenizer = BertTokenizer.from_pretrained(self.flags.model_name) self.model = Classifier(self.flags.model_name, self.flags.n_classes) self.model = self.model.to(self.flags.device) def train(self): train_data_loader = create_data_loader(self.flags.train_file, self.tokenizer, self.flags.max_seq_len, self.flags.batch_size) val_data_loader = create_data_loader(self.flags.eval_file, self.tokenizer, self.flags.max_seq_len, self.flags.batch_size) optimizer = AdamW(self.model.parameters(), lr=self.flags.learning_rate, correct_bias=False) total_steps = len(train_data_loader) * self.flags.epochs scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=total_steps ) loss_fn = nn.CrossEntropyLoss().to(self.flags.device) history = defaultdict(list) best_accuracy = 0 if isinstance(self.flags.train_file, str): train_df = pd.read_csv(self.flags.train_file) if isinstance(self.flags.eval_file, str): eval_df = pd.read_csv(self.flags.eval_file) for epoch in range(self.flags.epochs): print(f'Epoch {epoch + 1}/{self.flags.epochs}') print('-' * 10) train_acc, train_loss = self.train_epoch( self.model, train_data_loader, loss_fn, optimizer, self.flags.device, scheduler, len(train_df) ) print(f'Train loss {train_loss} accuracy {train_acc}') val_acc, val_loss = self.eval_model( self.model, val_data_loader, loss_fn, self.flags.device, len(eval_df) ) print(f'Val loss {val_loss} accuracy {val_acc}') print() history['train_acc'].append(train_acc) history['train_loss'].append(train_loss) history['val_acc'].append(val_acc) history['val_loss'].append(val_loss) if val_acc > best_accuracy: torch.save(self.model.state_dict(), 'best_model_state.bin') best_accuracy = val_acc def train_epoch(self, model, data_loader, loss_fn, optimizer, device, scheduler, n_examples): model = model.train() losses = [] correct_predictions = 0 tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\") for bi, d in enumerate(tk0): input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() scheduler.step() optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) def eval_model(self, model, data_loader, loss_fn, device, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Evaluating\") for bi, d in enumerate(tk0): input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses) Download Data and Preparation !wget https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv --2020-08-26 15:57:14-- https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 23677025 (23M) [text/plain] Saving to: \u2018imdb_tr.csv\u2019 imdb_tr.csv 100%[===================>] 22.58M 39.9MB/s in 0.6s 2020-08-26 15:57:16 (39.9 MB/s) - \u2018imdb_tr.csv\u2019 saved [23677025/23677025] data = pd.read_csv('imdb_tr.csv', encoding = \"ISO-8859-1\") data.columns = ['row_Number', 'text', 'labels'] train_data = data.sample(1000) test_data = data.sample(100) train_data.to_csv('data.csv', index=False) test_data.to_csv('eval.csv', index=False) Training from collections import defaultdict import numpy as np class Config: train_file = './data.csv' eval_file = './eval.csv' max_seq_len = 128 batch_size = 32 epochs = 5 model_name = 'bert-base-uncased' learning_rate = 2e-5 n_classes = 2 device = 'cuda' flags = Config classification = ClassificationModel(flags) classification.train() Epoch 1/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.6540139000862837 accuracy 0.622 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.4141501262784004 accuracy 0.78 Epoch 2/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.3276493112789467 accuracy 0.864 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.3254726273007691 accuracy 0.87 Epoch 3/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.12970392164424993 accuracy 0.9530000000000001 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.4319960339926183 accuracy 0.8300000000000001 Epoch 4/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.0639086696319282 accuracy 0.982 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.5208611574489623 accuracy 0.8300000000000001 Epoch 5/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.01748604617023375 accuracy 0.996 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.4388579736405518 accuracy 0.9","title":"Generic Transformer Classification"},{"location":"Generic_Transformer_Classification/#1-set-configuration","text":"class Config: train_file = './data.csv' eval_file = './eval.csv' max_seq_len = 128 batch_size = 32 epochs = 5 model_name = 'bert-base-uncased' learning_rate = 2e-5 n_classes = 3 device = 'cpu' flags = Config","title":"1. Set Configuration"},{"location":"Generic_Transformer_Classification/#2-build-dataset-pipeline","text":"class TextLabelDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_len): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.texts) def __getitem__(self, item): text = str(self.texts[item]) label = self.labels[item] encoding = self.tokenizer.encode_plus( text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True ) return { 'texts': text, 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'targets': torch.tensor(label, dtype=torch.long) } def create_data_loader(df, tokenizer, max_len, batch_size, is_prediction=False): if isinstance(df, str): df = pd.read_csv(df) else: pass if is_prediction: ds = TextLabelDataset( texts=df.text.to_numpy(), labels=np.array([-1]*len(df.text.values)), tokenizer=tokenizer, max_len=max_len ) else: ds = TextLabelDataset( texts=df.text.to_numpy(), labels=df.labels.to_numpy(), tokenizer=tokenizer, max_len=max_len ) return DataLoader( ds, batch_size=batch_size, num_workers=4 )","title":"2. Build Dataset Pipeline"},{"location":"Generic_Transformer_Classification/#3-build-model","text":"class Classifier(nn.Module): def __init__(self, model_name, n_classes): super(Classifier, self).__init__() self.bert = AutoModel.from_pretrained(model_name) self.drop = nn.Dropout(p=0.3) self.out = nn.Linear(self.bert.config.hidden_size, n_classes) def forward(self, input_ids, attention_mask): _, pooled_output = self.bert( input_ids=input_ids, attention_mask=attention_mask ) output = self.drop(pooled_output) return self.out(output) class ClassificationModel: def __init__(self, flags): self.flags = flags self.tokenizer = BertTokenizer.from_pretrained(self.flags.model_name) self.model = Classifier(self.flags.model_name, self.flags.n_classes) self.model = self.model.to(self.flags.device) def train(self): train_data_loader = create_data_loader(self.flags.train_file, self.tokenizer, self.flags.max_seq_len, self.flags.batch_size) val_data_loader = create_data_loader(self.flags.eval_file, self.tokenizer, self.flags.max_seq_len, self.flags.batch_size) optimizer = AdamW(self.model.parameters(), lr=self.flags.learning_rate, correct_bias=False) total_steps = len(train_data_loader) * self.flags.epochs scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=0, num_training_steps=total_steps ) loss_fn = nn.CrossEntropyLoss().to(self.flags.device) history = defaultdict(list) best_accuracy = 0 if isinstance(self.flags.train_file, str): train_df = pd.read_csv(self.flags.train_file) if isinstance(self.flags.eval_file, str): eval_df = pd.read_csv(self.flags.eval_file) for epoch in range(self.flags.epochs): print(f'Epoch {epoch + 1}/{self.flags.epochs}') print('-' * 10) train_acc, train_loss = self.train_epoch( self.model, train_data_loader, loss_fn, optimizer, self.flags.device, scheduler, len(train_df) ) print(f'Train loss {train_loss} accuracy {train_acc}') val_acc, val_loss = self.eval_model( self.model, val_data_loader, loss_fn, self.flags.device, len(eval_df) ) print(f'Val loss {val_loss} accuracy {val_acc}') print() history['train_acc'].append(train_acc) history['train_loss'].append(train_loss) history['val_acc'].append(val_acc) history['val_loss'].append(val_loss) if val_acc > best_accuracy: torch.save(self.model.state_dict(), 'best_model_state.bin') best_accuracy = val_acc def train_epoch(self, model, data_loader, loss_fn, optimizer, device, scheduler, n_examples): model = model.train() losses = [] correct_predictions = 0 tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\") for bi, d in enumerate(tk0): input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() scheduler.step() optimizer.zero_grad() return correct_predictions.double() / n_examples, np.mean(losses) def eval_model(self, model, data_loader, loss_fn, device, n_examples): model = model.eval() losses = [] correct_predictions = 0 with torch.no_grad(): tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Evaluating\") for bi, d in enumerate(tk0): input_ids = d[\"input_ids\"].to(device) attention_mask = d[\"attention_mask\"].to(device) targets = d[\"targets\"].to(device) outputs = model( input_ids=input_ids, attention_mask=attention_mask ) _, preds = torch.max(outputs, dim=1) loss = loss_fn(outputs, targets) correct_predictions += torch.sum(preds == targets) losses.append(loss.item()) return correct_predictions.double() / n_examples, np.mean(losses)","title":"3. Build Model"},{"location":"Generic_Transformer_Classification/#download-data-and-preparation","text":"!wget https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv --2020-08-26 15:57:14-- https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 23677025 (23M) [text/plain] Saving to: \u2018imdb_tr.csv\u2019 imdb_tr.csv 100%[===================>] 22.58M 39.9MB/s in 0.6s 2020-08-26 15:57:16 (39.9 MB/s) - \u2018imdb_tr.csv\u2019 saved [23677025/23677025] data = pd.read_csv('imdb_tr.csv', encoding = \"ISO-8859-1\") data.columns = ['row_Number', 'text', 'labels'] train_data = data.sample(1000) test_data = data.sample(100) train_data.to_csv('data.csv', index=False) test_data.to_csv('eval.csv', index=False)","title":"Download Data and Preparation"},{"location":"Generic_Transformer_Classification/#training","text":"from collections import defaultdict import numpy as np class Config: train_file = './data.csv' eval_file = './eval.csv' max_seq_len = 128 batch_size = 32 epochs = 5 model_name = 'bert-base-uncased' learning_rate = 2e-5 n_classes = 2 device = 'cuda' flags = Config classification = ClassificationModel(flags) classification.train() Epoch 1/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.6540139000862837 accuracy 0.622 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.4141501262784004 accuracy 0.78 Epoch 2/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.3276493112789467 accuracy 0.864 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.3254726273007691 accuracy 0.87 Epoch 3/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.12970392164424993 accuracy 0.9530000000000001 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.4319960339926183 accuracy 0.8300000000000001 Epoch 4/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.0639086696319282 accuracy 0.982 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.5208611574489623 accuracy 0.8300000000000001 Epoch 5/5 ---------- HBox(children=(FloatProgress(value=0.0, description='Training', max=32.0, style=ProgressStyle(description_widt\u2026 Train loss 0.01748604617023375 accuracy 0.996 HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=4.0, style=ProgressStyle(description_wid\u2026 Val loss 0.4388579736405518 accuracy 0.9","title":"Training"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/","text":"Question Answering with a Fine-Tuned BERT by Ankur Singh Part 1: How BERT is applied to Question Answering The SQuAD v1.1 Benchmark When someone mentions \"Question Answering\" as an application of BERT, what they are really referring to is applying BERT to the Stanford Question Answering Dataset (SQuAD). The task posed by the SQuAD benchmark is a little different than you might think. Given a question, and a passage of text containing the answer , BERT needs to highlight the \"span\" of text corresponding to the correct answer. The SQuAD homepage has a fantastic tool for exploring the questions and reference text for this dataset, and even shows the predictions made by top-performing models. For example, here are some interesting examples on the topic of Super Bowl 50. BERT Input Format To feed a QA task into BERT, we pack both the question and the reference text into the input. The two pieces of text are separated by the special [SEP] token. BERT also uses \"Segment Embeddings\" to differentiate the question from the reference text. These are simply two embeddings (for segments \"A\" and \"B\") that BERT learned, and which it adds to the token embeddings before feeding them into the input layer. Start & End Token Classifiers BERT needs to highlight a \"span\" of text containing the answer--this is represented as simply predicting which token marks the start of the answer, and which token marks the end. For every token in the text, we feed its final embedding into the start token classifier. The start token classifier only has a single set of weights (represented by the blue \"start\" rectangle in the above illustration) which it applies to every word. After taking the dot product between the output embeddings and the 'start' weights, we apply the softmax activation to produce a probability distribution over all of the words. Whichever word has the highest probability of being the start token is the one that we pick. We repeat this process for the end token--we have a separate weight vector this. Part 2: Example Code In the example code below, we'll be downloading a model that's already been fine-tuned for question answering, and try it out on our own text. If you do want to fine-tune on your own dataset, it is possible to fine-tune BERT for question answering yourself. See run_squad.py in the transformers library. However,you may find that the below \"fine-tuned-on-squad\" model already does a good job, even if your text is from a different domain. Note: The example code in this Notebook is a commented and expanded version of the short example provided in the transformers documentation here . 1. Install huggingface transformers library This example uses the transformers library by huggingface. We'll start by installing the package. !pip install transformers Collecting transformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 778kB 8.6MB/s \u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Collecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 14.4MB/s \u001b[?25hCollecting tokenizers==0.8.1.rc1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 43.4MB/s \u001b[?25hCollecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 44.5MB/s \u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0) Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=09966dce42bbfaa2aa5abc0f5f82653b74458d122a6fefeb7c895da49b40ce50 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2 import torch 2. Load Fine-Tuned BERT-large For Question Answering we use the BertForQuestionAnswering class from the transformers library. This class supports fine-tuning, but for this example we will keep things simpler and load a BERT model that has already been fine-tuned for the SQuAD benchmark. The transformers library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation here . For Question Answering, they have a version of BERT-large that has already been fine-tuned for the SQuAD benchmark. BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance. (Note that this download is not using your own network bandwidth--it's between the Google instance and wherever the model is stored on the web). Note: I believe this model was trained on version 1 of SQuAD, since it's not outputting whether the question is \"impossible\" to answer from the text (which is part of the task in v2 of SQuAD). from transformers import BertForQuestionAnswering model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443.0, style=ProgressStyle(description_\u2026 HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1340675298.0, style=ProgressStyle(descr\u2026 Load the tokenizer as well. Side note: Apparently the vocabulary of this model is identicaly to the one in bert-base-uncased. You can load the tokenizer from bert-base-uncased and that works just as well. from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti\u2026 3. Ask a Question Now we're ready to feed in an example! A QA example consists of a question and a passage of text containing the answer to that question. Let's try an example using the text in this tutorial! question = \"How many parameters does BERT-large have?\" answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\" We'll need to run the BERT tokenizer against both the question and the answer_text . To feed these into BERT, we actually concatenate them together and place the special [SEP] token in between. # Apply the tokenizer to the input text, treating them as a text-pair. input_ids = tokenizer.encode(question, answer_text) print('The input has a total of {:} tokens.'.format(len(input_ids))) The input has a total of 70 tokens. Just to see exactly what the tokenizer is doing, let's print out the tokens with their IDs. # BERT only needs the token IDs, but for the purpose of inspecting the # tokenizer's behavior, let's also get the token strings and display them. tokens = tokenizer.convert_ids_to_tokens(input_ids) # For each token and its id... for token, id in zip(tokens, input_ids): # If this is the [SEP] token, add some space around it to make it stand out. if id == tokenizer.sep_token_id: print('') # Print the token string and its ID in two columns. print('{:<12} {:>6,}'.format(token, id)) if id == tokenizer.sep_token_id: print('') [CLS] 101 how 2,129 many 2,116 parameters 11,709 does 2,515 bert 14,324 - 1,011 large 2,312 have 2,031 ? 1,029 [SEP] 102 bert 14,324 - 1,011 large 2,312 is 2,003 really 2,428 big 2,502 . 1,012 . 1,012 . 1,012 it 2,009 has 2,038 24 2,484 - 1,011 layers 9,014 and 1,998 an 2,019 em 7,861 ##bed 8,270 ##ding 4,667 size 2,946 of 1,997 1 1,015 , 1,010 02 6,185 ##4 2,549 , 1,010 for 2,005 a 1,037 total 2,561 of 1,997 340 16,029 ##m 2,213 parameters 11,709 ! 999 altogether 10,462 it 2,009 is 2,003 1 1,015 . 1,012 34 4,090 ##gb 18,259 , 1,010 so 2,061 expect 5,987 it 2,009 to 2,000 take 2,202 a 1,037 couple 3,232 minutes 2,781 to 2,000 download 8,816 to 2,000 your 2,115 cola 15,270 ##b 2,497 instance 6,013 . 1,012 [SEP] 102 We've concatenated the question and answer_text together, but BERT still needs a way to distinguish them. BERT has two special \"Segment\" embeddings, one for segment \"A\" and one for segment \"B\". Before the word embeddings go into the BERT layers, the segment A embedding needs to be added to the question tokens, and the segment B embedding needs to be added to each of the answer_text tokens. These additions are handled for us by the transformer library, and all we need to do is specify a '0' or '1' for each token. Note: In the transformers library, huggingface likes to call these token_type_ids , but I'm going with segment_ids since this seems clearer, and is consistent with the BERT paper. # Search the input_ids for the first instance of the `[SEP]` token. sep_index = input_ids.index(tokenizer.sep_token_id) # The number of segment A tokens includes the [SEP] token istelf. num_seg_a = sep_index + 1 # The remainder are segment B. num_seg_b = len(input_ids) - num_seg_a # Construct the list of 0s and 1s. segment_ids = [0]*num_seg_a + [1]*num_seg_b # There should be a segment_id for every input token. assert len(segment_ids) == len(input_ids) Side Note: Where's the padding? The original example code does not perform any padding. I suspect that this is because we are only feeding in a single example . If we instead fed in a batch of examples, then we would need to pad or truncate all of the samples in the batch to a single length, and supply an attention mask to tell BERT to ignore the padding tokens. We're ready to feed our example into the model! # Run our example through the model. start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text. token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text Now we can highlight the answer just by looking at the most probable start and end words. # Find the tokens with the highest `start` and `end` scores. answer_start = torch.argmax(start_scores) answer_end = torch.argmax(end_scores) # Combine the tokens in the answer and print it out. answer = ' '.join(tokens[answer_start:answer_end+1]) print('Answer: \"' + answer + '\"') Answer: \"340 ##m\" It got it right! Awesome :) Side Note: It's a little naive to pick the highest scores for start and end--what if it predicts an end word that's before the start word?! The correct implementation is to pick the highest total score for which end >= start. With a little more effort, we can reconstruct any words that got broken down into subwords. # Start with the first token. answer = tokens[answer_start] # Select the remaining answer tokens and join them with whitespace. for i in range(answer_start + 1, answer_end + 1): # If it's a subword token, then recombine it with the previous token. if tokens[i][0:2] == '##': answer += tokens[i][2:] # Otherwise, add a space then the token. else: answer += ' ' + tokens[i] print('Answer: \"' + answer + '\"') Answer: \"340m\" 4. Visualizing Scores I was curious to see what the scores were for all of the words. The following cells generate bar plots showing the start and end scores for every word in the input. import matplotlib.pyplot as plt import seaborn as sns # Use plot styling from seaborn. sns.set(style='darkgrid') # Increase the plot size and font size. #sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (16,8) Retrieve all of the start and end scores, and use all of the tokens as x-axis labels. # Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays. s_scores = start_scores.detach().numpy().flatten() e_scores = end_scores.detach().numpy().flatten() # We'll use the tokens as the x-axis labels. In order to do that, they all need # to be unique, so we'll add the token index to the end of each one. token_labels = [] for (i, token) in enumerate(tokens): token_labels.append('{:} - {:>2}'.format(token, i)) Create a bar plot showing the score for every input word being the \"start\" word. # Create a barplot showing the start word score for all of the tokens. ax = sns.barplot(x=token_labels, y=s_scores, ci=None) # Turn the xlabels vertical. ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\") # Turn on the vertical grid to help align words to scores. ax.grid(True) plt.title('Start Word Scores') plt.show() Create a second bar plot showing the score for every input word being the \"end\" word. # Create a barplot showing the end word score for all of the tokens. ax = sns.barplot(x=token_labels, y=e_scores, ci=None) # Turn the xlabels vertical. ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\") # Turn on the vertical grid to help align words to scores. ax.grid(True) plt.title('End Word Scores') plt.show() Alternate View I also tried visualizing both the start and end scores on a single bar plot, but I think it may actually be more confusing then seeing them separately. import pandas as pd # Store the tokens and scores in a DataFrame. # Each token will have two rows, one for its start score and one for its end # score. The \"marker\" column will differentiate them. A little wacky, I know. scores = [] for (i, token_label) in enumerate(token_labels): # Add the token's start score as one row. scores.append({'token_label': token_label, 'score': s_scores[i], 'marker': 'start'}) # Add the token's end score as another row. scores.append({'token_label': token_label, 'score': e_scores[i], 'marker': 'end'}) df = pd.DataFrame(scores) # Draw a grouped barplot to show start and end scores for each word. # The \"hue\" parameter is where we tell it which datapoints belong to which # of the two series. g = sns.catplot(x=\"token_label\", y=\"score\", hue=\"marker\", data=df, kind=\"bar\", height=6, aspect=4) # Turn the xlabels vertical. g.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha=\"center\") # Turn on the vertical grid to help align words to scores. g.ax.grid(True) 5. More Examples Turn the QA process into a function so we can easily try out other examples. def answer_question(question, answer_text): ''' Takes a `question` string and an `answer_text` string (which contains the answer), and identifies the words within the `answer_text` that are the answer. Prints them out. ''' # ======== Tokenize ======== # Apply the tokenizer to the input text, treating them as a text-pair. input_ids = tokenizer.encode(question, answer_text) # Report how long the input sequence is. print('Query has {:,} tokens.\\n'.format(len(input_ids))) # ======== Set Segment IDs ======== # Search the input_ids for the first instance of the `[SEP]` token. sep_index = input_ids.index(tokenizer.sep_token_id) # The number of segment A tokens includes the [SEP] token istelf. num_seg_a = sep_index + 1 # The remainder are segment B. num_seg_b = len(input_ids) - num_seg_a # Construct the list of 0s and 1s. segment_ids = [0]*num_seg_a + [1]*num_seg_b # There should be a segment_id for every input token. assert len(segment_ids) == len(input_ids) # ======== Evaluate ======== # Run our example question through the model. start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text. token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text # ======== Reconstruct Answer ======== # Find the tokens with the highest `start` and `end` scores. answer_start = torch.argmax(start_scores) answer_end = torch.argmax(end_scores) # Get the string versions of the input tokens. tokens = tokenizer.convert_ids_to_tokens(input_ids) # Start with the first token. answer = tokens[answer_start] # Select the remaining answer tokens and join them with whitespace. for i in range(answer_start + 1, answer_end + 1): # If it's a subword token, then recombine it with the previous token. if tokens[i][0:2] == '##': answer += tokens[i][2:] # Otherwise, add a space then the token. else: answer += ' ' + tokens[i] print('Answer: \"' + answer + '\"') As our reference text, I've taken the Abstract of the BERT paper . import textwrap # Wrap text to 80 characters. wrapper = textwrap.TextWrapper(width=80) bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\" print(wrapper.fill(bert_abstract)) We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). Ask BERT what its name stands for (the answer is in the first sentence of the abstract). question = \"What does the 'B' in BERT stand for?\" answer_question(question, bert_abstract) Query has 258 tokens. Answer: \"bidirectional encoder representations from transformers\" Ask BERT about example applications of itself :) The answer to the question comes from this passage from the abstract: \"...BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.\" question = \"What are some example applications of BERT?\" answer_question(question, bert_abstract) Query has 255 tokens. Answer: \"question answering and language inference\"","title":"Question Answering with a Fine Tuned BERT"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#question-answering-with-a-fine-tuned-bert","text":"by Ankur Singh","title":"Question Answering with a Fine-Tuned BERT"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#part-1-how-bert-is-applied-to-question-answering","text":"","title":"Part 1: How BERT is applied to Question Answering"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#the-squad-v11-benchmark","text":"When someone mentions \"Question Answering\" as an application of BERT, what they are really referring to is applying BERT to the Stanford Question Answering Dataset (SQuAD). The task posed by the SQuAD benchmark is a little different than you might think. Given a question, and a passage of text containing the answer , BERT needs to highlight the \"span\" of text corresponding to the correct answer. The SQuAD homepage has a fantastic tool for exploring the questions and reference text for this dataset, and even shows the predictions made by top-performing models. For example, here are some interesting examples on the topic of Super Bowl 50.","title":"The SQuAD v1.1 Benchmark"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#bert-input-format","text":"To feed a QA task into BERT, we pack both the question and the reference text into the input. The two pieces of text are separated by the special [SEP] token. BERT also uses \"Segment Embeddings\" to differentiate the question from the reference text. These are simply two embeddings (for segments \"A\" and \"B\") that BERT learned, and which it adds to the token embeddings before feeding them into the input layer.","title":"BERT Input Format"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#start-end-token-classifiers","text":"BERT needs to highlight a \"span\" of text containing the answer--this is represented as simply predicting which token marks the start of the answer, and which token marks the end. For every token in the text, we feed its final embedding into the start token classifier. The start token classifier only has a single set of weights (represented by the blue \"start\" rectangle in the above illustration) which it applies to every word. After taking the dot product between the output embeddings and the 'start' weights, we apply the softmax activation to produce a probability distribution over all of the words. Whichever word has the highest probability of being the start token is the one that we pick. We repeat this process for the end token--we have a separate weight vector this.","title":"Start &amp; End Token Classifiers"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#part-2-example-code","text":"In the example code below, we'll be downloading a model that's already been fine-tuned for question answering, and try it out on our own text. If you do want to fine-tune on your own dataset, it is possible to fine-tune BERT for question answering yourself. See run_squad.py in the transformers library. However,you may find that the below \"fine-tuned-on-squad\" model already does a good job, even if your text is from a different domain. Note: The example code in this Notebook is a commented and expanded version of the short example provided in the transformers documentation here .","title":"Part 2: Example Code"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#1-install-huggingface-transformers-library","text":"This example uses the transformers library by huggingface. We'll start by installing the package. !pip install transformers Collecting transformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 778kB 8.6MB/s \u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Collecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 14.4MB/s \u001b[?25hCollecting tokenizers==0.8.1.rc1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 43.4MB/s \u001b[?25hCollecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 44.5MB/s \u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0) Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=09966dce42bbfaa2aa5abc0f5f82653b74458d122a6fefeb7c895da49b40ce50 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2 import torch","title":"1. Install huggingface transformers library"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#2-load-fine-tuned-bert-large","text":"For Question Answering we use the BertForQuestionAnswering class from the transformers library. This class supports fine-tuning, but for this example we will keep things simpler and load a BERT model that has already been fine-tuned for the SQuAD benchmark. The transformers library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation here . For Question Answering, they have a version of BERT-large that has already been fine-tuned for the SQuAD benchmark. BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance. (Note that this download is not using your own network bandwidth--it's between the Google instance and wherever the model is stored on the web). Note: I believe this model was trained on version 1 of SQuAD, since it's not outputting whether the question is \"impossible\" to answer from the text (which is part of the task in v2 of SQuAD). from transformers import BertForQuestionAnswering model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443.0, style=ProgressStyle(description_\u2026 HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1340675298.0, style=ProgressStyle(descr\u2026 Load the tokenizer as well. Side note: Apparently the vocabulary of this model is identicaly to the one in bert-base-uncased. You can load the tokenizer from bert-base-uncased and that works just as well. from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad') HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti\u2026","title":"2. Load Fine-Tuned BERT-large"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#3-ask-a-question","text":"Now we're ready to feed in an example! A QA example consists of a question and a passage of text containing the answer to that question. Let's try an example using the text in this tutorial! question = \"How many parameters does BERT-large have?\" answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\" We'll need to run the BERT tokenizer against both the question and the answer_text . To feed these into BERT, we actually concatenate them together and place the special [SEP] token in between. # Apply the tokenizer to the input text, treating them as a text-pair. input_ids = tokenizer.encode(question, answer_text) print('The input has a total of {:} tokens.'.format(len(input_ids))) The input has a total of 70 tokens. Just to see exactly what the tokenizer is doing, let's print out the tokens with their IDs. # BERT only needs the token IDs, but for the purpose of inspecting the # tokenizer's behavior, let's also get the token strings and display them. tokens = tokenizer.convert_ids_to_tokens(input_ids) # For each token and its id... for token, id in zip(tokens, input_ids): # If this is the [SEP] token, add some space around it to make it stand out. if id == tokenizer.sep_token_id: print('') # Print the token string and its ID in two columns. print('{:<12} {:>6,}'.format(token, id)) if id == tokenizer.sep_token_id: print('') [CLS] 101 how 2,129 many 2,116 parameters 11,709 does 2,515 bert 14,324 - 1,011 large 2,312 have 2,031 ? 1,029 [SEP] 102 bert 14,324 - 1,011 large 2,312 is 2,003 really 2,428 big 2,502 . 1,012 . 1,012 . 1,012 it 2,009 has 2,038 24 2,484 - 1,011 layers 9,014 and 1,998 an 2,019 em 7,861 ##bed 8,270 ##ding 4,667 size 2,946 of 1,997 1 1,015 , 1,010 02 6,185 ##4 2,549 , 1,010 for 2,005 a 1,037 total 2,561 of 1,997 340 16,029 ##m 2,213 parameters 11,709 ! 999 altogether 10,462 it 2,009 is 2,003 1 1,015 . 1,012 34 4,090 ##gb 18,259 , 1,010 so 2,061 expect 5,987 it 2,009 to 2,000 take 2,202 a 1,037 couple 3,232 minutes 2,781 to 2,000 download 8,816 to 2,000 your 2,115 cola 15,270 ##b 2,497 instance 6,013 . 1,012 [SEP] 102 We've concatenated the question and answer_text together, but BERT still needs a way to distinguish them. BERT has two special \"Segment\" embeddings, one for segment \"A\" and one for segment \"B\". Before the word embeddings go into the BERT layers, the segment A embedding needs to be added to the question tokens, and the segment B embedding needs to be added to each of the answer_text tokens. These additions are handled for us by the transformer library, and all we need to do is specify a '0' or '1' for each token. Note: In the transformers library, huggingface likes to call these token_type_ids , but I'm going with segment_ids since this seems clearer, and is consistent with the BERT paper. # Search the input_ids for the first instance of the `[SEP]` token. sep_index = input_ids.index(tokenizer.sep_token_id) # The number of segment A tokens includes the [SEP] token istelf. num_seg_a = sep_index + 1 # The remainder are segment B. num_seg_b = len(input_ids) - num_seg_a # Construct the list of 0s and 1s. segment_ids = [0]*num_seg_a + [1]*num_seg_b # There should be a segment_id for every input token. assert len(segment_ids) == len(input_ids) Side Note: Where's the padding? The original example code does not perform any padding. I suspect that this is because we are only feeding in a single example . If we instead fed in a batch of examples, then we would need to pad or truncate all of the samples in the batch to a single length, and supply an attention mask to tell BERT to ignore the padding tokens. We're ready to feed our example into the model! # Run our example through the model. start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text. token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text Now we can highlight the answer just by looking at the most probable start and end words. # Find the tokens with the highest `start` and `end` scores. answer_start = torch.argmax(start_scores) answer_end = torch.argmax(end_scores) # Combine the tokens in the answer and print it out. answer = ' '.join(tokens[answer_start:answer_end+1]) print('Answer: \"' + answer + '\"') Answer: \"340 ##m\" It got it right! Awesome :) Side Note: It's a little naive to pick the highest scores for start and end--what if it predicts an end word that's before the start word?! The correct implementation is to pick the highest total score for which end >= start. With a little more effort, we can reconstruct any words that got broken down into subwords. # Start with the first token. answer = tokens[answer_start] # Select the remaining answer tokens and join them with whitespace. for i in range(answer_start + 1, answer_end + 1): # If it's a subword token, then recombine it with the previous token. if tokens[i][0:2] == '##': answer += tokens[i][2:] # Otherwise, add a space then the token. else: answer += ' ' + tokens[i] print('Answer: \"' + answer + '\"') Answer: \"340m\"","title":"3. Ask a Question"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#4-visualizing-scores","text":"I was curious to see what the scores were for all of the words. The following cells generate bar plots showing the start and end scores for every word in the input. import matplotlib.pyplot as plt import seaborn as sns # Use plot styling from seaborn. sns.set(style='darkgrid') # Increase the plot size and font size. #sns.set(font_scale=1.5) plt.rcParams[\"figure.figsize\"] = (16,8) Retrieve all of the start and end scores, and use all of the tokens as x-axis labels. # Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays. s_scores = start_scores.detach().numpy().flatten() e_scores = end_scores.detach().numpy().flatten() # We'll use the tokens as the x-axis labels. In order to do that, they all need # to be unique, so we'll add the token index to the end of each one. token_labels = [] for (i, token) in enumerate(tokens): token_labels.append('{:} - {:>2}'.format(token, i)) Create a bar plot showing the score for every input word being the \"start\" word. # Create a barplot showing the start word score for all of the tokens. ax = sns.barplot(x=token_labels, y=s_scores, ci=None) # Turn the xlabels vertical. ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\") # Turn on the vertical grid to help align words to scores. ax.grid(True) plt.title('Start Word Scores') plt.show() Create a second bar plot showing the score for every input word being the \"end\" word. # Create a barplot showing the end word score for all of the tokens. ax = sns.barplot(x=token_labels, y=e_scores, ci=None) # Turn the xlabels vertical. ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\") # Turn on the vertical grid to help align words to scores. ax.grid(True) plt.title('End Word Scores') plt.show() Alternate View I also tried visualizing both the start and end scores on a single bar plot, but I think it may actually be more confusing then seeing them separately. import pandas as pd # Store the tokens and scores in a DataFrame. # Each token will have two rows, one for its start score and one for its end # score. The \"marker\" column will differentiate them. A little wacky, I know. scores = [] for (i, token_label) in enumerate(token_labels): # Add the token's start score as one row. scores.append({'token_label': token_label, 'score': s_scores[i], 'marker': 'start'}) # Add the token's end score as another row. scores.append({'token_label': token_label, 'score': e_scores[i], 'marker': 'end'}) df = pd.DataFrame(scores) # Draw a grouped barplot to show start and end scores for each word. # The \"hue\" parameter is where we tell it which datapoints belong to which # of the two series. g = sns.catplot(x=\"token_label\", y=\"score\", hue=\"marker\", data=df, kind=\"bar\", height=6, aspect=4) # Turn the xlabels vertical. g.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha=\"center\") # Turn on the vertical grid to help align words to scores. g.ax.grid(True)","title":"4. Visualizing Scores"},{"location":"Question_Answering_with_a_Fine_Tuned_BERT/#5-more-examples","text":"Turn the QA process into a function so we can easily try out other examples. def answer_question(question, answer_text): ''' Takes a `question` string and an `answer_text` string (which contains the answer), and identifies the words within the `answer_text` that are the answer. Prints them out. ''' # ======== Tokenize ======== # Apply the tokenizer to the input text, treating them as a text-pair. input_ids = tokenizer.encode(question, answer_text) # Report how long the input sequence is. print('Query has {:,} tokens.\\n'.format(len(input_ids))) # ======== Set Segment IDs ======== # Search the input_ids for the first instance of the `[SEP]` token. sep_index = input_ids.index(tokenizer.sep_token_id) # The number of segment A tokens includes the [SEP] token istelf. num_seg_a = sep_index + 1 # The remainder are segment B. num_seg_b = len(input_ids) - num_seg_a # Construct the list of 0s and 1s. segment_ids = [0]*num_seg_a + [1]*num_seg_b # There should be a segment_id for every input token. assert len(segment_ids) == len(input_ids) # ======== Evaluate ======== # Run our example question through the model. start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text. token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text # ======== Reconstruct Answer ======== # Find the tokens with the highest `start` and `end` scores. answer_start = torch.argmax(start_scores) answer_end = torch.argmax(end_scores) # Get the string versions of the input tokens. tokens = tokenizer.convert_ids_to_tokens(input_ids) # Start with the first token. answer = tokens[answer_start] # Select the remaining answer tokens and join them with whitespace. for i in range(answer_start + 1, answer_end + 1): # If it's a subword token, then recombine it with the previous token. if tokens[i][0:2] == '##': answer += tokens[i][2:] # Otherwise, add a space then the token. else: answer += ' ' + tokens[i] print('Answer: \"' + answer + '\"') As our reference text, I've taken the Abstract of the BERT paper . import textwrap # Wrap text to 80 characters. wrapper = textwrap.TextWrapper(width=80) bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\" print(wrapper.fill(bert_abstract)) We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). Ask BERT what its name stands for (the answer is in the first sentence of the abstract). question = \"What does the 'B' in BERT stand for?\" answer_question(question, bert_abstract) Query has 258 tokens. Answer: \"bidirectional encoder representations from transformers\" Ask BERT about example applications of itself :) The answer to the question comes from this passage from the abstract: \"...BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.\" question = \"What are some example applications of BERT?\" answer_question(question, bert_abstract) Query has 255 tokens. Answer: \"question answering and language inference\"","title":"5. More Examples"},{"location":"Seq2Seq_Pytorch/","text":"!pip install spacy --upgrade Collecting spacy \u001b[?25l Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.0MB 3.9MB/s \u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2) Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1) Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0) Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3) Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.6.0) Collecting thinc==7.4.1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.1MB 18.3MB/s \u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0) Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1) Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3) Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1) Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2) Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5) Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2) Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0) Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4) Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3) Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20) Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0) Installing collected packages: thinc, spacy Found existing installation: thinc 7.4.0 Uninstalling thinc-7.4.0: Successfully uninstalled thinc-7.4.0 Found existing installation: spacy 2.2.4 Uninstalling spacy-2.2.4: Successfully uninstalled spacy-2.2.4 Successfully installed spacy-2.3.2 thinc-7.4.1 !python -m spacy download en !python -m spacy download de !python -m spacy download hi Collecting en_core_web_sm==2.3.1 \u001b[?25l Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.1MB 803kB/s \u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.2) Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.1) Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2) Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1) Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1) Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2) Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0) Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5) Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2) Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3) Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1) Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.6.0) Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3) Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10) Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0) Building wheels for collected packages: en-core-web-sm Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp36-none-any.whl size=12047109 sha256=c566e8eddcd63bc8259a784a72fae15d14da6e23d183b7a16bf6032b9aeaeed2 Stored in directory: /tmp/pip-ephem-wheel-cache-76hzqxdi/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46 Successfully built en-core-web-sm Installing collected packages: en-core-web-sm Found existing installation: en-core-web-sm 2.2.5 Uninstalling en-core-web-sm-2.2.5: Successfully uninstalled en-core-web-sm-2.2.5 Successfully installed en-core-web-sm-2.3.1 \u001b[38;5;2m\u2714 Download and installation successful\u001b[0m You can now load the model via spacy.load('en_core_web_sm') \u001b[38;5;2m\u2714 Linking successful\u001b[0m /usr/local/lib/python3.6/dist-packages/en_core_web_sm --> /usr/local/lib/python3.6/dist-packages/spacy/data/en You can now load the model via spacy.load('en') Collecting de_core_news_sm==2.3.0 \u001b[?25l Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz (14.9MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.9MB 833kB/s \u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.3.0) (2.3.2) Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.18.5) Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.41.1) Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.23.0) Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0) Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2) Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3) Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.4.1) Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.6.0) Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.1) Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.2) Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.1) Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.3) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.6.20) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.24.3) Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.7.0) Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.1.0) Building wheels for collected packages: de-core-news-sm Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for de-core-news-sm: filename=de_core_news_sm-2.3.0-cp36-none-any.whl size=14907580 sha256=959d3b6df2936d8e86bbc601dbd80eddfb310d3af7aca8895529c319cb58b539 Stored in directory: /tmp/pip-ephem-wheel-cache-59e5ledg/wheels/db/f3/1e/0df0f27eee12bd1aaa94bcfef11b01eca62f90b9b9a0ce08fd Successfully built de-core-news-sm Installing collected packages: de-core-news-sm Found existing installation: de-core-news-sm 2.2.5 Uninstalling de-core-news-sm-2.2.5: Successfully uninstalled de-core-news-sm-2.2.5 Successfully installed de-core-news-sm-2.3.0 \u001b[38;5;2m\u2714 Download and installation successful\u001b[0m You can now load the model via spacy.load('de_core_news_sm') \u001b[38;5;2m\u2714 Linking successful\u001b[0m /usr/local/lib/python3.6/dist-packages/de_core_news_sm --> /usr/local/lib/python3.6/dist-packages/spacy/data/de You can now load the model via spacy.load('de') \u001b[38;5;1m\u2718 No compatible model found for 'hi' (spaCy v2.3.2).\u001b[0m import torch import torch.nn as nn import torch.optim as optim import spacy from torch.utils.tensorboard import SummaryWriter from torchtext.datasets import Multi30k from torchtext.data import Field, BucketIterator from spacy.lang.hi import Hindi spacy_ger = spacy.load(\"de\") spacy_eng = spacy.load(\"en\") spacy_hi = Hindi() def tokenize_hi(text): return [tok.text for tok in spacy_hi.tokenizer(text)] def tokenize_ger(text): return [tok.text for tok in spacy_ger.tokenizer(text)] def tokenize_eng(text): return [tok.text for tok in spacy_eng.tokenizer(text)] german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\") english = Field( tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\" ) train_data, valid_data, test_data = Multi30k.splits( exts=(\".de\", \".en\"), fields=(german, english) ) german.build_vocab(train_data, max_size=10000, min_freq=2) english.build_vocab(train_data, max_size=10000, min_freq=2) class Transformer(nn.Module): def __init__( self, embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_len, device, ): super(Transformer, self).__init__() self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size) self.src_position_embedding = nn.Embedding(max_len, embedding_size) self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size) self.trg_position_embedding = nn.Embedding(max_len, embedding_size) self.device = device self.transformer = nn.Transformer( embedding_size, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, ) self.fc_out = nn.Linear(embedding_size, trg_vocab_size) self.dropout = nn.Dropout(dropout) self.src_pad_idx = src_pad_idx def make_src_mask(self, src): src_mask = src.transpose(0, 1) == self.src_pad_idx # (N, src_len) return src_mask.to(self.device) def forward(self, src, trg): src_seq_length, N = src.shape trg_seq_length, N = trg.shape src_positions = ( torch.arange(0, src_seq_length) .unsqueeze(1) .expand(src_seq_length, N) .to(self.device) ) trg_positions = ( torch.arange(0, trg_seq_length) .unsqueeze(1) .expand(trg_seq_length, N) .to(self.device) ) embed_src = self.dropout( (self.src_word_embedding(src) + self.src_position_embedding(src_positions)) ) embed_trg = self.dropout( (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)) ) src_padding_mask = self.make_src_mask(src) trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to( self.device ) out = self.transformer( embed_src, embed_trg, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask, ) out = self.fc_out(out) return out # We're ready to define everything we need for training our Seq2Seq model device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") load_model = False save_model = True # Training hyperparameters num_epochs = 10000 learning_rate = 3e-4 batch_size = 32 # Model hyperparameters src_vocab_size = len(german.vocab) trg_vocab_size = len(english.vocab) embedding_size = 512 num_heads = 8 num_encoder_layers = 3 num_decoder_layers = 3 dropout = 0.10 max_len = 100 forward_expansion = 4 src_pad_idx = english.vocab.stoi[\"<pad>\"] # Tensorboard to get nice loss plot writer = SummaryWriter(\"runs/loss_plot\") step = 0 train_iterator, valid_iterator, test_iterator = BucketIterator.splits( (train_data, valid_data, test_data), batch_size=batch_size, sort_within_batch=True, sort_key=lambda x: len(x.src), device=device, ) model = Transformer( embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_len, device, ).to(device) def translate_sentence(model, sentence, german, english, device, max_length=50): # Load german tokenizer spacy_ger = spacy.load(\"de\") # Create tokens using spacy and everything in lower case (which is what our vocab is) if type(sentence) == str: tokens = [token.text.lower() for token in spacy_ger(sentence)] else: tokens = [token.lower() for token in sentence] # Add <SOS> and <EOS> in beginning and end respectively tokens.insert(0, german.init_token) tokens.append(german.eos_token) # Go through each german token and convert to an index text_to_indices = [german.vocab.stoi[token] for token in tokens] # Convert to Tensor sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device) outputs = [english.vocab.stoi[\"<sos>\"]] for i in range(max_length): trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device) with torch.no_grad(): output = model(sentence_tensor, trg_tensor) best_guess = output.argmax(2)[-1, :].item() outputs.append(best_guess) if best_guess == english.vocab.stoi[\"<eos>\"]: break translated_sentence = [english.vocab.itos[idx] for idx in outputs] # remove start token return translated_sentence[1:] def bleu(data, model, german, english, device): targets = [] outputs = [] for example in data: src = vars(example)[\"src\"] trg = vars(example)[\"trg\"] prediction = translate_sentence(model, src, german, english, device) prediction = prediction[:-1] # remove <eos> token targets.append([trg]) outputs.append(prediction) return bleu_score(outputs, targets) def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"): print(\"=> Saving checkpoint\") torch.save(state, filename) def load_checkpoint(checkpoint, model, optimizer): print(\"=> Loading checkpoint\") model.load_state_dict(checkpoint[\"state_dict\"]) optimizer.load_state_dict(checkpoint[\"optimizer\"]) optimizer = optim.Adam(model.parameters(), lr=learning_rate) scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, factor=0.1, patience=10, verbose=True ) pad_idx = english.vocab.stoi[\"<pad>\"] criterion = nn.CrossEntropyLoss(ignore_index=pad_idx) if load_model: load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer) sentence = \"ein pferd geht unter einer br\u00fccke neben einem boot.\" for epoch in range(num_epochs): print(f\"[Epoch {epoch} / {num_epochs}]\") if save_model: checkpoint = { \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict(), } save_checkpoint(checkpoint) model.eval() translated_sentence = translate_sentence( model, sentence, german, english, device, max_length=50 ) print(f\"Translated example sentence: \\n {translated_sentence}\") model.train() losses = [] for batch_idx, batch in enumerate(train_iterator): # Get input and targets and get to cuda inp_data = batch.src.to(device) target = batch.trg.to(device) # Forward prop output = model(inp_data, target[:-1, :]) # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss # doesn't take input in that form. For example if we have MNIST we want to have # output to be: (N, 10) and targets just (N). Here we can view it in a similar # way that we have output_words * batch_size that we want to send in into # our cost function, so we need to do some reshapin. # Let's also remove the start token while we're at it output = output.reshape(-1, output.shape[2]) target = target[1:].reshape(-1) optimizer.zero_grad() loss = criterion(output, target) losses.append(loss.item()) # Back prop loss.backward() # Clip to avoid exploding gradient issues, makes sure grads are # within a healthy range torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # Gradient descent step optimizer.step() # plot to tensorboard writer.add_scalar(\"Training loss\", loss, global_step=step) step += 1 mean_loss = sum(losses) / len(losses) scheduler.step(mean_loss) # running on entire test data takes a while score = bleu(test_data[1:100], model, german, english, device) print(f\"Bleu score {score * 100:.2f}\") [Epoch 0 / 10000] => Saving checkpoint Translated example sentence: ['.', 'secures', 'secures', 'half', 'secures', 'secures', 'half', '.', 'secures', 'toddler', 'secures', 'secures', 'half', '.', '.', 'olympians', 'half', '.', 'olympians', 'helmet', '.', 'secures', 'toddler', 'secures', 'secures', 'toddler', 'secures', 'secures', '.', 'secures', 'secures', '.', 'secures', '.', 'secures', 'secures', 'secures', 'secures', 'half', 'half', '.', 'secures', 'toddler', 'secures', 'secures', 'secures', 'mosaic', 'secures', 'secures', 'toddler'] [Epoch 1 / 10000] => Saving checkpoint Translated example sentence: ['a', 'horse', 'walking', 'under', 'a', 'boat', 'next', 'to', 'a', 'boat', '.', '<eos>'] [Epoch 2 / 10000] => Saving checkpoint Translated example sentence: ['a', 'horse', 'is', 'walking', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>'] [Epoch 3 / 10000] => Saving checkpoint Translated example sentence: ['a', 'horse', 'is', 'walking', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>'] [Epoch 4 / 10000] => Saving checkpoint Translated example sentence: ['a', 'horse', 'walks', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>'] [Epoch 5 / 10000] => Saving checkpoint Translated example sentence: ['a', 'horse', 'walks', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>'] [Epoch 6 / 10000] => Saving checkpoint Translated example sentence: ['a', 'horse', 'walks', 'underneath', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>'] [Epoch 7 / 10000] => Saving checkpoint Translated example sentence: ['a', 'horse', 'is', 'walking', 'under', 'a', 'bridge', 'beside', 'a', 'boat', '.', '<eos>']","title":"Seq2Seq Pytorch"},{"location":"SetFit_SST_2_Few_shot/","text":"Reference Links: https://towardsdatascience.com/sentence-transformer-fine-tuning-setfit-outperforms-gpt-3-on-few-shot-text-classification-while-d9a3788f0b4e https://arxiv.org/pdf/2109.14076.pdf https://huggingface.co/spaces/ought/raft-leaderboard https://github.com/timoschick/pet Init !pip install sentence_transformers -q \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78 kB 3.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.4 MB 11.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.3 MB 36.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 29.8 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61 kB 370 kB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596 kB 25.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 895 kB 32.0 MB/s \u001b[?25h Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone from sklearn.metrics import accuracy_score, f1_score from sklearn.linear_model import LogisticRegression from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets, evaluation from torch.utils.data import DataLoader from sklearn.manifold import TSNE from matplotlib import pyplot as plt # import warnings filter from warnings import simplefilter # ignore all future warnings simplefilter(action='ignore', category=FutureWarning) import pandas as pd import numpy as np import torch import random import torch def set_seed(seed): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) def sentence_pairs_generation(sentences, labels, pairs): # initialize two empty lists to hold the (sentence, sentence) pairs and # labels to indicate if a pair is positive or negative numClassesList = np.unique(labels) idx = [np.where(labels == i)[0] for i in numClassesList] for idxA in range(len(sentences)): currentSentence = sentences[idxA] label = labels[idxA] idxB = np.random.choice(idx[np.where(numClassesList==label)[0][0]]) posSentence = sentences[idxB] # prepare a positive pair and update the sentences and labels # lists, respectively pairs.append(InputExample(texts=[currentSentence, posSentence], label=1.0)) negIdx = np.where(labels != label)[0] negSentence = sentences[np.random.choice(negIdx)] # prepare a negative pair of images and update our lists pairs.append(InputExample(texts=[currentSentence, negSentence], label=0.0)) # return a 2-tuple of our image pairs and labels return (pairs) #SST-2 # Load SST-2 dataset into a pandas dataframe. train_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None) # Load the test dataset into a pandas dataframe. eval_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', delimiter='\\t', header=None) text_col=train_df.columns.values[0] category_col=train_df.columns.values[1] x_eval = eval_df[text_col].values.tolist() y_eval = eval_df[category_col].values.tolist() train_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 a stirring , funny and finally transporting re... 1 1 apparently reassembled from the cutting room f... 0 2 they presume their audience wo n't sit still f... 0 3 this is a visually stunning rumination on love... 1 4 jonathan parker 's bartleby should have been t... 1 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-ad145a91-fe65-4f27-90e9-8193b3dfe59a button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-ad145a91-fe65-4f27-90e9-8193b3dfe59a'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } SetFit #@title SetFit st_model = 'paraphrase-mpnet-base-v2' #@param ['paraphrase-mpnet-base-v2', 'all-mpnet-base-v1', 'all-mpnet-base-v2', 'stsb-mpnet-base-v2', 'all-MiniLM-L12-v2', 'paraphrase-albert-small-v2', 'all-roberta-large-v1'] num_training = 32 #@param [\"8\", \"16\", \"32\", \"54\", \"128\", \"256\", \"512\"] {type:\"raw\"} num_itr = 5 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"10\"] {type:\"raw\"} plot2d_checkbox = True #@param {type: 'boolean'} set_seed(0) # Equal samples per class training train_df_sample = pd.concat([train_df[train_df[1]==0].sample(num_training), train_df[train_df[1]==1].sample(num_training)]) x_train = train_df_sample[text_col].values.tolist() y_train = train_df_sample[category_col].values.tolist() train_df_sample.shape (64, 2) x_train[0:5], y_train[0:5] (['makes a joke out of car chases for an hour and then gives us half an hour of car chases', \"so mind numbingly awful that you hope britney wo n't do it one more time , as far as movies are concerned\", \"maid in manhattan proves that it 's easier to change the sheets than to change hackneyed concepts when it comes to dreaming up romantic comedies\", 'if you go , pack your knitting needles', 'time of favor could have given audiences the time of day by concentrating on the elements of a revealing alienation among a culture of people who sadly are at hostile odds with one another through recklessness and retaliation'], [0, 0, 0, 0, 0]) train_examples = [] for x in range(num_itr): train_examples = sentence_pairs_generation(np.array(x_train), np.array(y_train), train_examples) len(train_examples) 640 i_example = train_examples[0] i_example.texts, i_example.label (['makes a joke out of car chases for an hour and then gives us half an hour of car chases', 'it is that rare combination of bad writing , bad direction and bad acting the trifecta of badness'], 1.0) orig_model = SentenceTransformer(st_model) model = SentenceTransformer(st_model) Downloading: 0%| | 0.00/690 [00:00<?, ?B/s] Downloading: 0%| | 0.00/3.70k [00:00<?, ?B/s] Downloading: 0%| | 0.00/594 [00:00<?, ?B/s] Downloading: 0%| | 0.00/122 [00:00<?, ?B/s] Downloading: 0%| | 0.00/229 [00:00<?, ?B/s] Downloading: 0%| | 0.00/438M [00:00<?, ?B/s] Downloading: 0%| | 0.00/53.0 [00:00<?, ?B/s] Downloading: 0%| | 0.00/239 [00:00<?, ?B/s] Downloading: 0%| | 0.00/466k [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.19k [00:00<?, ?B/s] Downloading: 0%| | 0.00/232k [00:00<?, ?B/s] Downloading: 0%| | 0.00/190 [00:00<?, ?B/s] # S-BERT adaptation train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16) train_loss = losses.CosineSimilarityLoss(model) model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=10, show_progress_bar=True) Epoch: 0%| | 0/1 [00:00<?, ?it/s] Iteration: 0%| | 0/40 [00:00<?, ?it/s] len(y_train) 64 # No Fit X_train_noFT = orig_model.encode(x_train) X_eval_noFT = orig_model.encode(x_eval) sgd = LogisticRegression() sgd.fit(X_train_noFT, y_train) y_pred_eval_sgd = sgd.predict(X_eval_noFT) print('Acc. No Fit', accuracy_score(y_eval, y_pred_eval_sgd)) Acc. No Fit 0.8390993959362988 # With Fit (SetFit) X_train = model.encode(x_train) X_eval = model.encode(x_eval) sgd = LogisticRegression() sgd.fit(X_train, y_train) y_pred_eval_sgd = sgd.predict(X_eval) print('Acc. SetFit', accuracy_score(y_eval, y_pred_eval_sgd)) Acc. SetFit 0.9082921471718836 #Plot 2-D 2x2 figures if plot2d_checkbox: plt.figure(figsize=(20,10)) #Plot X_train_noFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_train_noFT)) plt.subplot(221) plt.title('X_train No Fit') for i, t in enumerate(set(np.array(y_train))): idx = np.array(y_train) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1)); #Plot X_eval noFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_eval_noFT)) plt.subplot(223) plt.title('X_eval No Fit') for i, t in enumerate(set(np.array(y_eval))): idx = np.array(y_eval) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1)); #Plot X_train SetFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_train)) plt.subplot(222) plt.title('X_train SetFit') for i, t in enumerate(set(np.array(y_train))): idx = np.array(y_train) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1)); #Plot X_eval SetFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_eval)) plt.subplot(224) plt.title('X_eval SetFit') for i, t in enumerate(set(np.array(y_eval))): idx = np.array(y_eval) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1));","title":"SetFit SST 2 Few shot"},{"location":"SetFit_SST_2_Few_shot/#init","text":"!pip install sentence_transformers -q \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78 kB 3.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.4 MB 11.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.3 MB 36.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 29.8 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61 kB 370 kB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596 kB 25.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 895 kB 32.0 MB/s \u001b[?25h Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone from sklearn.metrics import accuracy_score, f1_score from sklearn.linear_model import LogisticRegression from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets, evaluation from torch.utils.data import DataLoader from sklearn.manifold import TSNE from matplotlib import pyplot as plt # import warnings filter from warnings import simplefilter # ignore all future warnings simplefilter(action='ignore', category=FutureWarning) import pandas as pd import numpy as np import torch import random import torch def set_seed(seed): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) def sentence_pairs_generation(sentences, labels, pairs): # initialize two empty lists to hold the (sentence, sentence) pairs and # labels to indicate if a pair is positive or negative numClassesList = np.unique(labels) idx = [np.where(labels == i)[0] for i in numClassesList] for idxA in range(len(sentences)): currentSentence = sentences[idxA] label = labels[idxA] idxB = np.random.choice(idx[np.where(numClassesList==label)[0][0]]) posSentence = sentences[idxB] # prepare a positive pair and update the sentences and labels # lists, respectively pairs.append(InputExample(texts=[currentSentence, posSentence], label=1.0)) negIdx = np.where(labels != label)[0] negSentence = sentences[np.random.choice(negIdx)] # prepare a negative pair of images and update our lists pairs.append(InputExample(texts=[currentSentence, negSentence], label=0.0)) # return a 2-tuple of our image pairs and labels return (pairs) #SST-2 # Load SST-2 dataset into a pandas dataframe. train_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None) # Load the test dataset into a pandas dataframe. eval_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', delimiter='\\t', header=None) text_col=train_df.columns.values[0] category_col=train_df.columns.values[1] x_eval = eval_df[text_col].values.tolist() y_eval = eval_df[category_col].values.tolist() train_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 a stirring , funny and finally transporting re... 1 1 apparently reassembled from the cutting room f... 0 2 they presume their audience wo n't sit still f... 0 3 this is a visually stunning rumination on love... 1 4 jonathan parker 's bartleby should have been t... 1 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-ad145a91-fe65-4f27-90e9-8193b3dfe59a button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-ad145a91-fe65-4f27-90e9-8193b3dfe59a'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); }","title":"Init"},{"location":"SetFit_SST_2_Few_shot/#setfit","text":"#@title SetFit st_model = 'paraphrase-mpnet-base-v2' #@param ['paraphrase-mpnet-base-v2', 'all-mpnet-base-v1', 'all-mpnet-base-v2', 'stsb-mpnet-base-v2', 'all-MiniLM-L12-v2', 'paraphrase-albert-small-v2', 'all-roberta-large-v1'] num_training = 32 #@param [\"8\", \"16\", \"32\", \"54\", \"128\", \"256\", \"512\"] {type:\"raw\"} num_itr = 5 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"10\"] {type:\"raw\"} plot2d_checkbox = True #@param {type: 'boolean'} set_seed(0) # Equal samples per class training train_df_sample = pd.concat([train_df[train_df[1]==0].sample(num_training), train_df[train_df[1]==1].sample(num_training)]) x_train = train_df_sample[text_col].values.tolist() y_train = train_df_sample[category_col].values.tolist() train_df_sample.shape (64, 2) x_train[0:5], y_train[0:5] (['makes a joke out of car chases for an hour and then gives us half an hour of car chases', \"so mind numbingly awful that you hope britney wo n't do it one more time , as far as movies are concerned\", \"maid in manhattan proves that it 's easier to change the sheets than to change hackneyed concepts when it comes to dreaming up romantic comedies\", 'if you go , pack your knitting needles', 'time of favor could have given audiences the time of day by concentrating on the elements of a revealing alienation among a culture of people who sadly are at hostile odds with one another through recklessness and retaliation'], [0, 0, 0, 0, 0]) train_examples = [] for x in range(num_itr): train_examples = sentence_pairs_generation(np.array(x_train), np.array(y_train), train_examples) len(train_examples) 640 i_example = train_examples[0] i_example.texts, i_example.label (['makes a joke out of car chases for an hour and then gives us half an hour of car chases', 'it is that rare combination of bad writing , bad direction and bad acting the trifecta of badness'], 1.0) orig_model = SentenceTransformer(st_model) model = SentenceTransformer(st_model) Downloading: 0%| | 0.00/690 [00:00<?, ?B/s] Downloading: 0%| | 0.00/3.70k [00:00<?, ?B/s] Downloading: 0%| | 0.00/594 [00:00<?, ?B/s] Downloading: 0%| | 0.00/122 [00:00<?, ?B/s] Downloading: 0%| | 0.00/229 [00:00<?, ?B/s] Downloading: 0%| | 0.00/438M [00:00<?, ?B/s] Downloading: 0%| | 0.00/53.0 [00:00<?, ?B/s] Downloading: 0%| | 0.00/239 [00:00<?, ?B/s] Downloading: 0%| | 0.00/466k [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.19k [00:00<?, ?B/s] Downloading: 0%| | 0.00/232k [00:00<?, ?B/s] Downloading: 0%| | 0.00/190 [00:00<?, ?B/s] # S-BERT adaptation train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16) train_loss = losses.CosineSimilarityLoss(model) model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=10, show_progress_bar=True) Epoch: 0%| | 0/1 [00:00<?, ?it/s] Iteration: 0%| | 0/40 [00:00<?, ?it/s] len(y_train) 64 # No Fit X_train_noFT = orig_model.encode(x_train) X_eval_noFT = orig_model.encode(x_eval) sgd = LogisticRegression() sgd.fit(X_train_noFT, y_train) y_pred_eval_sgd = sgd.predict(X_eval_noFT) print('Acc. No Fit', accuracy_score(y_eval, y_pred_eval_sgd)) Acc. No Fit 0.8390993959362988 # With Fit (SetFit) X_train = model.encode(x_train) X_eval = model.encode(x_eval) sgd = LogisticRegression() sgd.fit(X_train, y_train) y_pred_eval_sgd = sgd.predict(X_eval) print('Acc. SetFit', accuracy_score(y_eval, y_pred_eval_sgd)) Acc. SetFit 0.9082921471718836 #Plot 2-D 2x2 figures if plot2d_checkbox: plt.figure(figsize=(20,10)) #Plot X_train_noFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_train_noFT)) plt.subplot(221) plt.title('X_train No Fit') for i, t in enumerate(set(np.array(y_train))): idx = np.array(y_train) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1)); #Plot X_eval noFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_eval_noFT)) plt.subplot(223) plt.title('X_eval No Fit') for i, t in enumerate(set(np.array(y_eval))): idx = np.array(y_eval) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1)); #Plot X_train SetFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_train)) plt.subplot(222) plt.title('X_train SetFit') for i, t in enumerate(set(np.array(y_train))): idx = np.array(y_train) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1)); #Plot X_eval SetFit X_embedded = TSNE(n_components=2).fit_transform(np.array(X_eval)) plt.subplot(224) plt.title('X_eval SetFit') for i, t in enumerate(set(np.array(y_eval))): idx = np.array(y_eval) == t plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t) plt.legend(bbox_to_anchor=(1, 1));","title":"SetFit"},{"location":"Simpletransformers_2/","text":"!pip install simpletransformers Collecting simpletransformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/14/f2/f0e219441ba3705dcfc6a4552171e177fa0f6d20df9adb62d94f76ff9fe6/simpletransformers-0.47.3-py3-none-any.whl (208kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 2.8MB/s \u001b[?25hCollecting wandb \u001b[?25l Downloading https://files.pythonhosted.org/packages/65/14/e7988204e4d4c9a349e73362399263b1c17f2b4d8a753864444f9eac1c92/wandb-0.9.5-py2.py3-none-any.whl (1.4MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 9.1MB/s \u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0) Collecting tqdm>=4.47.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 7.1MB/s \u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1) Collecting seqeval Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz Collecting transformers>=3.0.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 778kB 17.5MB/s \u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.5) Collecting tensorboardx \u001b[?25l Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 317kB 24.5MB/s \u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.5) Collecting tokenizers \u001b[?25l Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 26.4MB/s \u001b[?25hCollecting streamlit \u001b[?25l Downloading https://files.pythonhosted.org/packages/7a/95/c1f097bfd0ea06f97d02e09e6e0af9bfa4da2c1e761112d5916bfd3bf846/streamlit-0.65.2-py2.py3-none-any.whl (7.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.2MB 42.1MB/s \u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20) Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (1.15.0) Collecting GitPython>=1.0.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 56.4MB/s \u001b[?25hCollecting watchdog>=0.8.3 \u001b[?25l Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 12.1MB/s \u001b[?25hCollecting subprocess32>=3.5.3 \u001b[?25l Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 9.9MB/s \u001b[?25hCollecting shortuuid>=0.5.0 Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl Collecting configparser>=3.8.1 Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (5.4.8) Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (7.352.0) Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (7.1.2) Collecting gql==0.2.0 Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz Collecting docker-pycreds>=0.4.0 Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (3.13) Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (2.8.1) Collecting sentry-sdk>=0.4.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/8f/0f/e6ae366e926589878f2b1e41485473ee0368e5b1b62fd0f8c0bc8311eb75/sentry_sdk-0.17.0-py2.py3-none-any.whl (115kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 48.5MB/s \u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.6.20) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3) Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.4.3) Collecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 48.3MB/s \u001b[?25hCollecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 46.8MB/s \u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (3.0.12) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (20.4) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (0.7) Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.12.4) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.16.0) Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9) Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (5.1.1) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.14.47) Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.1.0) Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.14.1) Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (7.0.0) Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.1.1) Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.5.1) Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.10.1) Requirement already satisfied: botocore>=1.13.44 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.17.47) Collecting validators Downloading https://files.pythonhosted.org/packages/89/3b/23e14394d0a719d1a9f2e1944a1d227ac7107a3383aa7e8eba60003e7266/validators-0.18.0-py3-none-any.whl Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.8.1) Collecting enum-compat Downloading https://files.pythonhosted.org/packages/55/ae/467bc4509246283bb59746e21a1a2f5a8aecbef56b1fa6eaca78cd438c8b/enum_compat-0.0.3-py3-none-any.whl Collecting blinker \u001b[?25l Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 50.3MB/s \u001b[?25hCollecting pydeck>=0.1.dev5 \u001b[?25l Downloading https://files.pythonhosted.org/packages/51/1e/296f4108bf357e684617a776ecaf06ee93b43e30c35996dfac1aa985aa6c/pydeck-0.5.0b1-py2.py3-none-any.whl (4.4MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4MB 48.5MB/s \u001b[?25hCollecting base58 Downloading https://files.pythonhosted.org/packages/3c/03/58572025c77b9e6027155b272a1b96298e711cd4f95c24967f7137ab0c4b/base58-2.0.1-py3-none-any.whl Collecting gitdb<5,>=4.0.1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 9.7MB/s \u001b[?25hCollecting pathtools>=0.1.1 Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz Collecting graphql-core<2,>=0.5.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 8.7MB/s \u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb->simpletransformers) (2.3) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0) Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->simpletransformers) (2.4.7) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (49.6.0) Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit->simpletransformers) (0.10.0) Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit->simpletransformers) (0.3.3) Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3) Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0) Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.2) Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.10.0) Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore>=1.13.44->streamlit->simpletransformers) (0.15.2) Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit->simpletransformers) (4.4.2) Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.5.1) Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (4.3.3) Collecting ipykernel>=5.1.2; python_version >= \"3.4\" \u001b[?25l Downloading https://files.pythonhosted.org/packages/52/19/c2812690d8b340987eecd2cbc18549b1d130b94c5d97fcbe49f5f8710edf/ipykernel-5.3.4-py3-none-any.whl (120kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 47.7MB/s \u001b[?25hCollecting smmap<4,>=3.0.1 Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (1.1.1) Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0) Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.7) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1) Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5) Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.1.3) Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.18) Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5) Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1) Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.6.3) Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1) Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (19.0.2) Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5) Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0) Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3) Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1) Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4) Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4) Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.2) Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.1.5) Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0) Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1) Building wheels for collected packages: seqeval, watchdog, subprocess32, gql, sacremoses, blinker, pathtools, graphql-core Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=0c9e5b916f7c76a179fa625f0d8b93874fafd41ad962954fa5ab259a75769366 Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68 Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73873 sha256=e29159085739fda9cb7db78f7615b4d1de26b9b9c3bb0d9c1fa953d71ec41687 Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7 Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=627bc96100c3c3a95698ad43676e7a58057d6923b578f50ea6b4a8eebeec6dcf Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1 Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=178c6159f4f70ea80a4fb088ac0c6a8bac758e56266ff085915176cd0f18466a Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23 Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=85e8b314242d4c1921d352f3e6058ec6559835318e580eb8cda29ce54af540c8 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for blinker: filename=blinker-1.4-cp36-none-any.whl size=13450 sha256=e5ebfa369442a0bc4bb96651d7cf1a5f71257f0fe410bb1c25309f31eef4bdeb Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89 Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=8521122511ac313d2288474c30aa87876f562f34b1f1a412ac1294050011427b Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843 Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104651 sha256=5647c6aa90c5c7b0472476dfa74acf6b1d42386915e37c50856c526b4d79c271 Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5 Successfully built seqeval watchdog subprocess32 gql sacremoses blinker pathtools graphql-core \u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.3.4 which is incompatible.\u001b[0m \u001b[31mERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m Installing collected packages: smmap, gitdb, GitPython, pathtools, watchdog, subprocess32, shortuuid, configparser, graphql-core, gql, docker-pycreds, sentry-sdk, wandb, tqdm, seqeval, tokenizers, sentencepiece, sacremoses, transformers, tensorboardx, validators, enum-compat, blinker, ipykernel, pydeck, base58, streamlit, simpletransformers Found existing installation: tqdm 4.41.1 Uninstalling tqdm-4.41.1: Successfully uninstalled tqdm-4.41.1 Found existing installation: ipykernel 4.10.1 Uninstalling ipykernel-4.10.1: Successfully uninstalled ipykernel-4.10.1 Successfully installed GitPython-3.1.7 base58-2.0.1 blinker-1.4 configparser-5.0.0 docker-pycreds-0.4.0 enum-compat-0.0.3 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 ipykernel-5.3.4 pathtools-0.1.2 pydeck-0.5.0b1 sacremoses-0.0.43 sentencepiece-0.1.91 sentry-sdk-0.17.0 seqeval-0.0.12 shortuuid-1.0.1 simpletransformers-0.47.3 smmap-3.0.4 streamlit-0.65.2 subprocess32-3.5.4 tensorboardx-2.1 tokenizers-0.8.1 tqdm-4.48.2 transformers-3.0.2 validators-0.18.0 wandb-0.9.5 watchdog-0.10.3 from simpletransformers.classification import ClassificationModel, ClassificationArgs import pandas as pd import logging logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # Preparing train data train_data = [ [\"Aragorn was the heir of Isildur\", \"true\"], [\"Frodo was the heir of Isildur\", \"false\"], ] train_df = pd.DataFrame(train_data) train_df.columns = [\"text\", \"labels\"] # Preparing eval data eval_data = [ [\"Theoden was the king of Rohan\", \"true\"], [\"Merry was the king of Rohan\", \"false\"], ] eval_df = pd.DataFrame(eval_data) eval_df.columns = [\"text\", \"labels\"] # Optional model configuration model_args = ClassificationArgs() model_args.num_train_epochs=1 model_args.labels_list = [\"true\", \"false\"] # Create a ClassificationModel model = ClassificationModel( \"roberta\", \"roberta-base\", args=model_args,use_cuda=False ) # Train the model model.train_model(train_df) # Evaluate the model result, model_outputs, wrong_predictions = model.eval_model(eval_df) # Make predictions with the model predictions, raw_outputs = model.predict([\"Sam was a Wizard\"]) INFO:filelock:Lock 140155715063760 acquired on /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri\u2026 INFO:filelock:Lock 140155715063760 released on /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock WARNING:transformers.modeling_utils:Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight'] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). WARNING:transformers.modeling_utils:Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. INFO:filelock:Lock 140155715068592 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti\u2026 INFO:filelock:Lock 140155715068592 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock INFO:filelock:Lock 140155714555408 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti\u2026 INFO:filelock:Lock 140155714555408 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i\u2026 HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 1', max=1.0, style=ProgressStyle(descr\u2026 /usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/. INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=1.0, style=ProgressStyle(descrip\u2026 /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp) INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 1, 'tn': 0, 'fp': 1, 'fn': 0, 'eval_loss': 0.696165144443512} INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value=''))) predictions ['false'] ls outputs/ \u001b[0m\u001b[01;34mcheckpoint-1-epoch-1\u001b[0m/ merges.txt special_tokens_map.json vocab.json config.json model_args.json tokenizer_config.json eval_results.txt pytorch_model.bin training_args.bin from simpletransformers.classification import ClassificationModel, ClassificationArgs import pandas as pd import logging logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # Preparing train data train_data = [ [\"Aragorn was the heir of Isildur\", \"true\"], [\"Frodo was the heir of Isildur\", \"false\"], ] train_df = pd.DataFrame(train_data) train_df.columns = [\"text\", \"labels\"] # Preparing eval data eval_data = [ [\"Theoden was the king of Rohan\", \"true\"], [\"Merry was the king of Rohan\", \"false\"], ] eval_df = pd.DataFrame(eval_data) eval_df.columns = [\"text\", \"labels\"] # Optional model configuration model_args = ClassificationArgs() model_args.num_train_epochs=1 model_args.labels_list = [\"true\", \"false\"] model_args.overwrite_output_dir = True # Create a ClassificationModel model = ClassificationModel( \"roberta\", \"outputs/\", args=model_args,use_cuda=False ) # Train the model model.train_model(train_df) # Evaluate the model result, model_outputs, wrong_predictions = model.eval_model(eval_df) # Make predictions with the model predictions, raw_outputs = model.predict([\"Sam was a Wizard\"]) INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i\u2026 INFO:simpletransformers.classification.classification_model: Starting fine-tuning. HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 1', max=1.0, style=ProgressStyle(descr\u2026 /usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/. INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, description='Running Evaluation', max=1.0, style=ProgressStyle(descrip\u2026 /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp) INFO:simpletransformers.classification.classification_model:{'mcc': 0.0, 'tp': 1, 'tn': 0, 'fp': 1, 'fn': 0, 'eval_loss': 0.696165144443512} INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value=''))) import logging import pandas as pd import sklearn import wandb from simpletransformers.classification import ( ClassificationArgs, ClassificationModel, ) sweep_config = { \"method\": \"bayes\", # grid, random \"metric\": {\"name\": \"train_loss\", \"goal\": \"minimize\"}, \"parameters\": { \"num_train_epochs\": {\"values\": [2, 3, 5]}, \"learning_rate\": {\"min\": 5e-5, \"max\": 4e-4}, }, } sweep_id = wandb.sweep(sweep_config, project=\"Simple Sweep\") logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # Preparing train data train_data = [ [\"Aragorn was the heir of Isildur\", \"true\"], [\"Frodo was the heir of Isildur\", \"false\"], ] train_df = pd.DataFrame(train_data) train_df.columns = [\"text\", \"labels\"] # Preparing eval data eval_data = [ [\"Theoden was the king of Rohan\", \"true\"], [\"Merry was the king of Rohan\", \"false\"], ] eval_df = pd.DataFrame(eval_data) eval_df.columns = [\"text\", \"labels\"] model_args = ClassificationArgs() model_args.reprocess_input_data = True model_args.overwrite_output_dir = True model_args.evaluate_during_training = True model_args.manual_seed = 4 model_args.train_batch_size = 16 model_args.eval_batch_size = 8 model_args.labels_list = [\"true\", \"false\"] model_args.wandb_project = \"Simple Sweep\" def train(): # Initialize a new wandb run wandb.init() # Create a TransformerModel model = ClassificationModel( \"roberta\", \"roberta-base\", use_cuda=False, args=model_args, sweep_config=wandb.config, ) # Train the model model.train_model(train_df, eval_df=eval_df) # Evaluate the model model.eval_model(eval_df) # Sync wandb wandb.join() wandb.agent(sweep_id, train) Create sweep with ID: 03byb6mp Sweep URL: https://app.wandb.ai/ankur310794/Simple%20Sweep/sweeps/03byb6mp INFO:wandb.wandb_agent:Running runs: [] INFO:wandb.wandb_agent:Agent received command: run INFO:wandb.wandb_agent:Agent starting run with config: learning_rate: 0.00015971839686415725 num_train_epochs: 3 wandb: Agent Starting Run: zt8m9f5b with config: learning_rate: 0.00015971839686415725 num_train_epochs: 3 wandb: Agent Started Run: zt8m9f5b Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/> Project page: <a href=\"https://app.wandb.ai/ankur310794/Simple%20Sweep\" target=\"_blank\">https://app.wandb.ai/ankur310794/Simple%20Sweep</a><br/> Sweep page: <a href=\"https://app.wandb.ai/ankur310794/Simple%20Sweep/sweeps/03byb6mp\" target=\"_blank\">https://app.wandb.ai/ankur310794/Simple%20Sweep/sweeps/03byb6mp</a><br/> Run page: https://app.wandb.ai/ankur310794/Simple%20Sweep/runs/zt8m9f5b INFO:wandb.run_manager:system metrics and metadata threads started INFO:wandb.run_manager:checking resume status, waiting at most 10 seconds INFO:wandb.run_manager:resuming run from id: UnVuOnYxOnp0OG05ZjViOlNpbXBsZSBTd2VlcDphbmt1cjMxMDc5NA== INFO:wandb.run_manager:upserting run before process can begin, waiting at most 10 seconds INFO:wandb.run_manager:saving pip packages INFO:wandb.run_manager:initializing streaming files api INFO:wandb.run_manager:unblocking file change observer, beginning sync with W&B servers INFO:wandb.run_manager:file/dir modified: /content/wandb/run-20200827_001734-zt8m9f5b/config.yaml INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001734-zt8m9f5b/wandb-history.jsonl INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001734-zt8m9f5b/requirements.txt INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001734-zt8m9f5b/wandb-summary.json INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001734-zt8m9f5b/wandb-events.jsonl INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001734-zt8m9f5b/wandb-metadata.json INFO:wandb.wandb_agent:Running runs: ['zt8m9f5b'] WARNING:transformers.modeling_utils:Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight'] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). WARNING:transformers.modeling_utils:Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i\u2026 Logging results to Weights & Biases (Documentation) . Project page: https://app.wandb.ai/ankur310794/Simple%20Sweep Run page: https://app.wandb.ai/ankur310794/Simple%20Sweep/runs/s4yq3tsr INFO:wandb.run_manager:system metrics and metadata threads started INFO:wandb.run_manager:checking resume status, waiting at most 10 seconds INFO:wandb.run_manager:resuming run from id: UnVuOnYxOnM0eXEzdHNyOlNpbXBsZSBTd2VlcDphbmt1cjMxMDc5NA== INFO:wandb.run_manager:upserting run before process can begin, waiting at most 10 seconds INFO:wandb.run_manager:saving pip packages INFO:wandb.run_manager:initializing streaming files api INFO:wandb.run_manager:unblocking file change observer, beginning sync with W&B servers HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 3', max=1.0, style=ProgressStyle(descr\u2026 INFO:wandb.run_manager:file/dir modified: /content/wandb/run-20200827_001741-s4yq3tsr/config.yaml INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001741-s4yq3tsr/wandb-history.jsonl INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001741-s4yq3tsr/wandb-metadata.json INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001741-s4yq3tsr/wandb-events.jsonl INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001741-s4yq3tsr/wandb-summary.json INFO:wandb.run_manager:file/dir created: /content/wandb/run-20200827_001741-s4yq3tsr/requirements.txt Process Process-11: Traceback (most recent call last): File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap self.run() File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run self._target(*self._args, **self._kwargs) File \"/usr/local/lib/python3.6/dist-packages/wandb/wandb_agent.py\", line 64, in _start function() File \"<ipython-input-11-75bdbb0e00ff>\", line 67, in train model.train_model(train_df, eval_df=eval_df) File \"/usr/local/lib/python3.6/dist-packages/simpletransformers/classification/classification_model.py\", line 306, in train_model **kwargs, File \"/usr/local/lib/python3.6/dist-packages/simpletransformers/classification/classification_model.py\", line 494, in train loss.backward() File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 185, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 127, in backward allow_unreachable=True) # allow_unreachable flag RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork \u001b[34m\u001b[1mwandb\u001b[0m: Ctrl-c pressed. Waiting for runs to end. Press ctrl-c again to terminate them. from simpletransformers.classification import ClassificationModel, ClassificationArgs import pandas as pd import logging logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # Preparing train data train_data = [ [\"Aragorn was the heir of Isildur\", 1], [\"Frodo was the heir of Isildur\", 0], ] train_df = pd.DataFrame(train_data) train_df.columns = [\"text\", \"labels\"] # Preparing eval data eval_data = [ [\"Theoden was the king of Rohan\", 1], [\"Merry was the king of Rohan\", 0], ] eval_df = pd.DataFrame(eval_data) eval_df.columns = [\"text\", \"labels\"] # Train only the classifier layers model_args = ClassificationArgs() model_args.train_custom_parameters_only = True model_args.custom_parameter_groups = [ { \"params\": [\"classifier.weight\"], \"lr\": 1e-3, }, { \"params\": [\"classifier.bias\"], \"lr\": 1e-3, \"weight_decay\": 0.0, }, ] model_args.overwrite_output_dir=True # Create a ClassificationModel model = ClassificationModel( \"bert\", \"bert-base-cased\", args=model_args,use_cuda=False ) # Train the model model.train_model(train_df) WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value=''))) HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i\u2026 HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 1', max=1.0, style=ProgressStyle(descr\u2026 /usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs/.","title":"Simpletransformers 2"},{"location":"TAPAS_fine_tuning_in_tf/","text":"!pip install git+https://github.com/huggingface/transformers.git !pip install tensorflow_probability -qqq import requests, zipfile, io import os def download_files(dir_name): if not os.path.exists(dir_name): # 28 training examples from the SQA training set + table csv data urls = [\"https://www.dropbox.com/s/2p6ez9xro357i63/sqa_train_set_28_examples.zip?dl=1\", \"https://www.dropbox.com/s/abhum8ssuow87h6/table_csv.zip?dl=1\" ] for url in urls: r = requests.get(url) z = zipfile.ZipFile(io.BytesIO(r.content)) z.extractall() dir_name = \"sqa_data\" download_files(dir_name) import pandas as pd data = pd.read_excel(\"sqa_train_set_28_examples.xlsx\") data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id annotator position question table_file answer_coordinates answer_text 0 nt-639 0 0 where are the players from? table_csv/203_149.csv ['(0, 4)', '(1, 4)', '(2, 4)', '(3, 4)', '(4, ... ['Louisiana State University', 'Valley HS (Las... 1 nt-639 0 1 which player went to louisiana state university? table_csv/203_149.csv ['(0, 1)'] ['Ben McDonald'] 2 nt-639 1 0 who are the players? table_csv/203_149.csv ['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ... ['Ben McDonald', 'Tyler Houston', 'Roger Salke... 3 nt-639 1 1 which ones are in the top 26 picks? table_csv/203_149.csv ['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ... ['Ben McDonald', 'Tyler Houston', 'Roger Salke... 4 nt-639 1 2 and of those, who is from louisiana state univ... table_csv/203_149.csv ['(0, 1)'] ['Ben McDonald'] .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-c6886234-94fb-4aba-aa0a-c3ce77f062d4 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-c6886234-94fb-4aba-aa0a-c3ce77f062d4'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } import ast def _parse_answer_coordinates(answer_coordinate_str): \"\"\"Parses the answer_coordinates of a question. Args: answer_coordinate_str: A string representation of a Python list of tuple strings. For example: \"['(1, 4)','(1, 3)', ...]\" \"\"\" try: answer_coordinates = [] # make a list of strings coords = ast.literal_eval(answer_coordinate_str) # parse each string as a tuple for row_index, column_index in sorted( ast.literal_eval(coord) for coord in coords): answer_coordinates.append((row_index, column_index)) except SyntaxError: raise ValueError('Unable to evaluate %s' % answer_coordinate_str) return answer_coordinates def _parse_answer_text(answer_text): \"\"\"Populates the answer_texts field of `answer` by parsing `answer_text`. Args: answer_text: A string representation of a Python list of strings. For example: \"[u'test', u'hello', ...]\" answer: an Answer object. \"\"\" try: answer = [] for value in ast.literal_eval(answer_text): answer.append(value) except SyntaxError: raise ValueError('Unable to evaluate %s' % answer_text) return answer data['answer_coordinates'] = data['answer_coordinates'].apply(lambda coords_str: _parse_answer_coordinates(coords_str)) data['answer_text'] = data['answer_text'].apply(lambda txt: _parse_answer_text(txt)) data.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id annotator position question table_file answer_coordinates answer_text 0 nt-639 0 0 where are the players from? table_csv/203_149.csv [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4... [Louisiana State University, Valley HS (Las Ve... 1 nt-639 0 1 which player went to louisiana state university? table_csv/203_149.csv [(0, 1)] [Ben McDonald] 2 nt-639 1 0 who are the players? table_csv/203_149.csv [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1... [Ben McDonald, Tyler Houston, Roger Salkeld, J... 3 nt-639 1 1 which ones are in the top 26 picks? table_csv/203_149.csv [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1... [Ben McDonald, Tyler Houston, Roger Salkeld, J... 4 nt-639 1 2 and of those, who is from louisiana state univ... table_csv/203_149.csv [(0, 1)] [Ben McDonald] 5 nt-639 2 0 who are the players in the top 26? table_csv/203_149.csv [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1... [Ben McDonald, Tyler Houston, Roger Salkeld, J... 6 nt-639 2 1 of those, which one was from louisiana state u... table_csv/203_149.csv [(0, 1)] [Ben McDonald] 7 nt-11649 0 0 what are all the names of the teams? table_csv/204_135.csv [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1... [Cordoba CF, CD Malaga, Granada CF, UD Las Pal... 8 nt-11649 0 1 of these, which teams had any losses? table_csv/204_135.csv [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1... [Cordoba CF, CD Malaga, Granada CF, UD Las Pal... 9 nt-11649 0 2 of these teams, which had more than 21 losses? table_csv/204_135.csv [(15, 1)] [CD Villarrobledo] .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-4f00d12e-e50f-4bb7-b3f6-d47c3db7f63d button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4f00d12e-e50f-4bb7-b3f6-d47c3db7f63d'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } def get_sequence_id(example_id, annotator): if \"-\" in str(annotator): raise ValueError('\"-\" not allowed in annotator.') return f\"{example_id}-{annotator}\" data['sequence_id'] = data.apply(lambda x: get_sequence_id(x.id, x.annotator), axis=1) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id annotator position question table_file answer_coordinates answer_text sequence_id 0 nt-639 0 0 where are the players from? table_csv/203_149.csv [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4... [Louisiana State University, Valley HS (Las Ve... nt-639-0 1 nt-639 0 1 which player went to louisiana state university? table_csv/203_149.csv [(0, 1)] [Ben McDonald] nt-639-0 2 nt-639 1 0 who are the players? table_csv/203_149.csv [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1... [Ben McDonald, Tyler Houston, Roger Salkeld, J... nt-639-1 3 nt-639 1 1 which ones are in the top 26 picks? table_csv/203_149.csv [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1... [Ben McDonald, Tyler Houston, Roger Salkeld, J... nt-639-1 4 nt-639 1 2 and of those, who is from louisiana state univ... table_csv/203_149.csv [(0, 1)] [Ben McDonald] nt-639-1 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-5a485d31-a71d-4e7f-986e-e000ed1f0c74 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-5a485d31-a71d-4e7f-986e-e000ed1f0c74'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } # let's group table-question pairs by sequence id, and remove some columns we don't need grouped = data.groupby(by='sequence_id').agg(lambda x: x.tolist()) grouped = grouped.drop(columns=['id', 'annotator', 'position']) grouped['table_file'] = grouped['table_file'].apply(lambda x: x[0]) grouped.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } question table_file answer_coordinates answer_text sequence_id ns-1292-0 [who are all the athletes?, where are they fro... table_csv/204_521.csv [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ... [[Tommy Green, Janis Dalins, Ugo Frigerio, Kar... nt-10730-0 [what was the production numbers of each revol... table_csv/203_253.csv [[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ... [[1,900 (estimated), 14,500 (estimated), 6,000... nt-10730-1 [what three revolver models had the least amou... table_csv/203_253.csv [[(0, 0), (6, 0), (7, 0)], [(0, 0)]] [[Remington-Beals Army Model Revolver, New Mod... nt-10730-2 [what are all of the remington models?, how ma... table_csv/203_253.csv [[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, ... [[Remington-Beals Army Model Revolver, Remingt... nt-11649-0 [what are all the names of the teams?, of thes... table_csv/204_135.csv [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ... [[Cordoba CF, CD Malaga, Granada CF, UD Las Pa... nt-11649-1 [what are the losses?, what team had more than... table_csv/204_135.csv [[(0, 6), (1, 6), (2, 6), (3, 6), (4, 6), (5, ... [[6, 6, 9, 10, 10, 12, 12, 11, 13, 14, 15, 14,... nt-11649-2 [what were all the teams?, what were the loss ... table_csv/204_135.csv [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ... [[Cordoba CF, CD Malaga, Granada CF, UD Las Pa... nt-639-0 [where are the players from?, which player wen... table_csv/203_149.csv [[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ... [[Louisiana State University, Valley HS (Las V... nt-639-1 [who are the players?, which ones are in the t... table_csv/203_149.csv [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ... [[Ben McDonald, Tyler Houston, Roger Salkeld, ... nt-639-2 [who are the players in the top 26?, of those,... table_csv/203_149.csv [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ... [[Ben McDonald, Tyler Houston, Roger Salkeld, ... .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-3ad59c92-9893-4667-b59a-2f3e3d48d94c button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-3ad59c92-9893-4667-b59a-2f3e3d48d94c'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } # path to the directory containing all csv files table_csv_path = \"table_csv\" item = grouped.iloc[0] table = pd.read_csv(table_csv_path + item.table_file[9:]).astype(str) display(table) print(\"\") print(item.question) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Rank Name Nationality Time (hand) Notes 0 nan Tommy Green Great Britain 4:50:10 OR 1 nan Janis Dalins Latvia 4:57:20 nan 2 nan Ugo Frigerio Italy 4:59:06 nan 3 4.0 Karl Hahnel Germany 5:06:06 nan 4 5.0 Ettore Rivolta Italy 5:07:39 nan 5 6.0 Paul Sievert Germany 5:16:41 nan 6 7.0 Henri Quintric France 5:27:25 nan 7 8.0 Ernie Crosbie United States 5:28:02 nan 8 9.0 Bill Chisholm United States 5:51:00 nan 9 10.0 Alfred Maasik Estonia 6:19:00 nan 10 nan Henry Cieman Canada nan DNF 11 nan John Moralis Greece nan DNF 12 nan Francesco Pretti Italy nan DNF 13 nan Arthur Tell Schwab Switzerland nan DNF 14 nan Harry Hinkel United States nan DNF .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-a87bdcd9-f979-453f-998d-d8130202968b button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-a87bdcd9-f979-453f-998d-d8130202968b'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } ['who are all the athletes?', 'where are they from?', 'along with paul sievert, which athlete is from germany?'] import tensorflow as tf from transformers import TapasTokenizer # initialize the tokenizer tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base\") Downloading: 0%| | 0.00/256k [00:00<?, ?B/s] Downloading: 0%| | 0.00/154 [00:00<?, ?B/s] Downloading: 0%| | 0.00/490 [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.49k [00:00<?, ?B/s] encoding = tokenizer(table=table, queries=item.question, answer_coordinates=item.answer_coordinates, answer_text=item.answer_text, truncation=True, padding=\"max_length\", return_tensors=\"tf\") encoding.keys() dict_keys(['input_ids', 'labels', 'numeric_values', 'numeric_values_scale', 'token_type_ids', 'attention_mask']) tokenizer.decode(encoding[\"input_ids\"][0]) '[CLS] who are all the athletes? [SEP] rank name nationality time ( hand ) notes [EMPTY] tommy green great britain 4 : 50 : 10 or [EMPTY] janis dalins latvia 4 : 57 : 20 [EMPTY] [EMPTY] ugo frigerio italy 4 : 59 : 06 [EMPTY] 4. 0 karl hahnel germany 5 : 06 : 06 [EMPTY] 5. 0 ettore rivolta italy 5 : 07 : 39 [EMPTY] 6. 0 paul sievert germany 5 : 16 : 41 [EMPTY] 7. 0 henri quintric france 5 : 27 : 25 [EMPTY] 8. 0 ernie crosbie united states 5 : 28 : 02 [EMPTY] 9. 0 bill chisholm united states 5 : 51 : 00 [EMPTY] 10. 0 alfred maasik estonia 6 : 19 : 00 [EMPTY] [EMPTY] henry cieman canada [EMPTY] dnf [EMPTY] john moralis greece [EMPTY] dnf [EMPTY] francesco pretti italy [EMPTY] dnf [EMPTY] arthur tell schwab switzerland [EMPTY] dnf [EMPTY] harry hinkel united states [EMPTY] dnf [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]' encoding[\"labels\"][0] <tf.Tensor: shape=(512,), dtype=int32, numpy= array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)> class TableDataset: def __init__(self, df, tokenizer): self.df = df self.tokenizer = tokenizer def __iter__(self): for idx in range(self.__len__()): item = self.df.iloc[idx] table = pd.read_csv(table_csv_path + item.table_file[9:]).astype(str) # TapasTokenizer expects the table data to be text only if item.position != 0: # use the previous table-question pair to correctly set the prev_labels token type ids previous_item = self.df.iloc[idx-1] encoding = self.tokenizer(table=table, queries=[previous_item.question, item.question], answer_coordinates=[previous_item.answer_coordinates, item.answer_coordinates], answer_text=[previous_item.answer_text, item.answer_text], padding=\"max_length\", truncation=True, return_tensors=\"tf\" ) # use encodings of second table-question pair in the batch encoding = {key: val[-1] for key, val in encoding.items()} else: # this means it's the first table-question pair in a sequence encoding = self.tokenizer(table=table, queries=item.question, answer_coordinates=item.answer_coordinates, answer_text=item.answer_text, padding=\"max_length\", truncation=True, return_tensors=\"tf\" ) # remove the batch dimension which the tokenizer adds encoding = {key: tf.squeeze(val,0) for key, val in encoding.items()} yield encoding['input_ids'],encoding['attention_mask'],encoding['numeric_values'], \\ encoding['numeric_values_scale'], encoding['token_type_ids'], encoding['labels'] def __len__(self): return len(self.df) __call__ = __iter__ train_dataset = TableDataset(df=data, tokenizer=tokenizer) output_signature = ( tf.TensorSpec(shape=(512,), dtype=tf.int32), tf.TensorSpec(shape=(512,), dtype=tf.int32), tf.TensorSpec(shape=(512,), dtype=tf.float32), tf.TensorSpec(shape=(512,), dtype=tf.float32), tf.TensorSpec(shape=(512,7), dtype=tf.int32), tf.TensorSpec(shape=(512,), dtype=tf.int32) ) train_dataloader = tf.data.Dataset.from_generator(train_dataset,output_signature=output_signature).batch(2) batch = next(iter(train_dataloader)) batch[0].shape TensorShape([2, 512]) batch[4].shape TensorShape([2, 512, 7]) batch[2].shape TensorShape([2, 512]) tokenizer.decode(batch[0][1]) '[CLS] which player went to louisiana state university? [SEP] pick player team position school 1 ben mcdonald baltimore orioles rhp louisiana state university 2 tyler houston atlanta braves c valley hs ( las vegas, nv ) 3 roger salkeld seattle mariners rhp saugus ( ca ) hs 4 jeff jackson philadelphia phillies of simeon hs ( chicago, il ) 5 donald harris texas rangers of texas tech university 6 paul coleman saint louis cardinals of frankston ( tx ) hs 7 frank thomas chicago white sox 1b auburn university 8 earl cunningham chicago cubs of lancaster ( sc ) hs 9 kyle abbott california angels lhp long beach state university 10 charles johnson montreal expos c westwood hs ( fort pierce, fl ) 11 calvin murray cleveland indians 3b w. t. white high school ( dallas, tx ) 12 jeff juden houston astros rhp salem ( ma ) hs 13 brent mayne kansas city royals c cal state fullerton 14 steve hosey san francisco giants of fresno state university 15 kiki jones los angeles dodgers rhp hillsborough hs ( tampa, fl ) 16 greg blosser boston red sox of sarasota ( fl ) hs 17 cal eldred milwaukee brewers rhp university of iowa 18 willie greene pittsburgh pirates ss jones county hs ( gray, ga ) 19 eddie zosky toronto blue jays ss fresno state university 20 scott bryant cincinnati reds of university of texas 21 greg gohr detroit tigers rhp santa clara university 22 tom goodwin los angeles dodgers of fresno state university 23 mo vaughn boston red sox 1b seton hall university 24 alan zinter new york mets c university of arizona 25 chuck knoblauch minnesota twins 2b texas a & m university 26 scott burrell seattle mariners rhp hamden ( ct ) hs [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]' assert sum(batch[4][0][:,3]) == 0 print(sum(batch[4][1][:,3])) tf.Tensor(132, shape=(), dtype=int32) for id, prev_label in zip(batch[0][1], batch[4][1][:,3]): if id != 0: print(tokenizer.decode([id]), prev_label.numpy().item()) [CLS] 0 which 0 player 0 went 0 to 0 louisiana 0 state 0 university 0 ? 0 [SEP] 0 pick 0 player 0 team 0 position 0 school 0 1 0 ben 0 mcdonald 0 baltimore 0 orioles 0 r 0 ##hp 0 louisiana 1 state 1 university 1 2 0 tyler 0 houston 0 atlanta 0 braves 0 c 0 valley 1 hs 1 ( 1 las 1 vegas 1 , 1 n 1 ##v 1 ) 1 3 0 roger 0 sal 0 ##kel 0 ##d 0 seattle 0 mariners 0 r 0 ##hp 0 sa 1 ##ug 1 ##us 1 ( 1 ca 1 ) 1 hs 1 4 0 jeff 0 jackson 0 philadelphia 0 phillies 0 of 0 simeon 1 hs 1 ( 1 chicago 1 , 1 il 1 ) 1 5 0 donald 0 harris 0 texas 0 rangers 0 of 0 texas 1 tech 1 university 1 6 0 paul 0 coleman 0 saint 0 louis 0 cardinals 0 of 0 franks 1 ##ton 1 ( 1 tx 1 ) 1 hs 1 7 0 frank 0 thomas 0 chicago 0 white 0 sox 0 1b 0 auburn 1 university 1 8 0 earl 0 cunningham 0 chicago 0 cubs 0 of 0 lancaster 1 ( 1 sc 1 ) 1 hs 1 9 0 kyle 0 abbott 0 california 0 angels 0 l 0 ##hp 0 long 1 beach 1 state 1 university 1 10 0 charles 0 johnson 0 montreal 0 expo 0 ##s 0 c 0 westwood 1 hs 1 ( 1 fort 1 pierce 1 , 1 fl 1 ) 1 11 0 calvin 0 murray 0 cleveland 0 indians 0 3 0 ##b 0 w 1 . 1 t 1 . 1 white 1 high 1 school 1 ( 1 dallas 1 , 1 tx 1 ) 1 12 0 jeff 0 jude 0 ##n 0 houston 0 astros 0 r 0 ##hp 0 salem 1 ( 1 ma 1 ) 1 hs 1 13 0 brent 0 may 0 ##ne 0 kansas 0 city 0 royals 0 c 0 cal 1 state 1 fuller 1 ##ton 1 14 0 steve 0 hose 0 ##y 0 san 0 francisco 0 giants 0 of 0 fresno 1 state 1 university 1 15 0 ki 0 ##ki 0 jones 0 los 0 angeles 0 dodgers 0 r 0 ##hp 0 hillsborough 1 hs 1 ( 1 tampa 1 , 1 fl 1 ) 1 16 0 greg 0 b 0 ##los 0 ##ser 0 boston 0 red 0 sox 0 of 0 sara 1 ##so 1 ##ta 1 ( 1 fl 1 ) 1 hs 1 17 0 cal 0 el 0 ##dre 0 ##d 0 milwaukee 0 brewers 0 r 0 ##hp 0 university 1 of 1 iowa 1 18 0 willie 0 greene 0 pittsburgh 0 pirates 0 ss 0 jones 1 county 1 hs 1 ( 1 gray 1 , 1 ga 1 ) 1 19 0 eddie 0 z 0 ##os 0 ##ky 0 toronto 0 blue 0 jays 0 ss 0 fresno 1 state 1 university 1 20 0 scott 0 bryant 0 cincinnati 0 reds 0 of 0 university 1 of 1 texas 1 21 0 greg 0 go 0 ##hr 0 detroit 0 tigers 0 r 0 ##hp 0 santa 1 clara 1 university 1 22 0 tom 0 goodwin 0 los 0 angeles 0 dodgers 0 of 0 fresno 1 state 1 university 1 23 0 mo 0 vaughn 0 boston 0 red 0 sox 0 1b 0 seton 1 hall 1 university 1 24 0 alan 0 z 0 ##int 0 ##er 0 new 0 york 0 mets 0 c 0 university 1 of 1 arizona 1 25 0 chuck 0 knob 0 ##lau 0 ##ch 0 minnesota 0 twins 0 2 0 ##b 0 texas 1 a 1 & 1 m 1 university 1 26 0 scott 0 burr 0 ##ell 0 seattle 0 mariners 0 r 0 ##hp 0 ham 1 ##den 1 ( 1 ct 1 ) 1 hs 1 from transformers import TFTapasForQuestionAnswering model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base\") Downloading: 0%| | 0.00/422M [00:00<?, ?B/s] All model checkpoint layers were used when initializing TFTapasForQuestionAnswering. Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base and are newly initialized: ['compute_column_logits', 'dropout_37', 'compute_token_logits'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. model.config.num_aggregation_labels 0 optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) for epoch in range(10): # loop over the dataset multiple times print(\"Epoch:\", epoch) for idx, batch in enumerate(train_dataloader): # get the inputs; input_ids = batch[0] attention_mask = batch[1] token_type_ids = batch[4] labels = batch[-1] with tf.GradientTape() as tape: outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels, training=True) print(\"loss: \",outputs.loss.numpy().item()) grads = tape.gradient(outputs.loss, model.trainable_weights) optimizer.apply_gradients(zip(grads, model.trainable_weights)) Epoch: 0 loss: 2.2713990211486816 WARNING:tensorflow:Gradients do not exist for variables ['tf_tapas_for_question_answering/tapas/pooler/dense/kernel:0', 'tf_tapas_for_question_answering/tapas/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument? loss: 2.0187385082244873 WARNING:tensorflow:Gradients do not exist for variables ['tf_tapas_for_question_answering/tapas/pooler/dense/kernel:0', 'tf_tapas_for_question_answering/tapas/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument? loss: 1.3549939393997192 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-33-dbbf9094cd94> in <module>() 16 17 print(\"loss: \",outputs.loss.numpy().item()) ---> 18 grads = tape.gradient(outputs.loss, model.trainable_weights) 19 optimizer.apply_gradients(zip(grads, model.trainable_weights)) /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients) 1088 output_gradients=output_gradients, 1089 sources_raw=flat_sources_raw, -> 1090 unconnected_gradients=unconnected_gradients) 1091 1092 if not self._persistent: /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients) 75 output_gradients, 76 sources_raw, ---> 77 compat.as_str(unconnected_gradients.value)) /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope) 157 gradient_name_scope += forward_pass_name_scope + \"/\" 158 with ops.name_scope(gradient_name_scope): --> 159 return grad_fn(mock_op, *out_grads) 160 else: 161 return grad_fn(mock_op, *out_grads) /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py in _BatchMatMulV2(op, grad) 1866 if not adj_x: 1867 if not adj_y: -> 1868 grad_x = math_ops.matmul(grad, y, adjoint_a=False, adjoint_b=True) 1869 grad_y = math_ops.matmul(x, grad, adjoint_a=True, adjoint_b=False) 1870 else: /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 148 filtered_tb = None 149 try: --> 150 return fn(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in op_dispatch_handler(*args, **kwargs) 1094 # Fallback dispatch system (dispatch v1): 1095 try: -> 1096 return dispatch_target(*args, **kwargs) 1097 except (TypeError, ValueError): 1098 # Note: convert_to_eager_tensor currently raises a ValueError, not a /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name) 3652 else: 3653 return gen_math_ops.batch_mat_mul_v2( -> 3654 a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name) 3655 3656 # Neither matmul nor sparse_matmul support adjoint, so we conjugate /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in batch_mat_mul_v2(x, y, adj_x, adj_y, name) 1570 try: 1571 _result = pywrap_tfe.TFE_Py_FastPathExecute( -> 1572 _ctx, \"BatchMatMulV2\", name, x, y, \"adj_x\", adj_x, \"adj_y\", adj_y) 1573 return _result 1574 except _core._NotOkStatusException as e: KeyboardInterrupt: import tensorflow_probability as tfp import collections import numpy as np def compute_prediction_sequence(model, data): \"\"\"Computes predictions using model's answers to the previous questions.\"\"\" # prepare data all_logits = [] prev_answers = None batch_size = inputs[\"input_ids\"].shape[0] input_ids = inputs[\"input_ids\"] attention_mask = inputs[\"attention_mask\"] token_type_ids = inputs[\"token_type_ids\"] token_type_ids_example = None for index in range(batch_size): # If sequences have already been processed, the token type IDs will be created according to the previous # answer. if prev_answers is not None: prev_labels_example = token_type_ids_example[:, 3] # shape (seq_len,) model_labels = np.zeros_like(prev_labels_example, dtype=np.int32) # shape (seq_len,) token_type_ids_example = token_type_ids[index].numpy() # shape (seq_len, 7) for i in range(model_labels.shape[0]): segment_id = token_type_ids_example[:, 0].tolist()[i] col_id = token_type_ids_example[:, 1].tolist()[i] - 1 row_id = token_type_ids_example[:, 2].tolist()[i] - 1 if row_id >= 0 and col_id >= 0 and segment_id == 1: model_labels[i] = int(prev_answers[(col_id, row_id)]) token_type_ids_example[:, 3] = model_labels input_ids_example = input_ids[index] attention_mask_example = attention_mask[index] # shape (seq_len,) token_type_ids_example = token_type_ids[index] # shape (seq_len, 7) outputs = model( input_ids=np.expand_dims(input_ids_example, axis=0), attention_mask=np.expand_dims(attention_mask_example, axis=0), token_type_ids=np.expand_dims(token_type_ids_example, axis=0), ) logits = outputs.logits all_logits.append(logits) dist_per_token = tfp.distributions.Bernoulli(logits=logits) probabilities = dist_per_token.probs_parameter() * tf.cast(attention_mask_example, tf.float32) coords_to_probs = collections.defaultdict(list) token_type_ids_example = token_type_ids_example.numpy() for i, p in enumerate(tf.squeeze(probabilities).numpy().tolist()): segment_id = token_type_ids_example[:, 0].tolist()[i] col = token_type_ids_example[:, 1].tolist()[i] - 1 row = token_type_ids_example[:, 2].tolist()[i] - 1 if col >= 0 and row >= 0 and segment_id == 1: coords_to_probs[(col, row)].append(p) prev_answers = {key: np.array(coords_to_probs[key]).mean() > 0.5 for key in coords_to_probs} logits_batch = tf.concat(tuple(all_logits), 0) return logits_batch data = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], 'Age': [\"56\", \"45\", \"59\"], 'Number of movies': [\"87\", \"53\", \"69\"], 'Date of birth': [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"]} queries = [\"How many movies has George Clooney played in?\", \"How old is he?\", \"What's his date of birth?\"] table = pd.DataFrame.from_dict(data) inputs = tokenizer(table=table, queries=queries, padding='max_length', return_tensors=\"tf\") logits = compute_prediction_sequence(model, inputs) predicted_answer_coordinates, = tokenizer.convert_logits_to_predictions(inputs, logits) predicted_answer_coordinates [[(0, 3), (1, 3), (2, 3)], [(0, 3), (1, 3), (2, 3)], [(0, 3), (1, 3), (2, 3)]] # handy helper function in case inference on Pandas dataframe answers = [] for coordinates in predicted_answer_coordinates: if len(coordinates) == 1: # only a single cell: answers.append(table.iat[coordinates[0]]) else: # multiple cells cell_values = [] for coordinate in coordinates: cell_values.append(table.iat[coordinate]) answers.append(\", \".join(cell_values)) display(table) print(\"\") for query, answer in zip(queries, answers): print(query) print(\"Predicted answer: \" + answer) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Actors Age Number of movies Date of birth 0 Brad Pitt 56 87 7 february 1967 1 Leonardo Di Caprio 45 53 10 june 1996 2 George Clooney 59 69 28 november 1967 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-eb723aa7-316e-4c7f-99bb-7e2b0be5ba2a button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-eb723aa7-316e-4c7f-99bb-7e2b0be5ba2a'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } How many movies has George Clooney played in? Predicted answer: 7 february 1967, 10 june 1996, 28 november 1967 How old is he? Predicted answer: 7 february 1967, 10 june 1996, 28 november 1967 What's his date of birth? Predicted answer: 7 february 1967, 10 june 1996, 28 november 1967 Reference https://github.com/kamalkraj/Tapas-Tutorial","title":"TAPAS fine tuning in tf"},{"location":"TAPAS_fine_tuning_in_tf/#reference","text":"https://github.com/kamalkraj/Tapas-Tutorial","title":"Reference"},{"location":"Using_Transformers_with_Fastai_Tutorial/","text":"Package Installation !git clone https://github.com/devkosal/fastai_roberta.git Cloning into 'fastai_roberta'... remote: Enumerating objects: 171, done.\u001b[K remote: Counting objects: 100% (171/171), done.\u001b[K remote: Compressing objects: 100% (121/121), done.\u001b[K remote: Total 171 (delta 91), reused 111 (delta 44), pack-reused 0\u001b[K Receiving objects: 100% (171/171), 25.46 MiB | 18.58 MiB/s, done. Resolving deltas: 100% (91/91), done. !pip install fastai==1.0.60 transformers==2.3.0 Collecting fastai==1.0.60 \u001b[?25l Downloading https://files.pythonhosted.org/packages/f5/e4/a7025bf28f303dbda0f862c09a7f957476fa92c9271643b4061a81bb595f/fastai-1.0.60-py3-none-any.whl (237kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 245kB 4.7MB/s \u001b[?25hCollecting transformers==2.3.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 7.4MB/s \u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (3.2.2) Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.3.2) Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.18.5) Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (7.0.0) Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (7.352.0) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (0.7) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.23.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (3.13) Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.7.1) Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.0.0) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (20.4) Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (0.7.0+cu101) Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (4.6.3) Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.2.4) Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.6.0+cu101) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.4.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.0.5) Collecting sentencepiece \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 7.0MB/s \u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.14.48) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.41.1) Collecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 26.1MB/s \u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.20) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (2.4.7) Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (2.8.1) Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (1.2.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (1.24.3) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (2020.6.20) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (3.0.4) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (2.10) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai==1.0.60) (1.15.0) Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.0.2) Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (0.7.1) Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (0.4.1) Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.1.3) Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (3.0.2) Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.0.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (49.6.0) Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (2.0.3) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (7.4.0) Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.0.2) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai==1.0.60) (0.16.0) Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==1.0.60) (2018.9) Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.10.0) Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.3.3) Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (1.17.48) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (0.16.0) Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai==1.0.60) (1.7.0) Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->transformers==2.3.0) (0.15.2) Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai==1.0.60) (3.1.0) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=aef96c81bb474827b2b34caa9690e68458a664de90731bcc580754ce52b74dd7 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: fastai, sentencepiece, sacremoses, transformers Found existing installation: fastai 1.0.61 Uninstalling fastai-1.0.61: Successfully uninstalled fastai-1.0.61 Successfully installed fastai-1.0.60 sacremoses-0.0.43 sentencepiece-0.1.91 transformers-2.3.0 Load And Set Configuration from fastai.text import * from fastai.metrics import * from transformers import RobertaTokenizer # Creating a config object to store task specific information class Config(dict): def __init__(self, **kwargs): super().__init__(**kwargs) for k, v in kwargs.items(): setattr(self, k, v) def set(self, key, val): self[key] = val setattr(self, key, val) config = Config( testing=True, seed = 2019, roberta_model_name='roberta-base', # can also be exchnaged with roberta-large max_lr=1e-5, epochs=1, use_fp16=False, bs=4, max_seq_len=256, num_labels = 2, hidden_dropout_prob=.05, hidden_size=768, # 1024 for roberta-large start_tok = \"<s>\", end_tok = \"</s>\", ) df = pd.read_csv(\"fastai_roberta/fastai_roberta_imdb/imdb_dataset.csv\") if config.testing: df = df[:5000] print(df.shape) (5000, 2) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } review sentiment 0 One of the other reviewers has mentioned that ... positive 1 A wonderful little production. <br /><br />The... positive 2 I thought this was a wonderful way to spend ti... positive 3 Basically there's a family where a little boy ... negative 4 Petter Mattei's \"Love in the Time of Money\" is... positive feat_cols = \"review\" label_cols = \"sentiment\" Setting Up the Tokenizer class FastAiRobertaTokenizer(BaseTokenizer): \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\" def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): self._pretrained_tokenizer = tokenizer self.max_seq_len = max_seq_len def __call__(self, *args, **kwargs): return self def tokenizer(self, t:str) -> List[str]: \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok] # create fastai tokenizer for roberta roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\") fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[]) # create fastai vocabulary for roberta path = Path() roberta_tok.save_vocabulary(path) with open('vocab.json', 'r') as f: roberta_vocab_dict = json.load(f) fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys())) # Setting up pre-processors class RobertaTokenizeProcessor(TokenizeProcessor): def __init__(self, tokenizer): super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False) class RobertaNumericalizeProcessor(NumericalizeProcessor): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None): \"\"\" Constructing preprocessors for Roberta We remove sos and eos tokens since we add that ourselves in the tokenizer. We also use a custom vocabulary to match the numericalization with the original Roberta model. \"\"\" return [RobertaTokenizeProcessor(tokenizer=tokenizer), RobertaNumericalizeProcessor(vocab=vocab)] Setting up the DataBunch # Creating a Roberta specific DataBunch class class RobertaDataBunch(TextDataBunch): \"Create a `TextDataBunch` suitable for training Roberta\" @classmethod def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1, pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch: \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\" datasets = cls._init_ds(train_ds, valid_ds, test_ds) val_bs = ifnone(val_bs, bs) collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards) train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs) train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs) dataloaders = [train_dl] for ds in datasets[1:]: lengths = [len(t) for t in ds.x.items] sampler = SortSampler(ds.x, key=lengths.__getitem__) dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs)) return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check) class RobertaTextList(TextList): _bunch = RobertaDataBunch _label_cls = TextList # loading the tokenizer and vocab processors processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab) # creating our databunch data = RobertaTextList.from_df(df, \".\", cols=feat_cols, processor=processor) \\ .split_by_rand_pct(seed=config.seed) \\ .label_from_df(cols=label_cols,label_cls=CategoryList) \\ .databunch(bs=config.bs, pad_first=False, pad_idx=0) data RobertaDataBunch; Train: LabelList (4000 items) x: RobertaTextList <s> One \u0120of \u0120the \u0120other \u0120reviewers \u0120has \u0120mentioned \u0120that \u0120after \u0120watching \u0120just \u01201 \u0120Oz \u0120episode \u0120you 'll \u0120be \u0120hooked . \u0120They \u0120are \u0120right , \u0120as \u0120this \u0120is \u0120exactly \u0120what \u0120happened \u0120with \u0120me .< br \u0120/ >< br \u0120/> The \u0120first \u0120thing \u0120that \u0120struck \u0120me \u0120about \u0120Oz \u0120was \u0120its \u0120brutality \u0120and \u0120unfl inch ing \u0120scenes \u0120of \u0120violence , \u0120which \u0120set \u0120in \u0120right \u0120from \u0120the \u0120word \u0120GO . \u0120Trust \u0120me , \u0120this \u0120is \u0120not \u0120a \u0120show \u0120for \u0120the \u0120faint \u0120heart ed \u0120or \u0120timid . \u0120This \u0120show \u0120pulls \u0120no \u0120punches \u0120with \u0120regards \u0120to \u0120drugs , \u0120sex \u0120or \u0120violence . \u0120Its \u0120is \u0120hardcore , \u0120in \u0120the \u0120classic \u0120use \u0120of \u0120the \u0120word .< br \u0120/ >< br \u0120/> It \u0120is \u0120called \u0120O Z \u0120as \u0120that \u0120is \u0120the \u0120nickname \u0120given \u0120to \u0120the \u0120Oswald \u0120Maximum \u0120Security \u0120State \u0120Pen itent ary . \u0120It \u0120focuses \u0120mainly \u0120on \u0120Emerald \u0120City , \u0120an \u0120experimental \u0120section \u0120of \u0120the \u0120prison \u0120where \u0120all \u0120the \u0120cells \u0120have \u0120glass \u0120fronts \u0120and \u0120face \u0120in wards , \u0120so \u0120privacy \u0120is \u0120not \u0120high \u0120on \u0120the \u0120agenda . \u0120Em \u0120City \u0120is \u0120home \u0120to \u0120many .. A ry ans , \u0120Muslims , \u0120gang st as , \u0120Latinos , \u0120Christians , \u0120Italians , \u0120Irish \u0120and \u0120more .... so \u0120sc uff les , \u0120death \u0120stares , \u0120dod gy \u0120dealings \u0120and \u0120shady \u0120agreements \u0120are \u0120never \u0120far \u0120away .< br \u0120/ >< br \u0120/> I \u0120would \u0120say \u0120the \u0120main \u0120appeal \u0120of \u0120the \u0120show \u0120is \u0120due \u0120to \u0120the \u0120fact \u0120that \u0120it \u0120goes \u0120where \u0120other \u0120shows \u0120wouldn 't \u0120dare . \u0120Forget \u0120pretty \u0120pictures \u0120painted \u0120for \u0120mainstream \u0120audiences , \u0120forget \u0120charm , \u0120forget </s>,<s> A \u0120wonderful \u0120little \u0120production . \u0120< br \u0120/ >< br \u0120/> The \u0120filming \u0120technique \u0120is \u0120very \u0120un assuming - \u0120very \u0120old - time - BBC \u0120fashion \u0120and \u0120gives \u0120a \u0120comforting , \u0120and \u0120sometimes \u0120discomfort ing , \u0120sense \u0120of \u0120realism \u0120to \u0120the \u0120entire \u0120piece . \u0120< br \u0120/ >< br \u0120/> The \u0120actors \u0120are \u0120extremely \u0120well \u0120chosen - \u0120Michael \u0120Sheen \u0120not \u0120only \u0120\" has \u0120got \u0120all \u0120the \u0120pol ari \" \u0120but \u0120he \u0120has \u0120all \u0120the \u0120voices \u0120down \u0120pat \u0120too ! \u0120You \u0120can \u0120truly \u0120see \u0120the \u0120seamless \u0120editing \u0120guided \u0120by \u0120the \u0120references \u0120to \u0120Williams ' \u0120diary \u0120entries , \u0120not \u0120only \u0120is \u0120it \u0120well \u0120worth \u0120the \u0120watching \u0120but \u0120it \u0120is \u0120a \u0120terrific ly \u0120written \u0120and \u0120performed \u0120piece . \u0120A \u0120master ful \u0120production \u0120about \u0120one \u0120of \u0120the \u0120great \u0120master 's \u0120of \u0120comedy \u0120and \u0120his \u0120life . \u0120< br \u0120/ >< br \u0120/> The \u0120realism \u0120really \u0120comes \u0120home \u0120with \u0120the \u0120little \u0120things : \u0120the \u0120fantasy \u0120of \u0120the \u0120guard \u0120which , \u0120rather \u0120than \u0120use \u0120the \u0120traditional \u0120' dream ' \u0120techniques \u0120remains \u0120solid \u0120then \u0120disappears . \u0120It \u0120plays \u0120on \u0120our \u0120knowledge \u0120and \u0120our \u0120senses , \u0120particularly \u0120with \u0120the \u0120scenes \u0120concerning \u0120Or ton \u0120and \u0120Hall i well \u0120and \u0120the \u0120sets \u0120( particularly \u0120of \u0120their \u0120flat \u0120with \u0120Hall i well 's \u0120mur als \u0120decor ating \u0120every \u0120surface ) \u0120are \u0120terribly \u0120well \u0120done . </s>,<s> Basically \u0120there 's \u0120a \u0120family \u0120where \u0120a \u0120little \u0120boy \u0120( Jake ) \u0120thinks \u0120there 's \u0120a \u0120zombie \u0120in \u0120his \u0120closet \u0120& \u0120his \u0120parents \u0120are \u0120fighting \u0120all \u0120the \u0120time .< br \u0120/ >< br \u0120/> This \u0120movie \u0120is \u0120slower \u0120than \u0120a \u0120soap \u0120opera ... \u0120and \u0120suddenly , \u0120Jake \u0120decides \u0120to \u0120become \u0120Ram bo \u0120and \u0120kill \u0120the \u0120zombie .< br \u0120/ >< br \u0120/> OK , \u0120first \u0120of \u0120all \u0120when \u0120you 're \u0120going \u0120to \u0120make \u0120a \u0120film \u0120you \u0120must \u0120Dec ide \u0120if \u0120its \u0120a \u0120thriller \u0120or \u0120a \u0120drama ! \u0120As \u0120a \u0120drama \u0120the \u0120movie \u0120is \u0120watch able . \u0120Parents \u0120are \u0120divor cing \u0120& \u0120arguing \u0120like \u0120in \u0120real \u0120life . \u0120And \u0120then \u0120we \u0120have \u0120Jake \u0120with \u0120his \u0120closet \u0120which \u0120totally \u0120ruins \u0120all \u0120the \u0120film ! \u0120I \u0120expected \u0120to \u0120see \u0120a \u0120B OO GE Y MAN \u0120similar \u0120movie , \u0120and \u0120instead \u0120i \u0120watched \u0120a \u0120drama \u0120with \u0120some \u0120meaningless \u0120thriller \u0120spots .< br \u0120/ >< br \u0120/> 3 \u0120out \u0120of \u012010 \u0120just \u0120for \u0120the \u0120well \u0120playing \u0120parents \u0120& \u0120descent \u0120dialog s . \u0120As \u0120for \u0120the \u0120shots \u0120with \u0120Jake : \u0120just \u0120ignore \u0120them . </s>,<s> Pet ter \u0120Matte i 's \u0120\" Love \u0120in \u0120the \u0120Time \u0120of \u0120Money \" \u0120is \u0120a \u0120visually \u0120stunning \u0120film \u0120to \u0120watch . \u0120Mr . \u0120Matte i \u0120offers \u0120us \u0120a \u0120vivid \u0120portrait \u0120about \u0120human \u0120relations . \u0120This \u0120is \u0120a \u0120movie \u0120that \u0120seems \u0120to \u0120be \u0120telling \u0120us \u0120what \u0120money , \u0120power \u0120and \u0120success \u0120do \u0120to \u0120people \u0120in \u0120the \u0120different \u0120situations \u0120we \u0120encounter . \u0120< br \u0120/ >< br \u0120/> This \u0120being \u0120a \u0120variation \u0120on \u0120the \u0120Arthur \u0120Schn itz ler 's \u0120play \u0120about \u0120the \u0120same \u0120theme , \u0120the \u0120director \u0120transfers \u0120the \u0120action \u0120to \u0120the \u0120present \u0120time \u0120New \u0120York \u0120where \u0120all \u0120these \u0120different \u0120characters \u0120meet \u0120and \u0120connect . \u0120Each \u0120one \u0120is \u0120connected \u0120in \u0120one \u0120way , \u0120or \u0120another \u0120to \u0120the \u0120next \u0120person , \u0120but \u0120no \u0120one \u0120seems \u0120to \u0120know \u0120the \u0120previous \u0120point \u0120of \u0120contact . \u0120Sty lish ly , \u0120the \u0120film \u0120has \u0120a \u0120sophisticated \u0120luxurious \u0120look . \u0120We \u0120are \u0120taken \u0120to \u0120see \u0120how \u0120these \u0120people \u0120live \u0120and \u0120the \u0120world \u0120they \u0120live \u0120in \u0120their \u0120own \u0120habitat .< br \u0120/ >< br \u0120/> The \u0120only \u0120thing \u0120one \u0120gets \u0120out \u0120of \u0120all \u0120these \u0120souls \u0120in \u0120the \u0120picture \u0120is \u0120the \u0120different \u0120stages \u0120of \u0120loneliness \u0120each \u0120one \u0120inhab its . \u0120A \u0120big \u0120city \u0120is \u0120not \u0120exactly \u0120the \u0120best \u0120place \u0120in \u0120which \u0120human \u0120relations \u0120find \u0120sincere \u0120fulfillment , \u0120as \u0120one \u0120discern s \u0120is \u0120the \u0120case \u0120with \u0120most \u0120of \u0120the \u0120people \u0120we \u0120encounter .< br \u0120/ >< br \u0120/> The \u0120acting \u0120is \u0120good \u0120under \u0120Mr . \u0120Matte i 's \u0120direction . \u0120Steve \u0120Bus ce mi , \u0120Ros ario \u0120Dawson , \u0120Carol \u0120Kane , \u0120Michael \u0120Imper iol </s>,<s> Probably \u0120my \u0120all - time \u0120favorite \u0120movie , \u0120a \u0120story \u0120of \u0120self lessness , \u0120sacrifice \u0120and \u0120dedication \u0120to \u0120a \u0120noble \u0120cause , \u0120but \u0120it 's \u0120not \u0120preach y \u0120or \u0120boring . \u0120It \u0120just \u0120never \u0120gets \u0120old , \u0120despite \u0120my \u0120having \u0120seen \u0120it \u0120some \u012015 \u0120or \u0120more \u0120times \u0120in \u0120the \u0120last \u012025 \u0120years . \u0120Paul \u0120Luk as ' \u0120performance \u0120brings \u0120tears \u0120to \u0120my \u0120eyes , \u0120and \u0120Bet te \u0120Davis , \u0120in \u0120one \u0120of \u0120her \u0120very \u0120few \u0120truly \u0120sympathetic \u0120roles , \u0120is \u0120a \u0120delight . \u0120The \u0120kids \u0120are , \u0120as \u0120grandma \u0120says , \u0120more \u0120like \u0120\" d ressed - up \u0120mid gets \" \u0120than \u0120children , \u0120but \u0120that \u0120only \u0120makes \u0120them \u0120more \u0120fun \u0120to \u0120watch . \u0120And \u0120the \u0120mother 's \u0120slow \u0120awakening \u0120to \u0120what 's \u0120happening \u0120in \u0120the \u0120world \u0120and \u0120under \u0120her \u0120own \u0120roof \u0120is \u0120believable \u0120and \u0120startling . \u0120If \u0120I \u0120had \u0120a \u0120dozen \u0120thumbs , \u0120they 'd \u0120all \u0120be \u0120\" up \" \u0120for \u0120this \u0120movie . </s> y: CategoryList positive,positive,negative,positive,positive Path: .; Valid: LabelList (1000 items) x: RobertaTextList <s> Apparently , \u0120The \u0120Mut ilation \u0120Man \u0120is \u0120about \u0120a \u0120guy \u0120who \u0120wand ers \u0120the \u0120land \u0120performing \u0120shows \u0120of \u0120self - mut ilation \u0120as \u0120a \u0120way \u0120of \u0120coping \u0120with \u0120his \u0120abusive \u0120childhood . \u0120I \u0120use \u0120the \u0120word \u0120' app arently ' \u0120because \u0120without \u0120listening \u0120to \u0120a \u0120director \u0120Andy \u0120Co pp 's \u0120commentary \u0120( which \u0120I \u0120didn 't \u0120have \u0120available \u0120to \u0120me ) \u0120or \u0120reading \u0120up \u0120on \u0120the \u0120film \u0120prior \u0120to \u0120watching , \u0120viewers \u0120won 't \u0120have \u0120a \u0120clue \u0120what \u0120it \u0120is \u0120about .< br \u0120/ >< br \u0120/> G ore h ounds \u0120and \u0120fans \u0120of \u0120extreme \u0120movies \u0120may \u0120be \u0120lured \u0120into \u0120watching \u0120The \u0120Mut ilation \u0120Man \u0120with \u0120the \u0120promise \u0120of \u0120some \u0120harsh \u0120scenes \u0120of \u0120spl atter \u0120and \u0120unsettling \u0120real - life \u0120footage , \u0120but \u0120unless \u0120they 're \u0120also \u0120fond \u0120of \u0120pret entious , \u0120headache - inducing , \u0120experimental \u0120art - house \u0120cinema , \u0120they 'll \u0120find \u0120this \u0120one \u0120a \u0120real \u0120chore \u0120to \u0120sit \u0120through .< br \u0120/ >< br \u0120/> 82 \u0120minutes \u0120of \u0120ugly \u0120imagery \u0120accompanied \u0120by \u0120dis - ch ord ant \u0120sound , \u0120terrible \u0120music \u0120and \u0120incomprehensible \u0120dialogue , \u0120this \u0120mind - n umb ingly \u0120awful \u0120dri vel \u0120is \u0120the \u0120perfect \u0120way \u0120to \u0120test \u0120one 's \u0120sanity : \u0120if \u0120you 've \u0120still \u0120got \u0120all \u0120your \u0120mar bles , \u0120you 'll \u0120switch \u0120this \u0120rubbish \u0120off \u0120and \u0120watch \u0120something \u0120decent \u0120instead \u0120( I \u0120watched \u0120the \u0120whole \u0120thing , \u0120but \u0120am \u0120well \u0120aware \u0120that \u0120I 'm \u0120completely \u0120barking !). </s>,<s> Peter \u0120C ushing \u0120and \u0120Donald \u0120Ple as ance \u0120are \u0120legendary \u0120actors , \u0120and \u0120director \u0120K ost as \u0120Kar ag ian nis \u0120was \u0120the \u0120man \u0120behind \u0120the \u0120successful \u0120Greek \u0120Gi allo - es quire \u0120thriller \u0120Death \u0120Kiss \u0120in \u01201974 ; \u0120and \u0120yet \u0120when \u0120you \u0120combine \u0120the \u0120three \u0120talents , \u0120all \u0120you \u0120get \u0120is \u0120this \u0120complete \u0120load \u0120of \u0120dri vel ! \u0120God \u0120only \u0120knows \u0120what \u0120drove \u0120the \u0120likes \u0120of \u0120Peter \u0120C ushing \u0120and \u0120Donald \u0120Ple as ance \u0120to \u0120star \u0120in \u0120this \u0120cheap ie \u0120devil \u0120worship \u0120flick , \u0120but \u0120I \u0120really \u0120do \u0120hope \u0120they \u0120were \u0120well \u0120paid \u0120as \u0120neither \u0120one \u0120deserves \u0120something \u0120as \u0120amateur ish \u0120as \u0120this \u0120on \u0120their \u0120resumes . \u0120The \u0120story \u0120focuses \u0120on \u0120a \u0120group \u0120of \u0120devil \u0120worsh ippers \u0120that \u0120kidnap \u0120some \u0120kids , \u0120leading \u0120another \u0120group \u0120to \u0120go \u0120after \u0120them . \u0120The \u0120pace \u0120of \u0120the \u0120plot \u0120is \u0120very \u0120slow \u0120and \u0120this \u0120ensures \u0120that \u0120the \u0120film \u0120is \u0120very \u0120boring . \u0120The \u0120plot \u0120is \u0120also \u0120a \u0120long \u0120way \u0120from \u0120being \u0120original \u0120and \u0120anyone \u0120with \u0120even \u0120a \u0120passing \u0120interest \u0120in \u0120the \u0120horror \u0120genre \u0120will \u0120have \u0120seen \u0120something \u0120a \u0120bit \u0120like \u0120this , \u0120and \u0120no \u0120doubt \u0120done \u0120much \u0120better . \u0120The \u0120obvious \u0120lack \u0120of \u0120budget \u0120is \u0120felt \u0120throughout \u0120and \u0120the \u0120film \u0120doesn 't \u0120manage \u0120to \u0120overcome \u0120this \u0120at \u0120any \u0120point . \u0120This \u0120really \u0120is \u0120a \u0120depressing \u0120and \u0120miserable \u0120watch \u0120and \u0120not \u0120even \u0120a \u0120slightly \u0120decent \u0120ending \u0120manages \u0120to \u0120up \u0120the \u0120ante \u0120enough \u0120to \u0120lift \u0120this \u0120film \u0120out \u0120of \u0120the \u0120very \u0120bottom \u0120of \u0120the \u0120barrel . \u0120Extreme ly \u0120poor \u0120stuff \u0120and \u0120definitely \u0120not \u0120recommended ! </s>,<s> Back \u0120in \u0120the \u01201970 s , \u0120WP IX \u0120ran \u0120\" The \u0120Adventures \u0120of \u0120Superman \" \u0120every \u0120weekday \u0120afternoon \u0120for \u0120quite \u0120a \u0120few \u0120years . \u0120Every \u0120once \u0120in \u0120a \u0120while , \u0120we 'd \u0120get \u0120a \u0120treat \u0120when \u0120they \u0120would \u0120preempt \u0120neighboring \u0120shows \u0120to \u0120air \u0120\" Super man \u0120and \u0120the \u0120Mole \u0120Men .\" \u0120I \u0120always \u0120looked \u0120forward \u0120to \u0120those \u0120days . \u0120Watching \u0120it \u0120recently , \u0120I \u0120was \u0120surprised \u0120at \u0120just \u0120how \u0120bad \u0120it \u0120really \u0120was .< br \u0120/ >< br \u0120/> It \u0120wasn 't \u0120bad \u0120because \u0120of \u0120the \u0120special \u0120effects , \u0120or \u0120lack \u0120thereof . \u0120True , \u0120George \u0120Reeves ' \u0120Superman \u0120costume \u0120was \u0120pretty \u0120bad , \u0120the \u0120edges \u0120of \u0120the \u0120foam \u0120padding \u0120used \u0120to \u0120make \u0120him \u0120look \u0120more \u0120imposing \u0120being \u0120plainly \u0120visible . \u0120And \u0120true , \u0120the \u0120Mole \u0120Men 's \u0120costumes \u0120were \u0120even \u0120worse . \u0120What \u0120was \u0120supposed \u0120to \u0120be \u0120a \u0120furry \u0120covering \u0120wouldn 't \u0120have \u0120fooled \u0120a \u0120ten \u0120year - old , \u0120since \u0120the \u0120z ippers , \u0120sleeve \u0120he ms \u0120and \u0120badly \u0120p illing \u0120fabric \u0120badly \u0120tailored \u0120into \u0120bag gy \u0120costumes \u0120were \u0120all \u0120painfully \u0120obvious . \u0120But \u0120these \u0120were \u0120forg ivable \u0120shortcomings .< br \u0120/ >< br \u0120/> No , \u0120what \u0120made \u0120it \u0120bad \u0120were \u0120the \u0120cont rived \u0120plot \u0120devices . \u0120Time \u0120and \u0120again , \u0120Superman \u0120failed \u0120to \u0120do \u0120anything \u0120to \u0120keep \u0120the \u0120situation \u0120from \u0120deteriorating . \u0120A \u0120lyn ch \u0120mob \u0120is \u0120searching \u0120for \u0120the \u0120creatures ? \u0120Rather \u0120than \u0120round \u0120up \u0120the \u0120hysterical \u0120crowd \u0120or \u0120search \u0120for \u0120the \u0120creatures \u0120himself , \u0120he \u0120stands \u0120around \u0120explaining \u0120the \u0120dangers \u0120of \u0120the \u0120situation \u0120to \u0120Lois \u0120and \u0120the \u0120PR </s>,<s> Sat an 's \u0120Little \u0120Hel per \u0120is \u0120one \u0120of \u0120the \u0120better \u0120B \u0120Horror \u0120movies \u0120I \u0120have \u0120seen . \u0120When \u0120I \u0120say \u0120better \u0120I \u0120mean \u0120the \u0120story . \u0120The \u0120film \u0120hat ches \u0120a \u0120new \u0120plot , \u0120something \u0120that 's \u0120not \u0120so \u0120clich\u00c3\u00a9 \u0120in \u0120the \u0120Horror \u0120genre \u0120- \u0120something \u0120fresh . \u0120But \u0120there \u0120are \u0120also \u0120some \u0120ridiculous \u0120questions \u0120that \u0120come \u0120along \u0120with \u0120it . \u0120Questions \u0120you \u0120will \u0120be \u0120asking \u0120yourself \u0120throughout \u0120the \u0120movie .< br \u0120/ >< br \u0120/> The \u0120film \u0120first \u0120caught \u0120my \u0120attention \u0120while \u0120I \u0120was \u0120cruising \u0120the \u0120Horror \u0120section \u0120in \u0120HM V . \u0120I \u0120was \u0120tired \u0120of \u0120all \u0120the \u0120so \u0120called \u0120\" ter r ifi ying \" \u0120Hollywood \u0120block busters \u0120and \u0120wanted \u0120something \u0120different . \u0120The \u0120cover \u0120art \u0120for \u0120Satan 's \u0120Little \u0120Hel per \u0120immediately \u0120caught \u0120my \u0120attention . \u0120As \u0120you \u0120can \u0120see , \u0120the \u0120image \u0120draws \u0120you \u0120in \u0120- \u0120it 's \u0120chilling ! \u0120I \u0120knew \u0120it \u0120was \u0120a \u0120straight \u0120to \u0120DVD \u0120release \u0120- \u0120but \u0120I \u0120took \u0120a \u0120chance . \u0120I \u0120mean , \u0120I \u0120just \u0120seen \u0120\" Boo gey \u0120Man \" \u0120the \u0120night \u0120before \u0120- \u0120so \u0120It \u0120couldn 't \u0120get \u0120any \u0120worse ! \u0120After \u0120I \u0120watched \u0120the \u0120movie , \u0120I \u0120was \u0120semi - s atisf ied . \u0120I \u0120loved \u0120the \u0120plot \u0120of \u0120the \u0120movie . \u0120It \u0120was \u0120really \u0120creepy \u0120how \u0120the \u0120killer \u0120was \u0120pretending \u0120to \u0120be \u0120the \u0120little \u0120boys \u0120friend , \u0120so \u0120he \u0120could \u0120kill . \u0120In \u0120some \u0120sick \u0120der anged \u0120way , \u0120he \u0120actually \u0120thought \u0120he \u0120and \u0120the \u0120little \u0120boy \u0120would \u0120become \u0120partners \u0120- \u0120a \u0120duo \u0120of \u0120terror . \u0120It \u0120was \u0120a </s>,<s> I \u0120saw \u0120this \u0120gem \u0120of \u0120a \u0120film \u0120at \u0120Cannes \u0120where \u0120it \u0120was \u0120part \u0120of \u0120the \u0120directors \u0120fortnight .< br \u0120/ >< br \u0120/> Welcome \u0120to \u0120Coll in wood \u0120is \u0120nothing \u0120short \u0120of \u0120superb . \u0120Great \u0120fun \u0120throughout , \u0120with \u0120all \u0120members \u0120of \u0120a \u0120strong \u0120cast \u0120acting \u0120their \u0120socks \u0120off . \u0120It 's \u0120a \u0120sometimes \u0120laugh \u0120out \u0120loud \u0120comedy \u0120about \u0120a \u0120petty \u0120cro ok \u0120( Cos imo , \u0120played \u0120by \u0120Luis \u0120Gu zman ) \u0120who \u0120gets \u0120caught \u0120trying \u0120to \u0120steal \u0120a \u0120car \u0120and \u0120sent \u0120to \u0120prison . \u0120While \u0120in \u0120prison \u0120he \u0120meets \u0120a \u0120` l ifer ' \u0120who \u0120tells \u0120him \u0120of \u0120` the \u0120ultimate \u0120bell ini ' \u0120\u00c2 \u0138 \u0120which \u0120to \u0120you \u0120and \u0120me \u0120\u00c2 \u0138 \u0120is \u0120a \u0120sure - fire \u0120get \u0120rich \u0120quick \u0120scheme . \u0120It \u0120turns \u0120out \u0120that \u0120there \u0120is \u0120a \u0120way \u0120through \u0120from \u0120a \u0120deserted \u0120building \u0120into \u0120the \u0120towns \u0120jew ell ers \u0120shop \u0120\u00c2 \u0138 \u0120which \u0120could \u0120net \u0120millions . \u0120Sounds \u0120simple ? \u0120\u00c2 \u0138 \u0120well \u0120throw \u0120in \u0120all \u0120kinds \u0120of \u0120w acky \u0120characters \u0120and \u0120incidents \u0120along \u0120the \u0120way \u0120and \u0120you \u0120have \u0120got \u0120the \u0120ingredients \u0120for \u0120a \u0120one \u0120wild \u0120ride !! \u0120\u00c2 \u0138 \u0120word \u0120passes \u0120from \u0120one \u0120low \u0120life \u0120loser \u0120to \u0120the \u0120next \u0120and \u0120soon \u0120a \u0120team \u0120of \u0120them \u0120are \u0120assembled \u0120to \u0120try \u0120and \u0120cash \u0120in \u0120on \u0120Cos im os \u0120` bell ini ' \u0120lead \u0120by \u0120failed \u0120boxer \u0120Per o \u0120( Super bly \u0120played \u0120by \u0120Sam \u0120Rock well \u0120\u00c2 \u0138 \u0120surely \u0120a \u0120star \u0120in \u0120the \u0120making ) \u0120and \u0120reluctant \u0120cro ok \u0120Riley \u0120( William \u0120H . \u0120Macy ) \u0120who \u0120is \u0120forced \u0120to </s> y: CategoryList negative,negative,negative,negative,positive Path: .; Test: None Building the Model import torch import torch.nn as nn from transformers import RobertaModel # defining our model architecture class CustomRobertaModel(nn.Module): def __init__(self,num_labels=2): super(CustomRobertaModel,self).__init__() self.num_labels = num_labels self.roberta = RobertaModel.from_pretrained(config.roberta_model_name) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None): _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # logits = self.classifier(pooled_output) return logits roberta_model = CustomRobertaModel(num_labels=config.num_labels) learn = Learner(data, roberta_model, metrics=[accuracy]) learn.model.roberta.train() # setting roberta to train as it is in eval mode by default learn.fit_one_cycle(config.epochs, max_lr=config.max_lr) epoch train_loss valid_loss accuracy time 0 0.184614 0.180552 0.929000 02:17 Getting Predictions def get_preds_as_nparray(ds_type) -> np.ndarray: learn.model.roberta.eval() preds = learn.get_preds(ds_type)[0].detach().cpu().numpy() sampler = [i for i in data.dl(ds_type).sampler] reverse_sampler = np.argsort(sampler) ordered_preds = preds[reverse_sampler, :] pred_values = np.argmax(ordered_preds, axis=1) return ordered_preds, pred_values preds, pred_values = get_preds_as_nparray(DatasetType.Valid) # accuracy on valid (pred_values == data.valid_ds.y.items).mean() 0.929 Saving/Loading the model weights def save_model(learner, file_name): st = learner.model.state_dict() torch.save(st, file_name) # will save model in current dir # backend is pickle def load_model(learner, file_name): st = torch.load(file_name) learner.model.load_state_dict(st) # monkey patching Learner methods to save and load model file Learner.save_model = save_model Learner.load_model = load_model learn.save_model(\"my_model.bin\") learn.load_model(\"my_model.bin\")","title":"Package Installation"},{"location":"Using_Transformers_with_Fastai_Tutorial/#package-installation","text":"!git clone https://github.com/devkosal/fastai_roberta.git Cloning into 'fastai_roberta'... remote: Enumerating objects: 171, done.\u001b[K remote: Counting objects: 100% (171/171), done.\u001b[K remote: Compressing objects: 100% (121/121), done.\u001b[K remote: Total 171 (delta 91), reused 111 (delta 44), pack-reused 0\u001b[K Receiving objects: 100% (171/171), 25.46 MiB | 18.58 MiB/s, done. Resolving deltas: 100% (91/91), done. !pip install fastai==1.0.60 transformers==2.3.0 Collecting fastai==1.0.60 \u001b[?25l Downloading https://files.pythonhosted.org/packages/f5/e4/a7025bf28f303dbda0f862c09a7f957476fa92c9271643b4061a81bb595f/fastai-1.0.60-py3-none-any.whl (237kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 245kB 4.7MB/s \u001b[?25hCollecting transformers==2.3.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 7.4MB/s \u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (3.2.2) Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.3.2) Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.18.5) Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (7.0.0) Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (7.352.0) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (0.7) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.23.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (3.13) Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.7.1) Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.0.0) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (20.4) Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (0.7.0+cu101) Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (4.6.3) Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.2.4) Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.6.0+cu101) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.4.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.0.5) Collecting sentencepiece \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 7.0MB/s \u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.14.48) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.41.1) Collecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 26.1MB/s \u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.20) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (2.4.7) Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (2.8.1) Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai==1.0.60) (1.2.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (1.24.3) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (2020.6.20) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (3.0.4) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai==1.0.60) (2.10) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai==1.0.60) (1.15.0) Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.0.2) Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (0.7.1) Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (0.4.1) Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.1.3) Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (3.0.2) Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.0.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (49.6.0) Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (2.0.3) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (7.4.0) Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai==1.0.60) (1.0.2) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai==1.0.60) (0.16.0) Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai==1.0.60) (2018.9) Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.10.0) Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.3.3) Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (1.17.48) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (0.16.0) Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai==1.0.60) (1.7.0) Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->transformers==2.3.0) (0.15.2) Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai==1.0.60) (3.1.0) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=aef96c81bb474827b2b34caa9690e68458a664de90731bcc580754ce52b74dd7 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: fastai, sentencepiece, sacremoses, transformers Found existing installation: fastai 1.0.61 Uninstalling fastai-1.0.61: Successfully uninstalled fastai-1.0.61 Successfully installed fastai-1.0.60 sacremoses-0.0.43 sentencepiece-0.1.91 transformers-2.3.0","title":"Package Installation"},{"location":"Using_Transformers_with_Fastai_Tutorial/#load-and-set-configuration","text":"from fastai.text import * from fastai.metrics import * from transformers import RobertaTokenizer # Creating a config object to store task specific information class Config(dict): def __init__(self, **kwargs): super().__init__(**kwargs) for k, v in kwargs.items(): setattr(self, k, v) def set(self, key, val): self[key] = val setattr(self, key, val) config = Config( testing=True, seed = 2019, roberta_model_name='roberta-base', # can also be exchnaged with roberta-large max_lr=1e-5, epochs=1, use_fp16=False, bs=4, max_seq_len=256, num_labels = 2, hidden_dropout_prob=.05, hidden_size=768, # 1024 for roberta-large start_tok = \"<s>\", end_tok = \"</s>\", ) df = pd.read_csv(\"fastai_roberta/fastai_roberta_imdb/imdb_dataset.csv\") if config.testing: df = df[:5000] print(df.shape) (5000, 2) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } review sentiment 0 One of the other reviewers has mentioned that ... positive 1 A wonderful little production. <br /><br />The... positive 2 I thought this was a wonderful way to spend ti... positive 3 Basically there's a family where a little boy ... negative 4 Petter Mattei's \"Love in the Time of Money\" is... positive feat_cols = \"review\" label_cols = \"sentiment\"","title":"Load And Set Configuration"},{"location":"Using_Transformers_with_Fastai_Tutorial/#setting-up-the-tokenizer","text":"class FastAiRobertaTokenizer(BaseTokenizer): \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\" def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): self._pretrained_tokenizer = tokenizer self.max_seq_len = max_seq_len def __call__(self, *args, **kwargs): return self def tokenizer(self, t:str) -> List[str]: \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok] # create fastai tokenizer for roberta roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\") fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[]) # create fastai vocabulary for roberta path = Path() roberta_tok.save_vocabulary(path) with open('vocab.json', 'r') as f: roberta_vocab_dict = json.load(f) fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys())) # Setting up pre-processors class RobertaTokenizeProcessor(TokenizeProcessor): def __init__(self, tokenizer): super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False) class RobertaNumericalizeProcessor(NumericalizeProcessor): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None): \"\"\" Constructing preprocessors for Roberta We remove sos and eos tokens since we add that ourselves in the tokenizer. We also use a custom vocabulary to match the numericalization with the original Roberta model. \"\"\" return [RobertaTokenizeProcessor(tokenizer=tokenizer), RobertaNumericalizeProcessor(vocab=vocab)]","title":"Setting Up the Tokenizer"},{"location":"Using_Transformers_with_Fastai_Tutorial/#setting-up-the-databunch","text":"# Creating a Roberta specific DataBunch class class RobertaDataBunch(TextDataBunch): \"Create a `TextDataBunch` suitable for training Roberta\" @classmethod def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1, pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch: \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\" datasets = cls._init_ds(train_ds, valid_ds, test_ds) val_bs = ifnone(val_bs, bs) collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards) train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs) train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs) dataloaders = [train_dl] for ds in datasets[1:]: lengths = [len(t) for t in ds.x.items] sampler = SortSampler(ds.x, key=lengths.__getitem__) dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs)) return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check) class RobertaTextList(TextList): _bunch = RobertaDataBunch _label_cls = TextList # loading the tokenizer and vocab processors processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab) # creating our databunch data = RobertaTextList.from_df(df, \".\", cols=feat_cols, processor=processor) \\ .split_by_rand_pct(seed=config.seed) \\ .label_from_df(cols=label_cols,label_cls=CategoryList) \\ .databunch(bs=config.bs, pad_first=False, pad_idx=0) data RobertaDataBunch; Train: LabelList (4000 items) x: RobertaTextList <s> One \u0120of \u0120the \u0120other \u0120reviewers \u0120has \u0120mentioned \u0120that \u0120after \u0120watching \u0120just \u01201 \u0120Oz \u0120episode \u0120you 'll \u0120be \u0120hooked . \u0120They \u0120are \u0120right , \u0120as \u0120this \u0120is \u0120exactly \u0120what \u0120happened \u0120with \u0120me .< br \u0120/ >< br \u0120/> The \u0120first \u0120thing \u0120that \u0120struck \u0120me \u0120about \u0120Oz \u0120was \u0120its \u0120brutality \u0120and \u0120unfl inch ing \u0120scenes \u0120of \u0120violence , \u0120which \u0120set \u0120in \u0120right \u0120from \u0120the \u0120word \u0120GO . \u0120Trust \u0120me , \u0120this \u0120is \u0120not \u0120a \u0120show \u0120for \u0120the \u0120faint \u0120heart ed \u0120or \u0120timid . \u0120This \u0120show \u0120pulls \u0120no \u0120punches \u0120with \u0120regards \u0120to \u0120drugs , \u0120sex \u0120or \u0120violence . \u0120Its \u0120is \u0120hardcore , \u0120in \u0120the \u0120classic \u0120use \u0120of \u0120the \u0120word .< br \u0120/ >< br \u0120/> It \u0120is \u0120called \u0120O Z \u0120as \u0120that \u0120is \u0120the \u0120nickname \u0120given \u0120to \u0120the \u0120Oswald \u0120Maximum \u0120Security \u0120State \u0120Pen itent ary . \u0120It \u0120focuses \u0120mainly \u0120on \u0120Emerald \u0120City , \u0120an \u0120experimental \u0120section \u0120of \u0120the \u0120prison \u0120where \u0120all \u0120the \u0120cells \u0120have \u0120glass \u0120fronts \u0120and \u0120face \u0120in wards , \u0120so \u0120privacy \u0120is \u0120not \u0120high \u0120on \u0120the \u0120agenda . \u0120Em \u0120City \u0120is \u0120home \u0120to \u0120many .. A ry ans , \u0120Muslims , \u0120gang st as , \u0120Latinos , \u0120Christians , \u0120Italians , \u0120Irish \u0120and \u0120more .... so \u0120sc uff les , \u0120death \u0120stares , \u0120dod gy \u0120dealings \u0120and \u0120shady \u0120agreements \u0120are \u0120never \u0120far \u0120away .< br \u0120/ >< br \u0120/> I \u0120would \u0120say \u0120the \u0120main \u0120appeal \u0120of \u0120the \u0120show \u0120is \u0120due \u0120to \u0120the \u0120fact \u0120that \u0120it \u0120goes \u0120where \u0120other \u0120shows \u0120wouldn 't \u0120dare . \u0120Forget \u0120pretty \u0120pictures \u0120painted \u0120for \u0120mainstream \u0120audiences , \u0120forget \u0120charm , \u0120forget </s>,<s> A \u0120wonderful \u0120little \u0120production . \u0120< br \u0120/ >< br \u0120/> The \u0120filming \u0120technique \u0120is \u0120very \u0120un assuming - \u0120very \u0120old - time - BBC \u0120fashion \u0120and \u0120gives \u0120a \u0120comforting , \u0120and \u0120sometimes \u0120discomfort ing , \u0120sense \u0120of \u0120realism \u0120to \u0120the \u0120entire \u0120piece . \u0120< br \u0120/ >< br \u0120/> The \u0120actors \u0120are \u0120extremely \u0120well \u0120chosen - \u0120Michael \u0120Sheen \u0120not \u0120only \u0120\" has \u0120got \u0120all \u0120the \u0120pol ari \" \u0120but \u0120he \u0120has \u0120all \u0120the \u0120voices \u0120down \u0120pat \u0120too ! \u0120You \u0120can \u0120truly \u0120see \u0120the \u0120seamless \u0120editing \u0120guided \u0120by \u0120the \u0120references \u0120to \u0120Williams ' \u0120diary \u0120entries , \u0120not \u0120only \u0120is \u0120it \u0120well \u0120worth \u0120the \u0120watching \u0120but \u0120it \u0120is \u0120a \u0120terrific ly \u0120written \u0120and \u0120performed \u0120piece . \u0120A \u0120master ful \u0120production \u0120about \u0120one \u0120of \u0120the \u0120great \u0120master 's \u0120of \u0120comedy \u0120and \u0120his \u0120life . \u0120< br \u0120/ >< br \u0120/> The \u0120realism \u0120really \u0120comes \u0120home \u0120with \u0120the \u0120little \u0120things : \u0120the \u0120fantasy \u0120of \u0120the \u0120guard \u0120which , \u0120rather \u0120than \u0120use \u0120the \u0120traditional \u0120' dream ' \u0120techniques \u0120remains \u0120solid \u0120then \u0120disappears . \u0120It \u0120plays \u0120on \u0120our \u0120knowledge \u0120and \u0120our \u0120senses , \u0120particularly \u0120with \u0120the \u0120scenes \u0120concerning \u0120Or ton \u0120and \u0120Hall i well \u0120and \u0120the \u0120sets \u0120( particularly \u0120of \u0120their \u0120flat \u0120with \u0120Hall i well 's \u0120mur als \u0120decor ating \u0120every \u0120surface ) \u0120are \u0120terribly \u0120well \u0120done . </s>,<s> Basically \u0120there 's \u0120a \u0120family \u0120where \u0120a \u0120little \u0120boy \u0120( Jake ) \u0120thinks \u0120there 's \u0120a \u0120zombie \u0120in \u0120his \u0120closet \u0120& \u0120his \u0120parents \u0120are \u0120fighting \u0120all \u0120the \u0120time .< br \u0120/ >< br \u0120/> This \u0120movie \u0120is \u0120slower \u0120than \u0120a \u0120soap \u0120opera ... \u0120and \u0120suddenly , \u0120Jake \u0120decides \u0120to \u0120become \u0120Ram bo \u0120and \u0120kill \u0120the \u0120zombie .< br \u0120/ >< br \u0120/> OK , \u0120first \u0120of \u0120all \u0120when \u0120you 're \u0120going \u0120to \u0120make \u0120a \u0120film \u0120you \u0120must \u0120Dec ide \u0120if \u0120its \u0120a \u0120thriller \u0120or \u0120a \u0120drama ! \u0120As \u0120a \u0120drama \u0120the \u0120movie \u0120is \u0120watch able . \u0120Parents \u0120are \u0120divor cing \u0120& \u0120arguing \u0120like \u0120in \u0120real \u0120life . \u0120And \u0120then \u0120we \u0120have \u0120Jake \u0120with \u0120his \u0120closet \u0120which \u0120totally \u0120ruins \u0120all \u0120the \u0120film ! \u0120I \u0120expected \u0120to \u0120see \u0120a \u0120B OO GE Y MAN \u0120similar \u0120movie , \u0120and \u0120instead \u0120i \u0120watched \u0120a \u0120drama \u0120with \u0120some \u0120meaningless \u0120thriller \u0120spots .< br \u0120/ >< br \u0120/> 3 \u0120out \u0120of \u012010 \u0120just \u0120for \u0120the \u0120well \u0120playing \u0120parents \u0120& \u0120descent \u0120dialog s . \u0120As \u0120for \u0120the \u0120shots \u0120with \u0120Jake : \u0120just \u0120ignore \u0120them . </s>,<s> Pet ter \u0120Matte i 's \u0120\" Love \u0120in \u0120the \u0120Time \u0120of \u0120Money \" \u0120is \u0120a \u0120visually \u0120stunning \u0120film \u0120to \u0120watch . \u0120Mr . \u0120Matte i \u0120offers \u0120us \u0120a \u0120vivid \u0120portrait \u0120about \u0120human \u0120relations . \u0120This \u0120is \u0120a \u0120movie \u0120that \u0120seems \u0120to \u0120be \u0120telling \u0120us \u0120what \u0120money , \u0120power \u0120and \u0120success \u0120do \u0120to \u0120people \u0120in \u0120the \u0120different \u0120situations \u0120we \u0120encounter . \u0120< br \u0120/ >< br \u0120/> This \u0120being \u0120a \u0120variation \u0120on \u0120the \u0120Arthur \u0120Schn itz ler 's \u0120play \u0120about \u0120the \u0120same \u0120theme , \u0120the \u0120director \u0120transfers \u0120the \u0120action \u0120to \u0120the \u0120present \u0120time \u0120New \u0120York \u0120where \u0120all \u0120these \u0120different \u0120characters \u0120meet \u0120and \u0120connect . \u0120Each \u0120one \u0120is \u0120connected \u0120in \u0120one \u0120way , \u0120or \u0120another \u0120to \u0120the \u0120next \u0120person , \u0120but \u0120no \u0120one \u0120seems \u0120to \u0120know \u0120the \u0120previous \u0120point \u0120of \u0120contact . \u0120Sty lish ly , \u0120the \u0120film \u0120has \u0120a \u0120sophisticated \u0120luxurious \u0120look . \u0120We \u0120are \u0120taken \u0120to \u0120see \u0120how \u0120these \u0120people \u0120live \u0120and \u0120the \u0120world \u0120they \u0120live \u0120in \u0120their \u0120own \u0120habitat .< br \u0120/ >< br \u0120/> The \u0120only \u0120thing \u0120one \u0120gets \u0120out \u0120of \u0120all \u0120these \u0120souls \u0120in \u0120the \u0120picture \u0120is \u0120the \u0120different \u0120stages \u0120of \u0120loneliness \u0120each \u0120one \u0120inhab its . \u0120A \u0120big \u0120city \u0120is \u0120not \u0120exactly \u0120the \u0120best \u0120place \u0120in \u0120which \u0120human \u0120relations \u0120find \u0120sincere \u0120fulfillment , \u0120as \u0120one \u0120discern s \u0120is \u0120the \u0120case \u0120with \u0120most \u0120of \u0120the \u0120people \u0120we \u0120encounter .< br \u0120/ >< br \u0120/> The \u0120acting \u0120is \u0120good \u0120under \u0120Mr . \u0120Matte i 's \u0120direction . \u0120Steve \u0120Bus ce mi , \u0120Ros ario \u0120Dawson , \u0120Carol \u0120Kane , \u0120Michael \u0120Imper iol </s>,<s> Probably \u0120my \u0120all - time \u0120favorite \u0120movie , \u0120a \u0120story \u0120of \u0120self lessness , \u0120sacrifice \u0120and \u0120dedication \u0120to \u0120a \u0120noble \u0120cause , \u0120but \u0120it 's \u0120not \u0120preach y \u0120or \u0120boring . \u0120It \u0120just \u0120never \u0120gets \u0120old , \u0120despite \u0120my \u0120having \u0120seen \u0120it \u0120some \u012015 \u0120or \u0120more \u0120times \u0120in \u0120the \u0120last \u012025 \u0120years . \u0120Paul \u0120Luk as ' \u0120performance \u0120brings \u0120tears \u0120to \u0120my \u0120eyes , \u0120and \u0120Bet te \u0120Davis , \u0120in \u0120one \u0120of \u0120her \u0120very \u0120few \u0120truly \u0120sympathetic \u0120roles , \u0120is \u0120a \u0120delight . \u0120The \u0120kids \u0120are , \u0120as \u0120grandma \u0120says , \u0120more \u0120like \u0120\" d ressed - up \u0120mid gets \" \u0120than \u0120children , \u0120but \u0120that \u0120only \u0120makes \u0120them \u0120more \u0120fun \u0120to \u0120watch . \u0120And \u0120the \u0120mother 's \u0120slow \u0120awakening \u0120to \u0120what 's \u0120happening \u0120in \u0120the \u0120world \u0120and \u0120under \u0120her \u0120own \u0120roof \u0120is \u0120believable \u0120and \u0120startling . \u0120If \u0120I \u0120had \u0120a \u0120dozen \u0120thumbs , \u0120they 'd \u0120all \u0120be \u0120\" up \" \u0120for \u0120this \u0120movie . </s> y: CategoryList positive,positive,negative,positive,positive Path: .; Valid: LabelList (1000 items) x: RobertaTextList <s> Apparently , \u0120The \u0120Mut ilation \u0120Man \u0120is \u0120about \u0120a \u0120guy \u0120who \u0120wand ers \u0120the \u0120land \u0120performing \u0120shows \u0120of \u0120self - mut ilation \u0120as \u0120a \u0120way \u0120of \u0120coping \u0120with \u0120his \u0120abusive \u0120childhood . \u0120I \u0120use \u0120the \u0120word \u0120' app arently ' \u0120because \u0120without \u0120listening \u0120to \u0120a \u0120director \u0120Andy \u0120Co pp 's \u0120commentary \u0120( which \u0120I \u0120didn 't \u0120have \u0120available \u0120to \u0120me ) \u0120or \u0120reading \u0120up \u0120on \u0120the \u0120film \u0120prior \u0120to \u0120watching , \u0120viewers \u0120won 't \u0120have \u0120a \u0120clue \u0120what \u0120it \u0120is \u0120about .< br \u0120/ >< br \u0120/> G ore h ounds \u0120and \u0120fans \u0120of \u0120extreme \u0120movies \u0120may \u0120be \u0120lured \u0120into \u0120watching \u0120The \u0120Mut ilation \u0120Man \u0120with \u0120the \u0120promise \u0120of \u0120some \u0120harsh \u0120scenes \u0120of \u0120spl atter \u0120and \u0120unsettling \u0120real - life \u0120footage , \u0120but \u0120unless \u0120they 're \u0120also \u0120fond \u0120of \u0120pret entious , \u0120headache - inducing , \u0120experimental \u0120art - house \u0120cinema , \u0120they 'll \u0120find \u0120this \u0120one \u0120a \u0120real \u0120chore \u0120to \u0120sit \u0120through .< br \u0120/ >< br \u0120/> 82 \u0120minutes \u0120of \u0120ugly \u0120imagery \u0120accompanied \u0120by \u0120dis - ch ord ant \u0120sound , \u0120terrible \u0120music \u0120and \u0120incomprehensible \u0120dialogue , \u0120this \u0120mind - n umb ingly \u0120awful \u0120dri vel \u0120is \u0120the \u0120perfect \u0120way \u0120to \u0120test \u0120one 's \u0120sanity : \u0120if \u0120you 've \u0120still \u0120got \u0120all \u0120your \u0120mar bles , \u0120you 'll \u0120switch \u0120this \u0120rubbish \u0120off \u0120and \u0120watch \u0120something \u0120decent \u0120instead \u0120( I \u0120watched \u0120the \u0120whole \u0120thing , \u0120but \u0120am \u0120well \u0120aware \u0120that \u0120I 'm \u0120completely \u0120barking !). </s>,<s> Peter \u0120C ushing \u0120and \u0120Donald \u0120Ple as ance \u0120are \u0120legendary \u0120actors , \u0120and \u0120director \u0120K ost as \u0120Kar ag ian nis \u0120was \u0120the \u0120man \u0120behind \u0120the \u0120successful \u0120Greek \u0120Gi allo - es quire \u0120thriller \u0120Death \u0120Kiss \u0120in \u01201974 ; \u0120and \u0120yet \u0120when \u0120you \u0120combine \u0120the \u0120three \u0120talents , \u0120all \u0120you \u0120get \u0120is \u0120this \u0120complete \u0120load \u0120of \u0120dri vel ! \u0120God \u0120only \u0120knows \u0120what \u0120drove \u0120the \u0120likes \u0120of \u0120Peter \u0120C ushing \u0120and \u0120Donald \u0120Ple as ance \u0120to \u0120star \u0120in \u0120this \u0120cheap ie \u0120devil \u0120worship \u0120flick , \u0120but \u0120I \u0120really \u0120do \u0120hope \u0120they \u0120were \u0120well \u0120paid \u0120as \u0120neither \u0120one \u0120deserves \u0120something \u0120as \u0120amateur ish \u0120as \u0120this \u0120on \u0120their \u0120resumes . \u0120The \u0120story \u0120focuses \u0120on \u0120a \u0120group \u0120of \u0120devil \u0120worsh ippers \u0120that \u0120kidnap \u0120some \u0120kids , \u0120leading \u0120another \u0120group \u0120to \u0120go \u0120after \u0120them . \u0120The \u0120pace \u0120of \u0120the \u0120plot \u0120is \u0120very \u0120slow \u0120and \u0120this \u0120ensures \u0120that \u0120the \u0120film \u0120is \u0120very \u0120boring . \u0120The \u0120plot \u0120is \u0120also \u0120a \u0120long \u0120way \u0120from \u0120being \u0120original \u0120and \u0120anyone \u0120with \u0120even \u0120a \u0120passing \u0120interest \u0120in \u0120the \u0120horror \u0120genre \u0120will \u0120have \u0120seen \u0120something \u0120a \u0120bit \u0120like \u0120this , \u0120and \u0120no \u0120doubt \u0120done \u0120much \u0120better . \u0120The \u0120obvious \u0120lack \u0120of \u0120budget \u0120is \u0120felt \u0120throughout \u0120and \u0120the \u0120film \u0120doesn 't \u0120manage \u0120to \u0120overcome \u0120this \u0120at \u0120any \u0120point . \u0120This \u0120really \u0120is \u0120a \u0120depressing \u0120and \u0120miserable \u0120watch \u0120and \u0120not \u0120even \u0120a \u0120slightly \u0120decent \u0120ending \u0120manages \u0120to \u0120up \u0120the \u0120ante \u0120enough \u0120to \u0120lift \u0120this \u0120film \u0120out \u0120of \u0120the \u0120very \u0120bottom \u0120of \u0120the \u0120barrel . \u0120Extreme ly \u0120poor \u0120stuff \u0120and \u0120definitely \u0120not \u0120recommended ! </s>,<s> Back \u0120in \u0120the \u01201970 s , \u0120WP IX \u0120ran \u0120\" The \u0120Adventures \u0120of \u0120Superman \" \u0120every \u0120weekday \u0120afternoon \u0120for \u0120quite \u0120a \u0120few \u0120years . \u0120Every \u0120once \u0120in \u0120a \u0120while , \u0120we 'd \u0120get \u0120a \u0120treat \u0120when \u0120they \u0120would \u0120preempt \u0120neighboring \u0120shows \u0120to \u0120air \u0120\" Super man \u0120and \u0120the \u0120Mole \u0120Men .\" \u0120I \u0120always \u0120looked \u0120forward \u0120to \u0120those \u0120days . \u0120Watching \u0120it \u0120recently , \u0120I \u0120was \u0120surprised \u0120at \u0120just \u0120how \u0120bad \u0120it \u0120really \u0120was .< br \u0120/ >< br \u0120/> It \u0120wasn 't \u0120bad \u0120because \u0120of \u0120the \u0120special \u0120effects , \u0120or \u0120lack \u0120thereof . \u0120True , \u0120George \u0120Reeves ' \u0120Superman \u0120costume \u0120was \u0120pretty \u0120bad , \u0120the \u0120edges \u0120of \u0120the \u0120foam \u0120padding \u0120used \u0120to \u0120make \u0120him \u0120look \u0120more \u0120imposing \u0120being \u0120plainly \u0120visible . \u0120And \u0120true , \u0120the \u0120Mole \u0120Men 's \u0120costumes \u0120were \u0120even \u0120worse . \u0120What \u0120was \u0120supposed \u0120to \u0120be \u0120a \u0120furry \u0120covering \u0120wouldn 't \u0120have \u0120fooled \u0120a \u0120ten \u0120year - old , \u0120since \u0120the \u0120z ippers , \u0120sleeve \u0120he ms \u0120and \u0120badly \u0120p illing \u0120fabric \u0120badly \u0120tailored \u0120into \u0120bag gy \u0120costumes \u0120were \u0120all \u0120painfully \u0120obvious . \u0120But \u0120these \u0120were \u0120forg ivable \u0120shortcomings .< br \u0120/ >< br \u0120/> No , \u0120what \u0120made \u0120it \u0120bad \u0120were \u0120the \u0120cont rived \u0120plot \u0120devices . \u0120Time \u0120and \u0120again , \u0120Superman \u0120failed \u0120to \u0120do \u0120anything \u0120to \u0120keep \u0120the \u0120situation \u0120from \u0120deteriorating . \u0120A \u0120lyn ch \u0120mob \u0120is \u0120searching \u0120for \u0120the \u0120creatures ? \u0120Rather \u0120than \u0120round \u0120up \u0120the \u0120hysterical \u0120crowd \u0120or \u0120search \u0120for \u0120the \u0120creatures \u0120himself , \u0120he \u0120stands \u0120around \u0120explaining \u0120the \u0120dangers \u0120of \u0120the \u0120situation \u0120to \u0120Lois \u0120and \u0120the \u0120PR </s>,<s> Sat an 's \u0120Little \u0120Hel per \u0120is \u0120one \u0120of \u0120the \u0120better \u0120B \u0120Horror \u0120movies \u0120I \u0120have \u0120seen . \u0120When \u0120I \u0120say \u0120better \u0120I \u0120mean \u0120the \u0120story . \u0120The \u0120film \u0120hat ches \u0120a \u0120new \u0120plot , \u0120something \u0120that 's \u0120not \u0120so \u0120clich\u00c3\u00a9 \u0120in \u0120the \u0120Horror \u0120genre \u0120- \u0120something \u0120fresh . \u0120But \u0120there \u0120are \u0120also \u0120some \u0120ridiculous \u0120questions \u0120that \u0120come \u0120along \u0120with \u0120it . \u0120Questions \u0120you \u0120will \u0120be \u0120asking \u0120yourself \u0120throughout \u0120the \u0120movie .< br \u0120/ >< br \u0120/> The \u0120film \u0120first \u0120caught \u0120my \u0120attention \u0120while \u0120I \u0120was \u0120cruising \u0120the \u0120Horror \u0120section \u0120in \u0120HM V . \u0120I \u0120was \u0120tired \u0120of \u0120all \u0120the \u0120so \u0120called \u0120\" ter r ifi ying \" \u0120Hollywood \u0120block busters \u0120and \u0120wanted \u0120something \u0120different . \u0120The \u0120cover \u0120art \u0120for \u0120Satan 's \u0120Little \u0120Hel per \u0120immediately \u0120caught \u0120my \u0120attention . \u0120As \u0120you \u0120can \u0120see , \u0120the \u0120image \u0120draws \u0120you \u0120in \u0120- \u0120it 's \u0120chilling ! \u0120I \u0120knew \u0120it \u0120was \u0120a \u0120straight \u0120to \u0120DVD \u0120release \u0120- \u0120but \u0120I \u0120took \u0120a \u0120chance . \u0120I \u0120mean , \u0120I \u0120just \u0120seen \u0120\" Boo gey \u0120Man \" \u0120the \u0120night \u0120before \u0120- \u0120so \u0120It \u0120couldn 't \u0120get \u0120any \u0120worse ! \u0120After \u0120I \u0120watched \u0120the \u0120movie , \u0120I \u0120was \u0120semi - s atisf ied . \u0120I \u0120loved \u0120the \u0120plot \u0120of \u0120the \u0120movie . \u0120It \u0120was \u0120really \u0120creepy \u0120how \u0120the \u0120killer \u0120was \u0120pretending \u0120to \u0120be \u0120the \u0120little \u0120boys \u0120friend , \u0120so \u0120he \u0120could \u0120kill . \u0120In \u0120some \u0120sick \u0120der anged \u0120way , \u0120he \u0120actually \u0120thought \u0120he \u0120and \u0120the \u0120little \u0120boy \u0120would \u0120become \u0120partners \u0120- \u0120a \u0120duo \u0120of \u0120terror . \u0120It \u0120was \u0120a </s>,<s> I \u0120saw \u0120this \u0120gem \u0120of \u0120a \u0120film \u0120at \u0120Cannes \u0120where \u0120it \u0120was \u0120part \u0120of \u0120the \u0120directors \u0120fortnight .< br \u0120/ >< br \u0120/> Welcome \u0120to \u0120Coll in wood \u0120is \u0120nothing \u0120short \u0120of \u0120superb . \u0120Great \u0120fun \u0120throughout , \u0120with \u0120all \u0120members \u0120of \u0120a \u0120strong \u0120cast \u0120acting \u0120their \u0120socks \u0120off . \u0120It 's \u0120a \u0120sometimes \u0120laugh \u0120out \u0120loud \u0120comedy \u0120about \u0120a \u0120petty \u0120cro ok \u0120( Cos imo , \u0120played \u0120by \u0120Luis \u0120Gu zman ) \u0120who \u0120gets \u0120caught \u0120trying \u0120to \u0120steal \u0120a \u0120car \u0120and \u0120sent \u0120to \u0120prison . \u0120While \u0120in \u0120prison \u0120he \u0120meets \u0120a \u0120` l ifer ' \u0120who \u0120tells \u0120him \u0120of \u0120` the \u0120ultimate \u0120bell ini ' \u0120\u00c2 \u0138 \u0120which \u0120to \u0120you \u0120and \u0120me \u0120\u00c2 \u0138 \u0120is \u0120a \u0120sure - fire \u0120get \u0120rich \u0120quick \u0120scheme . \u0120It \u0120turns \u0120out \u0120that \u0120there \u0120is \u0120a \u0120way \u0120through \u0120from \u0120a \u0120deserted \u0120building \u0120into \u0120the \u0120towns \u0120jew ell ers \u0120shop \u0120\u00c2 \u0138 \u0120which \u0120could \u0120net \u0120millions . \u0120Sounds \u0120simple ? \u0120\u00c2 \u0138 \u0120well \u0120throw \u0120in \u0120all \u0120kinds \u0120of \u0120w acky \u0120characters \u0120and \u0120incidents \u0120along \u0120the \u0120way \u0120and \u0120you \u0120have \u0120got \u0120the \u0120ingredients \u0120for \u0120a \u0120one \u0120wild \u0120ride !! \u0120\u00c2 \u0138 \u0120word \u0120passes \u0120from \u0120one \u0120low \u0120life \u0120loser \u0120to \u0120the \u0120next \u0120and \u0120soon \u0120a \u0120team \u0120of \u0120them \u0120are \u0120assembled \u0120to \u0120try \u0120and \u0120cash \u0120in \u0120on \u0120Cos im os \u0120` bell ini ' \u0120lead \u0120by \u0120failed \u0120boxer \u0120Per o \u0120( Super bly \u0120played \u0120by \u0120Sam \u0120Rock well \u0120\u00c2 \u0138 \u0120surely \u0120a \u0120star \u0120in \u0120the \u0120making ) \u0120and \u0120reluctant \u0120cro ok \u0120Riley \u0120( William \u0120H . \u0120Macy ) \u0120who \u0120is \u0120forced \u0120to </s> y: CategoryList negative,negative,negative,negative,positive Path: .; Test: None","title":"Setting up the DataBunch"},{"location":"Using_Transformers_with_Fastai_Tutorial/#building-the-model","text":"import torch import torch.nn as nn from transformers import RobertaModel # defining our model architecture class CustomRobertaModel(nn.Module): def __init__(self,num_labels=2): super(CustomRobertaModel,self).__init__() self.num_labels = num_labels self.roberta = RobertaModel.from_pretrained(config.roberta_model_name) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None): _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # logits = self.classifier(pooled_output) return logits roberta_model = CustomRobertaModel(num_labels=config.num_labels) learn = Learner(data, roberta_model, metrics=[accuracy]) learn.model.roberta.train() # setting roberta to train as it is in eval mode by default learn.fit_one_cycle(config.epochs, max_lr=config.max_lr) epoch train_loss valid_loss accuracy time 0 0.184614 0.180552 0.929000 02:17","title":"Building the Model"},{"location":"Using_Transformers_with_Fastai_Tutorial/#getting-predictions","text":"def get_preds_as_nparray(ds_type) -> np.ndarray: learn.model.roberta.eval() preds = learn.get_preds(ds_type)[0].detach().cpu().numpy() sampler = [i for i in data.dl(ds_type).sampler] reverse_sampler = np.argsort(sampler) ordered_preds = preds[reverse_sampler, :] pred_values = np.argmax(ordered_preds, axis=1) return ordered_preds, pred_values preds, pred_values = get_preds_as_nparray(DatasetType.Valid) # accuracy on valid (pred_values == data.valid_ds.y.items).mean() 0.929","title":"Getting Predictions"},{"location":"Using_Transformers_with_Fastai_Tutorial/#savingloading-the-model-weights","text":"def save_model(learner, file_name): st = learner.model.state_dict() torch.save(st, file_name) # will save model in current dir # backend is pickle def load_model(learner, file_name): st = torch.load(file_name) learner.model.load_state_dict(st) # monkey patching Learner methods to save and load model file Learner.save_model = save_model Learner.load_model = load_model learn.save_model(\"my_model.bin\") learn.load_model(\"my_model.bin\")","title":"Saving/Loading the model weights"},{"location":"Wikipedia_answer_retrieval_DPR/","text":"!pip install wikipedia transformers sentence_transformers faiss-cpu -q \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.4 MB 36.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78 kB 5.8 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.6 MB 60.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.3 MB 54.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 895 kB 66.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 67 kB 4.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596 kB 64.9 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 54.0 MB/s \u001b[?25h Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone import wikipedia from wikipedia.exceptions import DisambiguationError from transformers import pipeline def divide_chunks(l, n): # looping till length l for i in range(0, len(l), n): yield l[i:i + n] def get_passages(text, k=100): tokens = text.split(\" \") tokens_chunks = list(divide_chunks(tokens, k)) passages = [\" \".join(c) for c in tokens_chunks] return passages def get_passage_for_question(question, wiki_hits=3, passage_len=100, debug=False): top_hits = wikipedia.search(question, wiki_hits) if debug: print(\"Top Wiki hits :\", top_hits) passages = [] for hit in top_hits: try: html_page = wikipedia.page(title = hit, auto_suggest = False) except DisambiguationError: continue hit_passages = get_passages(html_page.content, k=passage_len) passages.extend(hit_passages) return passages qa = pipeline(\"question-answering\", model=\"ankur310794/roberta-base-squad2-nq\") Downloading: 0%| | 0.00/643 [00:00<?, ?B/s] The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file. The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file. Downloading: 0%| | 0.00/473M [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.28k [00:00<?, ?B/s] The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file. Downloading: 0%| | 0.00/780k [00:00<?, ?B/s] Downloading: 0%| | 0.00/446k [00:00<?, ?B/s] Downloading: 0%| | 0.00/772 [00:00<?, ?B/s] The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file. The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file. from transformers import TFAutoModel, AutoTokenizer def combine_results(passages, k=4): passages_list = list(divide_chunks(passages, k)) passages_str = [\" \".join(p) for p in passages_list] return passages_str passage_encoder = TFAutoModel.from_pretrained(\"nlpconnect/dpr-ctx_encoder_bert_uncased_L-2_H-128_A-2\") query_encoder = TFAutoModel.from_pretrained(\"nlpconnect/dpr-question_encoder_bert_uncased_L-2_H-128_A-2\") p_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/dpr-ctx_encoder_bert_uncased_L-2_H-128_A-2\") q_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/dpr-question_encoder_bert_uncased_L-2_H-128_A-2\") Downloading: 0%| | 0.00/658 [00:00<?, ?B/s] Downloading: 0%| | 0.00/16.8M [00:00<?, ?B/s] All model checkpoint layers were used when initializing TFBertModel. All the layers of TFBertModel were initialized from the model checkpoint at nlpconnect/dpr-ctx_encoder_bert_uncased_L-2_H-128_A-2. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training. Downloading: 0%| | 0.00/656 [00:00<?, ?B/s] Downloading: 0%| | 0.00/16.8M [00:00<?, ?B/s] All model checkpoint layers were used when initializing TFBertModel. All the layers of TFBertModel were initialized from the model checkpoint at nlpconnect/dpr-question_encoder_bert_uncased_L-2_H-128_A-2. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training. Downloading: 0%| | 0.00/360 [00:00<?, ?B/s] Downloading: 0%| | 0.00/226k [00:00<?, ?B/s] Downloading: 0%| | 0.00/455k [00:00<?, ?B/s] Downloading: 0%| | 0.00/112 [00:00<?, ?B/s] Downloading: 0%| | 0.00/360 [00:00<?, ?B/s] Downloading: 0%| | 0.00/226k [00:00<?, ?B/s] Downloading: 0%| | 0.00/455k [00:00<?, ?B/s] Downloading: 0%| | 0.00/112 [00:00<?, ?B/s] import numpy as np def extracted_passage_embeddings(processed_passages, max_length=156): passage_inputs = p_tokenizer.batch_encode_plus( processed_passages, add_special_tokens=True, truncation=True, padding=\"max_length\", max_length=max_length, return_token_type_ids=True ) passage_embeddings = passage_encoder.predict([np.array(passage_inputs['input_ids']), np.array(passage_inputs['attention_mask']), np.array(passage_inputs['token_type_ids'])], batch_size=1024, verbose=1) return passage_embeddings def extracted_query_embeddings(queries, max_length=64): query_inputs = q_tokenizer.batch_encode_plus( queries, add_special_tokens=True, truncation=True, padding=\"max_length\", max_length=max_length, return_token_type_ids=True ) query_embeddings = query_encoder.predict([np.array(query_inputs['input_ids']), np.array(query_inputs['attention_mask']), np.array(query_inputs['token_type_ids'])], batch_size=1, verbose=1) return query_embeddings import faiss import spacy nlp = spacy.load(\"en\") def get_answer_full_sent(m_passages, answer_dict): all_sents = list(nlp(m_passages).sents) all_sents = [s.text for s in all_sents] for i in range(len(all_sents)): if len(\"\".join(all_sents[0:i])[answer_dict['start']:answer_dict['end']])>2: answer_dict['answer_sentence'] = all_sents[i-1] return answer_dict return answer_dict from sentence_transformers import CrossEncoder ranking_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=196) def get_reranked_passage(passages, question, top_rr): passage_question_pair = [(question, p) for p in passages] scores = ranking_model.predict(passage_question_pair) shorted_index = np.argpartition(scores, -top_rr)[::-1] shorted_scores = np.array([scores[i] for i in shorted_index]) return [passages[i] for i in shorted_index[0:top_rr]] # end to end with dpr topk_r=30 topk_rr=8 import pandas as pd def get_answer_dpr(question): passages = get_passage_for_question(question, debug=True) print(\"Total passages: \", len(passages)) passage_embeddings = extracted_passage_embeddings(passages) query_embeddings = extracted_query_embeddings([question]) faiss_index = faiss.IndexFlatL2(128) faiss_index.add(passage_embeddings.pooler_output) prob, index = faiss_index.search(query_embeddings.pooler_output, k=topk_r) r_passages = [passages[i] for i in index[0]] print(\"Top k retrieved passages :\", len(r_passages)) rr_passages = get_reranked_passage(r_passages, question, topk_rr) print(\"Top k reranked passages :\", len(rr_passages)) m_passages = combine_results(rr_passages) print(\"Merged passages :\", len(m_passages)) results = qa(question=[question]*len(m_passages), context=m_passages, max_seq_len=512) if isinstance(results, dict): results = [results] output_results = [get_answer_full_sent(m_passages[i],results[i]) for i in range(len(results))] return pd.DataFrame(output_results)[['answer', 'answer_sentence', 'score']].sort_values(\"score\", ascending=False) results= get_answer_dpr(\"where was tara located in gone with the wind?\") results Top Wiki hits : ['Tara (plantation)', 'Margaret Mitchell', 'RKO Forty Acres'] Total passages: 95 1/1 [==============================] - 0s 470ms/step 1/1 [==============================] - 0s 21ms/step Top k retrieved passages : 30 Top k reranked passages : 8 Merged passages : 2 /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray return array(a, dtype, copy=False, order=order) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } answer answer_sentence score 0 Talmadge Farms Now the Tara facade is still located at Talmad... 0.740677 1 virtually the same In the 2007 novel by Donald McCaig, Rhett Butl... 0.159294 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-17c68291-5890-4bd1-924f-9423f8d0f7f7 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-17c68291-5890-4bd1-924f-9423f8d0f7f7'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } results.sort_values(\"score\", ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } answer answer_sentence score 0 Talmadge Farms Now the Tara facade is still located at Talmad... 0.740677 1 virtually the same In the 2007 novel by Donald McCaig, Rhett Butl... 0.159294 .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } const buttonEl = document.querySelector('#df-e023e6a4-4c3f-46d9-9958-2b8b0e640726 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-e023e6a4-4c3f-46d9-9958-2b8b0e640726'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } !pip install gradio -q \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 865 kB 21.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.0 MB 37.0 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 210 kB 50.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61 kB 345 kB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 856 kB 42.0 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.6 MB 54.9 MB/s \u001b[?25h Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone import gradio as gr inp = gr.inputs.Textbox(lines=2, default='what is coronavirus?', label=\"Question\") out = gr.outputs.Dataframe(label=\"Answers\")#gr.outputs.Textbox(label=\"Answers\") gr.Interface(fn=get_answer_dpr, inputs=inp, outputs=out).launch() Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()` Running on public URL: https://21615.gradio.app This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces) (<Flask 'gradio.networking'>, 'http://127.0.0.1:7860/', 'https://21615.gradio.app')","title":"Wikipedia answer retrieval DPR"},{"location":"contextual_topic_modeling/","text":"!pip install contextualized_topic_models Collecting contextualized_topic_models Downloading https://files.pythonhosted.org/packages/e8/83/16ee302ae54b21692ac995d71ae90265473becfd73bcf942c41c0a9b5950/contextualized_topic_models-1.4.2-py2.py3-none-any.whl Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (1.6.0+cu101) Collecting gensim==3.8.3 \u001b[?25l Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24.2MB 130kB/s \u001b[?25hCollecting wheel==0.33.6 Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl Collecting pytest-runner==5.1 Downloading https://files.pythonhosted.org/packages/f8/31/f291d04843523406f242e63b5b90f7b204a756169b4250ff213e10326deb/pytest_runner-5.1-py2.py3-none-any.whl Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (0.7.0+cu101) Collecting sentence-transformers==0.3.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/78/e0/65ad8fd86eba720412d9ff102c4a3540a113bbc6cd29b01e7ecc33ebb1fa/sentence-transformers-0.3.2.tar.gz (65kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 9.6MB/s \u001b[?25hCollecting numpy==1.19.1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.5MB 238kB/s \u001b[?25hCollecting pytest==4.6.5 \u001b[?25l Downloading https://files.pythonhosted.org/packages/97/72/d4d6d22ad216f149685f2e93ce9280df9f36cbbc87fa546641ac92f22766/pytest-4.6.5-py2.py3-none-any.whl (230kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235kB 59.2MB/s \u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->contextualized_topic_models) (0.16.0) Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->contextualized_topic_models) (2.1.0) Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->contextualized_topic_models) (1.15.0) Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->contextualized_topic_models) (1.4.1) Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0->contextualized_topic_models) (7.0.0) Collecting transformers>=3.0.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 778kB 48.0MB/s \u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2->contextualized_topic_models) (4.41.1) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2->contextualized_topic_models) (0.22.2.post1) Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.2->contextualized_topic_models) (3.2.5) Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (1.4.0) Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (20.1.0) Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (0.2.5) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (20.4) Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (1.9.0) Collecting pluggy<1.0,>=0.12 Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (1.7.0) Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (8.4.0) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.23.0) Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.49.0) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (1.14.48) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.2->contextualized_topic_models) (3.0.12) Collecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 49.3MB/s \u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.2->contextualized_topic_models) (2019.12.20) Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.2->contextualized_topic_models) (0.7) Collecting tokenizers==0.8.1.rc1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 48.4MB/s \u001b[?25hCollecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 48.1MB/s \u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.2->contextualized_topic_models) (0.16.0) Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest==4.6.5->contextualized_topic_models) (2.4.7) Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12->pytest==4.6.5->contextualized_topic_models) (3.1.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (1.24.3) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.10) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (3.0.4) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2020.6.20) Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (0.10.0) Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (0.3.3) Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (1.17.48) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.2->contextualized_topic_models) (7.1.2) Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (0.15.2) Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.8.1) Building wheels for collected packages: sentence-transformers, sacremoses Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sentence-transformers: filename=sentence_transformers-0.3.2-cp36-none-any.whl size=93965 sha256=f16e69cf4e046025ab7a2d6862502cf2e431b7cfd90def10046f9bba45bf6abe Stored in directory: /root/.cache/pip/wheels/f7/06/a0/567f3651876165429f6510d3197b011652a25e547552816824 Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=936a3c608102bf3d7950dac3afb5db63605b8a340cd539e4390c9c3031cd29d8 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sentence-transformers sacremoses \u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m \u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m \u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m Installing collected packages: numpy, gensim, wheel, pytest-runner, sacremoses, tokenizers, sentencepiece, transformers, sentence-transformers, pluggy, pytest, contextualized-topic-models Found existing installation: numpy 1.18.5 Uninstalling numpy-1.18.5: Successfully uninstalled numpy-1.18.5 Found existing installation: gensim 3.6.0 Uninstalling gensim-3.6.0: Successfully uninstalled gensim-3.6.0 Found existing installation: wheel 0.35.1 Uninstalling wheel-0.35.1: Successfully uninstalled wheel-0.35.1 Found existing installation: pluggy 0.7.1 Uninstalling pluggy-0.7.1: Successfully uninstalled pluggy-0.7.1 Found existing installation: pytest 3.6.4 Uninstalling pytest-3.6.4: Successfully uninstalled pytest-3.6.4 Successfully installed contextualized-topic-models-1.4.2 gensim-3.8.3 numpy-1.19.1 pluggy-0.13.1 pytest-4.6.5 pytest-runner-5.1 sacremoses-0.0.43 sentence-transformers-0.3.2 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2 wheel-0.33.6 !git clone https://github.com/tdhopper/topic-modeling-datasets.git Cloning into 'topic-modeling-datasets'... remote: Enumerating objects: 18915, done.\u001b[K remote: Counting objects: 100% (18915/18915), done.\u001b[K remote: Compressing objects: 100% (18875/18875), done.\u001b[K remote: Total 18915 (delta 37), reused 18915 (delta 37), pack-reused 0\u001b[K Receiving objects: 100% (18915/18915), 29.69 MiB | 16.43 MiB/s, done. Resolving deltas: 100% (37/37), done. from contextualized_topic_models.models.ctm import CTM from contextualized_topic_models.utils.data_preparation import TextHandler from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file from contextualized_topic_models.datasets.dataset import CTMDataset !head -n 10 topic-modeling-datasets/data/lda-c/blei-ap/ap.txt <DOC> <DOCNO> AP881218-0003 </DOCNO> <TEXT> A 16-year-old student at a private Baptist school who allegedly killed one teacher and wounded another before firing into a filled classroom apparently ``just snapped,'' the school's pastor said. ``I don't know how it could have happened,'' said George Sweet, pastor of Atlantic Shores Baptist Church. ``This is a good, Christian school. We pride ourselves on discipline. Our kids are good kids.'' The Atlantic Shores Christian School sophomore was arrested and charged with first-degree murder, attempted murder, malicious assault and related felony charges for the Friday morning shooting. Police would not release the boy's name because he is a juvenile, but neighbors and relatives identified him as Nicholas Elliott. Police said the student was tackled by a teacher and other students when his semiautomatic pistol jammed as he fired on the classroom as the students cowered on the floor crying ``Jesus save us! God save us!'' Friends and family said the boy apparently was troubled by his grandmother's death and the divorce of his parents and had been tormented by classmates. Nicholas' grandfather, Clarence Elliott Sr., said Saturday that the boy's parents separated about four years ago and his maternal grandmother, Channey Williams, died last year after a long illness. The grandfather also said his grandson was fascinated with guns. ``The boy was always talking about guns,'' he said. ``He knew a lot about them. He knew all the names of them _ none of those little guns like a .32 or a .22 or nothing like that. He liked the big ones.'' The slain teacher was identified as Karen H. Farley, 40. The wounded teacher, 37-year-old Sam Marino, was in serious condition Saturday with gunshot wounds in the shoulder. Police said the boy also shot at a third teacher, Susan Allen, 31, as she fled from the room where Marino was shot. He then shot Marino again before running to a third classroom where a Bible class was meeting. The youngster shot the glass out of a locked door before opening fire, police spokesman Lewis Thurston said. When the youth's pistol jammed, he was tackled by teacher Maurice Matteson, 24, and other students, Thurston said. ``Once you see what went on in there, it's a miracle that we didn't have more people killed,'' Police Chief Charles R. Wall said. Police didn't have a motive, Detective Tom Zucaro said, but believe the boy's primary target was not a teacher but a classmate. Officers found what appeared to be three Molotov cocktails in the boy's locker and confiscated the gun and several spent shell casings. Fourteen rounds were fired before the gun jammed, Thurston said. The gun, which the boy carried to school in his knapsack, was purchased by an adult at the youngster's request, Thurston said, adding that authorities have interviewed the adult, whose name is being withheld pending an investigation by the federal Bureau of Alcohol, Tobacco and Firearms. The shootings occurred in a complex of four portable classrooms for junior and senior high school students outside the main building of the 4-year-old school. The school has 500 students in kindergarten through 12th grade. Police said they were trying to reconstruct the sequence of events and had not resolved who was shot first. The body of Ms. Farley was found about an hour after the shootings behind a classroom door. </TEXT> </DOC> <DOC> <DOCNO> AP880224-0195 </DOCNO> <TEXT> The Bechtel Group Inc. offered in 1985 to sell oil to Israel at a discount of at least $650 million for 10 years if it promised not to bomb a proposed Iraqi pipeline, a Foreign Ministry official said Wednesday. But then-Prime Minister Shimon Peres said the offer from Bruce Rappaport, a partner in the San Francisco-based construction and engineering company, was ``unimportant,'' the senior official told The Associated Press. Peres, now foreign minister, never discussed the offer with other government ministers, said the official, who spoke on condition of anonymity. The comments marked the first time Israel has acknowledged any offer was made for assurances not to bomb the planned $1 billion pipeline, which was to have run near Israel's border with Jordan. The pipeline was never built. In San Francisco, Tom Flynn, vice president for public relations for the Bechtel Group, said the company did not make any offer to Peres but that Rappaport, a Swiss financier, made it without Bechtel's knowledge or consent. Another Bechtel spokesman, Al Donner, said Bechtel ``at no point'' in development of the pipeline project had anything to do with the handling of the oil. He said proposals submitted by the company ``did not include any specific arrangements for the handling of the oil or for the disposal of the oil once it reached the terminal.'' Asked about Bechtel's disclaimers after they were made in San Francisco, the Israeli Foreign Ministry official said Peres believed Rappaport made the offer for the company. ``Rappaport came to Peres as a representative of Bechtel and said he was speaking on behalf of Bechtel,'' the official said. ``If he was not, he misrepresented himself.'' The Jerusalem Post on Wednesday quoted sources close to Peres as saying that according to Rappaport, Bechtel had said the oil sales would have to be conducted through a third party to keep the sales secret from Iraq and Jordan. The Foreign Ministry official said Peres did not take the offer seriously. ``This is a man who sees 10 people every day,'' he said. ``Thirty percent of them come with crazy ideas. He just says, `Yes, yes. We'll think about it.' That's how things work in Israel.'' The offer appeared to be the one mentioned in a September 1985 memo to Attorney General Edwin Meese III. The memo referred to an arrangement between Peres and Rappaport ``to the effect that Israel will receive somewhere between $65 million and $70 million a year for 10 years.'' The memo from Meese friend E. Robert Wallach, Rappaport's attorney, also states, ``What was also indicated to me, and which would be denied everywhere, is that a portion of those funds will go directly to Labor,'' a reference to the political party Peres leads. The Wallach memo has become the focus of an investigation into whether Meese knew of a possibly improper payment. Peres has denied any wrongdoing and has denounced the memo as ``complete nonsense.'' The Israeli official said Rappaport, a native of Israel and a close friend of Peres, relayed the offer to Peres earlier in September. ``Peres thought the offer was unimportant. For him, the most important thing was to have an Iraqi oil port near Israel's border,'' the official said. ``The thinking was that this would put Iraq in a position where it would not be able to wage war with Israel, out of concern for its pipeline.'' A person answering the telephone at Rappaport's Swiss residence said he was out of town and could not be reached for comment. input_file = 'topic-modeling-datasets/data/lda-c/blei-ap/ap.txt' handler = TextHandler(input_file) handler.prepare() # create vocabulary and training data bert_embeddings_from_file? training_bert = bert_embeddings_from_file(input_file, \"distilbert-base-nli-mean-tokens\") 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 245M/245M [00:03<00:00, 69.3MB/s] training_dataset = CTMDataset(handler.bow, training_bert, handler.idx2token) ctm = CTM(input_size=len(handler.vocab), bert_input_size=768, num_epochs=5, inference_type=\"combined\", n_components=50) ctm.fit(training_dataset) # run the model Settings: N Components: 50 Topic Prior Mean: 0.0 Topic Prior Variance: 0.98 Model Type: prodLDA Hidden Sizes: (100, 100) Activation: softplus Dropout: 0.2 Learn Priors: True Learning Rate: 0.002 Momentum: 0.99 Reduce On Plateau: False Save Dir: None Epoch: [1/5] Samples: [13500/67500] Train Loss: 675.9943767361111 Time: 0:00:14.059041 Epoch: [2/5] Samples: [27000/67500] Train Loss: 650.9020456090857 Time: 0:00:14.110983 Epoch: [3/5] Samples: [40500/67500] Train Loss: 647.2234633246528 Time: 0:00:14.079987 Epoch: [4/5] Samples: [54000/67500] Train Loss: 645.6622060908564 Time: 0:00:14.050282 Epoch: [5/5] Samples: [67500/67500] Train Loss: 643.8000949074074 Time: 0:00:14.067917 ctm.get_topic_lists(5)[0:5] [['younger', 'products.', 'Street,', 'pledged', 'claims.'], ['A', 'million', 'was', 'said.', 'The'], ['2,', 'section', 'Seattle', 'primary.', 'link'], ['to', 'that', 'the', 'is', 'be'], ['Association,', 'pledged', 'participate', '0.2', 'drugs,']] Prediction !head -n 10 topic-modeling-datasets/data/lda-c/blei-ap/ap.txt <DOC> <DOCNO> AP881218-0003 </DOCNO> <TEXT> A 16-year-old student at a private Baptist school who allegedly killed one teacher and wounded another before firing into a filled classroom apparently ``just snapped,'' the school's pastor said. ``I don't know how it could have happened,'' said George Sweet, pastor of Atlantic Shores Baptist Church. ``This is a good, Christian school. We pride ourselves on discipline. Our kids are good kids.'' The Atlantic Shores Christian School sophomore was arrested and charged with first-degree murder, attempted murder, malicious assault and related felony charges for the Friday morning shooting. Police would not release the boy's name because he is a juvenile, but neighbors and relatives identified him as Nicholas Elliott. Police said the student was tackled by a teacher and other students when his semiautomatic pistol jammed as he fired on the classroom as the students cowered on the floor crying ``Jesus save us! God save us!'' Friends and family said the boy apparently was troubled by his grandmother's death and the divorce of his parents and had been tormented by classmates. Nicholas' grandfather, Clarence Elliott Sr., said Saturday that the boy's parents separated about four years ago and his maternal grandmother, Channey Williams, died last year after a long illness. The grandfather also said his grandson was fascinated with guns. ``The boy was always talking about guns,'' he said. ``He knew a lot about them. He knew all the names of them _ none of those little guns like a .32 or a .22 or nothing like that. He liked the big ones.'' The slain teacher was identified as Karen H. Farley, 40. The wounded teacher, 37-year-old Sam Marino, was in serious condition Saturday with gunshot wounds in the shoulder. Police said the boy also shot at a third teacher, Susan Allen, 31, as she fled from the room where Marino was shot. He then shot Marino again before running to a third classroom where a Bible class was meeting. The youngster shot the glass out of a locked door before opening fire, police spokesman Lewis Thurston said. When the youth's pistol jammed, he was tackled by teacher Maurice Matteson, 24, and other students, Thurston said. ``Once you see what went on in there, it's a miracle that we didn't have more people killed,'' Police Chief Charles R. Wall said. Police didn't have a motive, Detective Tom Zucaro said, but believe the boy's primary target was not a teacher but a classmate. Officers found what appeared to be three Molotov cocktails in the boy's locker and confiscated the gun and several spent shell casings. Fourteen rounds were fired before the gun jammed, Thurston said. The gun, which the boy carried to school in his knapsack, was purchased by an adult at the youngster's request, Thurston said, adding that authorities have interviewed the adult, whose name is being withheld pending an investigation by the federal Bureau of Alcohol, Tobacco and Firearms. The shootings occurred in a complex of four portable classrooms for junior and senior high school students outside the main building of the 4-year-old school. The school has 500 students in kindergarten through 12th grade. Police said they were trying to reconstruct the sequence of events and had not resolved who was shot first. The body of Ms. Farley was found about an hour after the shootings behind a classroom door. </TEXT> </DOC> <DOC> <DOCNO> AP880224-0195 </DOCNO> <TEXT> The Bechtel Group Inc. offered in 1985 to sell oil to Israel at a discount of at least $650 million for 10 years if it promised not to bomb a proposed Iraqi pipeline, a Foreign Ministry official said Wednesday. But then-Prime Minister Shimon Peres said the offer from Bruce Rappaport, a partner in the San Francisco-based construction and engineering company, was ``unimportant,'' the senior official told The Associated Press. Peres, now foreign minister, never discussed the offer with other government ministers, said the official, who spoke on condition of anonymity. The comments marked the first time Israel has acknowledged any offer was made for assurances not to bomb the planned $1 billion pipeline, which was to have run near Israel's border with Jordan. The pipeline was never built. In San Francisco, Tom Flynn, vice president for public relations for the Bechtel Group, said the company did not make any offer to Peres but that Rappaport, a Swiss financier, made it without Bechtel's knowledge or consent. Another Bechtel spokesman, Al Donner, said Bechtel ``at no point'' in development of the pipeline project had anything to do with the handling of the oil. He said proposals submitted by the company ``did not include any specific arrangements for the handling of the oil or for the disposal of the oil once it reached the terminal.'' Asked about Bechtel's disclaimers after they were made in San Francisco, the Israeli Foreign Ministry official said Peres believed Rappaport made the offer for the company. ``Rappaport came to Peres as a representative of Bechtel and said he was speaking on behalf of Bechtel,'' the official said. ``If he was not, he misrepresented himself.'' The Jerusalem Post on Wednesday quoted sources close to Peres as saying that according to Rappaport, Bechtel had said the oil sales would have to be conducted through a third party to keep the sales secret from Iraq and Jordan. The Foreign Ministry official said Peres did not take the offer seriously. ``This is a man who sees 10 people every day,'' he said. ``Thirty percent of them come with crazy ideas. He just says, `Yes, yes. We'll think about it.' That's how things work in Israel.'' The offer appeared to be the one mentioned in a September 1985 memo to Attorney General Edwin Meese III. The memo referred to an arrangement between Peres and Rappaport ``to the effect that Israel will receive somewhere between $65 million and $70 million a year for 10 years.'' The memo from Meese friend E. Robert Wallach, Rappaport's attorney, also states, ``What was also indicated to me, and which would be denied everywhere, is that a portion of those funds will go directly to Labor,'' a reference to the political party Peres leads. The Wallach memo has become the focus of an investigation into whether Meese knew of a possibly improper payment. Peres has denied any wrongdoing and has denounced the memo as ``complete nonsense.'' The Israeli official said Rappaport, a native of Israel and a close friend of Peres, relayed the offer to Peres earlier in September. ``Peres thought the offer was unimportant. For him, the most important thing was to have an Iraqi oil port near Israel's border,'' the official said. ``The thinking was that this would put Iraq in a position where it would not be able to wage war with Israel, out of concern for its pipeline.'' A person answering the telephone at Rappaport's Swiss residence said he was out of town and could not be reached for comment. len(training_dataset) 13500 import numpy as np distribution = ctm.get_thetas(training_dataset)[100] # topic distribution for the eight document print(distribution) topic = np.argmax(distribution) ctm.get_topic_lists(5)[topic] [0.01682434044778347, 0.010533287189900875, 0.0016027707606554031, 0.018512511625885963, 0.021795328706502914, 0.024250835180282593, 0.007017648313194513, 0.008357801474630833, 0.008402344770729542, 0.0027029572520405054, 0.003236175747588277, 0.01058495044708252, 0.0018672933802008629, 0.023581067100167274, 0.08781484514474869, 0.012609513476490974, 0.007289501838386059, 0.022212006151676178, 0.004904361441731453, 0.03551305830478668, 0.02438373491168022, 0.002901036525145173, 0.02808697521686554, 0.03660248965024948, 0.00808636099100113, 0.0047630425542593, 0.001469444832764566, 0.02342250943183899, 0.02564718760550022, 0.171001598238945, 0.024755068123340607, 0.004217991139739752, 0.03483238071203232, 0.013865076005458832, 0.013528377749025822, 0.06646153330802917, 0.007620108313858509, 0.020605480298399925, 0.009293865412473679, 0.00579161336645484, 0.001458485028706491, 0.003241141326725483, 0.010330692864954472, 0.013481805101037025, 0.01557138655334711, 0.026073671877384186, 0.003067850833758712, 0.018384194001555443, 0.048528604209423065, 0.0029137199744582176] ['agreements', 'heating', 'administrative', \"governor's\", 'lines.']","title":"Contextual topic modeling"},{"location":"contextual_topic_modeling/#prediction","text":"!head -n 10 topic-modeling-datasets/data/lda-c/blei-ap/ap.txt <DOC> <DOCNO> AP881218-0003 </DOCNO> <TEXT> A 16-year-old student at a private Baptist school who allegedly killed one teacher and wounded another before firing into a filled classroom apparently ``just snapped,'' the school's pastor said. ``I don't know how it could have happened,'' said George Sweet, pastor of Atlantic Shores Baptist Church. ``This is a good, Christian school. We pride ourselves on discipline. Our kids are good kids.'' The Atlantic Shores Christian School sophomore was arrested and charged with first-degree murder, attempted murder, malicious assault and related felony charges for the Friday morning shooting. Police would not release the boy's name because he is a juvenile, but neighbors and relatives identified him as Nicholas Elliott. Police said the student was tackled by a teacher and other students when his semiautomatic pistol jammed as he fired on the classroom as the students cowered on the floor crying ``Jesus save us! God save us!'' Friends and family said the boy apparently was troubled by his grandmother's death and the divorce of his parents and had been tormented by classmates. Nicholas' grandfather, Clarence Elliott Sr., said Saturday that the boy's parents separated about four years ago and his maternal grandmother, Channey Williams, died last year after a long illness. The grandfather also said his grandson was fascinated with guns. ``The boy was always talking about guns,'' he said. ``He knew a lot about them. He knew all the names of them _ none of those little guns like a .32 or a .22 or nothing like that. He liked the big ones.'' The slain teacher was identified as Karen H. Farley, 40. The wounded teacher, 37-year-old Sam Marino, was in serious condition Saturday with gunshot wounds in the shoulder. Police said the boy also shot at a third teacher, Susan Allen, 31, as she fled from the room where Marino was shot. He then shot Marino again before running to a third classroom where a Bible class was meeting. The youngster shot the glass out of a locked door before opening fire, police spokesman Lewis Thurston said. When the youth's pistol jammed, he was tackled by teacher Maurice Matteson, 24, and other students, Thurston said. ``Once you see what went on in there, it's a miracle that we didn't have more people killed,'' Police Chief Charles R. Wall said. Police didn't have a motive, Detective Tom Zucaro said, but believe the boy's primary target was not a teacher but a classmate. Officers found what appeared to be three Molotov cocktails in the boy's locker and confiscated the gun and several spent shell casings. Fourteen rounds were fired before the gun jammed, Thurston said. The gun, which the boy carried to school in his knapsack, was purchased by an adult at the youngster's request, Thurston said, adding that authorities have interviewed the adult, whose name is being withheld pending an investigation by the federal Bureau of Alcohol, Tobacco and Firearms. The shootings occurred in a complex of four portable classrooms for junior and senior high school students outside the main building of the 4-year-old school. The school has 500 students in kindergarten through 12th grade. Police said they were trying to reconstruct the sequence of events and had not resolved who was shot first. The body of Ms. Farley was found about an hour after the shootings behind a classroom door. </TEXT> </DOC> <DOC> <DOCNO> AP880224-0195 </DOCNO> <TEXT> The Bechtel Group Inc. offered in 1985 to sell oil to Israel at a discount of at least $650 million for 10 years if it promised not to bomb a proposed Iraqi pipeline, a Foreign Ministry official said Wednesday. But then-Prime Minister Shimon Peres said the offer from Bruce Rappaport, a partner in the San Francisco-based construction and engineering company, was ``unimportant,'' the senior official told The Associated Press. Peres, now foreign minister, never discussed the offer with other government ministers, said the official, who spoke on condition of anonymity. The comments marked the first time Israel has acknowledged any offer was made for assurances not to bomb the planned $1 billion pipeline, which was to have run near Israel's border with Jordan. The pipeline was never built. In San Francisco, Tom Flynn, vice president for public relations for the Bechtel Group, said the company did not make any offer to Peres but that Rappaport, a Swiss financier, made it without Bechtel's knowledge or consent. Another Bechtel spokesman, Al Donner, said Bechtel ``at no point'' in development of the pipeline project had anything to do with the handling of the oil. He said proposals submitted by the company ``did not include any specific arrangements for the handling of the oil or for the disposal of the oil once it reached the terminal.'' Asked about Bechtel's disclaimers after they were made in San Francisco, the Israeli Foreign Ministry official said Peres believed Rappaport made the offer for the company. ``Rappaport came to Peres as a representative of Bechtel and said he was speaking on behalf of Bechtel,'' the official said. ``If he was not, he misrepresented himself.'' The Jerusalem Post on Wednesday quoted sources close to Peres as saying that according to Rappaport, Bechtel had said the oil sales would have to be conducted through a third party to keep the sales secret from Iraq and Jordan. The Foreign Ministry official said Peres did not take the offer seriously. ``This is a man who sees 10 people every day,'' he said. ``Thirty percent of them come with crazy ideas. He just says, `Yes, yes. We'll think about it.' That's how things work in Israel.'' The offer appeared to be the one mentioned in a September 1985 memo to Attorney General Edwin Meese III. The memo referred to an arrangement between Peres and Rappaport ``to the effect that Israel will receive somewhere between $65 million and $70 million a year for 10 years.'' The memo from Meese friend E. Robert Wallach, Rappaport's attorney, also states, ``What was also indicated to me, and which would be denied everywhere, is that a portion of those funds will go directly to Labor,'' a reference to the political party Peres leads. The Wallach memo has become the focus of an investigation into whether Meese knew of a possibly improper payment. Peres has denied any wrongdoing and has denounced the memo as ``complete nonsense.'' The Israeli official said Rappaport, a native of Israel and a close friend of Peres, relayed the offer to Peres earlier in September. ``Peres thought the offer was unimportant. For him, the most important thing was to have an Iraqi oil port near Israel's border,'' the official said. ``The thinking was that this would put Iraq in a position where it would not be able to wage war with Israel, out of concern for its pipeline.'' A person answering the telephone at Rappaport's Swiss residence said he was out of town and could not be reached for comment. len(training_dataset) 13500 import numpy as np distribution = ctm.get_thetas(training_dataset)[100] # topic distribution for the eight document print(distribution) topic = np.argmax(distribution) ctm.get_topic_lists(5)[topic] [0.01682434044778347, 0.010533287189900875, 0.0016027707606554031, 0.018512511625885963, 0.021795328706502914, 0.024250835180282593, 0.007017648313194513, 0.008357801474630833, 0.008402344770729542, 0.0027029572520405054, 0.003236175747588277, 0.01058495044708252, 0.0018672933802008629, 0.023581067100167274, 0.08781484514474869, 0.012609513476490974, 0.007289501838386059, 0.022212006151676178, 0.004904361441731453, 0.03551305830478668, 0.02438373491168022, 0.002901036525145173, 0.02808697521686554, 0.03660248965024948, 0.00808636099100113, 0.0047630425542593, 0.001469444832764566, 0.02342250943183899, 0.02564718760550022, 0.171001598238945, 0.024755068123340607, 0.004217991139739752, 0.03483238071203232, 0.013865076005458832, 0.013528377749025822, 0.06646153330802917, 0.007620108313858509, 0.020605480298399925, 0.009293865412473679, 0.00579161336645484, 0.001458485028706491, 0.003241141326725483, 0.010330692864954472, 0.013481805101037025, 0.01557138655334711, 0.026073671877384186, 0.003067850833758712, 0.018384194001555443, 0.048528604209423065, 0.0029137199744582176] ['agreements', 'heating', 'administrative', \"governor's\", 'lines.']","title":"Prediction"},{"location":"knowledge_distillation_exploration/","text":"import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers vocab_size = 20000 # Only consider the top 20k words maxlen = 200 # Only consider the first 200 words of each movie review (x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size) print(len(x_train), \"Training sequences\") print(len(x_val), \"Validation sequences\") x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen) 25000 Training sequences 25000 Validation sequences class Distiller(keras.Model): def __init__(self, student, teacher): super(Distiller, self).__init__() self.teacher = student self.student = teacher def compile( self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3, ): \"\"\" Configure the distiller. Args: optimizer: Keras optimizer for the student weights metrics: Keras metrics for evaluation student_loss_fn: Loss function of difference between student predictions and ground-truth distillation_loss_fn: Loss function of difference between soft student predictions and soft teacher predictions alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn temperature: Temperature for softening probability distributions. Larger temperature gives softer distributions. \"\"\" super(Distiller, self).compile(optimizer=optimizer, metrics=metrics) self.student_loss_fn = student_loss_fn self.distillation_loss_fn = distillation_loss_fn self.alpha = alpha self.temperature = temperature def train_step(self, data): # Unpack data x, y = data # Forward pass of teacher teacher_predictions = self.teacher(x, training=False) with tf.GradientTape() as tape: # Forward pass of student student_predictions = self.student(x, training=True) # Compute losses student_loss = self.student_loss_fn(y, student_predictions) distillation_loss = self.distillation_loss_fn( tf.nn.softmax(teacher_predictions / self.temperature, axis=1), tf.nn.softmax(student_predictions / self.temperature, axis=1), ) loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss # Compute gradients trainable_vars = self.student.trainable_variables gradients = tape.gradient(loss, trainable_vars) # Update weights self.optimizer.apply_gradients(zip(gradients, trainable_vars)) # Update the metrics configured in `compile()`. self.compiled_metrics.update_state(y, student_predictions) # Return a dict of performance results = {m.name: m.result() for m in self.metrics} results.update( {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss} ) return results def test_step(self, data): # Unpack the data x, y = data # Compute predictions y_prediction = self.student(x, training=False) # Calculate the loss student_loss = self.student_loss_fn(y, y_prediction) # Update the metrics. self.compiled_metrics.update_state(y, y_prediction) # Return a dict of performance results = {m.name: m.result() for m in self.metrics} results.update({\"student_loss\": student_loss}) return results # Create the teacher # Input for variable-length sequences of integers inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # Embed each integer in a 128-dimensional vector x = layers.Embedding(vocab_size, 128)(inputs) # Add 2 bidirectional LSTMs and GRUs x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) x = layers.Bidirectional(layers.GRU(128))(x) # Add a classifier outputs = layers.Dense(2)(x) teacher = keras.Model(inputs, outputs, name='teacher') teacher.summary() # Create the student # Input for variable-length sequences of integers inputs = keras.Input(shape=(maxlen,), dtype=\"int32\") # Embed each integer in a 128-dimensional vector x = layers.Embedding(vocab_size, 64)(inputs) # Add 2 bidirectional LSTMs and GRUs x = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(x) x = layers.Bidirectional(layers.GRU(32))(x) # Add a classifier outputs = layers.Dense(2)(x) student = keras.Model(inputs, outputs, name='student') student.summary() # Clone student for later comparison student_scratch = keras.models.clone_model(student) Model: \"teacher\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 200)] 0 _________________________________________________________________ embedding (Embedding) (None, 200, 128) 2560000 _________________________________________________________________ bidirectional (Bidirectional (None, 200, 128) 98816 _________________________________________________________________ bidirectional_1 (Bidirection (None, 256) 198144 _________________________________________________________________ dense (Dense) (None, 2) 514 ================================================================= Total params: 2,857,474 Trainable params: 2,857,474 Non-trainable params: 0 _________________________________________________________________ Model: \"student\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 200)] 0 _________________________________________________________________ embedding_1 (Embedding) (None, 200, 64) 1280000 _________________________________________________________________ bidirectional_2 (Bidirection (None, 200, 64) 24832 _________________________________________________________________ bidirectional_3 (Bidirection (None, 64) 18816 _________________________________________________________________ dense_1 (Dense) (None, 2) 130 ================================================================= Total params: 1,323,778 Trainable params: 1,323,778 Non-trainable params: 0 _________________________________________________________________ # Train teacher as usual teacher.compile( optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.SparseCategoricalAccuracy()], ) # Train and evaluate teacher on data. teacher.fit(x_train, y_train, epochs=3) teacher.evaluate(x_val, y_val) Epoch 1/3 782/782 [==============================] - 43s 55ms/step - loss: 0.3714 - sparse_categorical_accuracy: 0.8351 Epoch 2/3 782/782 [==============================] - 43s 55ms/step - loss: 0.2004 - sparse_categorical_accuracy: 0.9232 Epoch 3/3 782/782 [==============================] - 42s 54ms/step - loss: 0.1297 - sparse_categorical_accuracy: 0.9519 782/782 [==============================] - 12s 15ms/step - loss: 0.4922 - sparse_categorical_accuracy: 0.8590 [0.49218621850013733, 0.8590400218963623] # Initialize and compile distiller distiller = Distiller(student=student, teacher=teacher) distiller.compile( optimizer=keras.optimizers.Adam(), metrics=[keras.metrics.SparseCategoricalAccuracy()], student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True), distillation_loss_fn=keras.losses.KLDivergence(), alpha=0.1, temperature=10, ) # Distill teacher to student distiller.fit(x_train, y_train, epochs=3) # Evaluate student on test dataset distiller.evaluate(x_val, y_val) Epoch 1/3 782/782 [==============================] - 53s 68ms/step - sparse_categorical_accuracy: 0.9664 - student_loss: 0.1382 - distillation_loss: 0.0084 Epoch 2/3 782/782 [==============================] - 54s 69ms/step - sparse_categorical_accuracy: 0.9781 - student_loss: 0.1119 - distillation_loss: 0.0085 Epoch 3/3 782/782 [==============================] - 54s 69ms/step - sparse_categorical_accuracy: 0.9868 - student_loss: 0.0937 - distillation_loss: 0.0088 782/782 [==============================] - 12s 15ms/step - sparse_categorical_accuracy: 0.8476 - student_loss: 0.3908 0.8475599884986877 # Train student as doen usually student_scratch.compile( optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.SparseCategoricalAccuracy()], ) # Train and evaluate student trained from scratch. student_scratch.fit(x_train, y_train, epochs=3) Epoch 1/3 782/782 [==============================] - 31s 39ms/step - loss: 0.3796 - sparse_categorical_accuracy: 0.8246 Epoch 2/3 782/782 [==============================] - 30s 39ms/step - loss: 0.1918 - sparse_categorical_accuracy: 0.9283 Epoch 3/3 782/782 [==============================] - 30s 39ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.9602 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-7-2e22fcfd29d9> in <module>() 8 # Train and evaluate student trained from scratch. 9 student_scratch.fit(x_train, y_train, epochs=3) ---> 10 student_scratch.evaluate(x_test, y_test) NameError: name 'x_test' is not defined student_scratch.evaluate(x_val, y_val) 782/782 [==============================] - 12s 15ms/step - loss: 0.3720 - sparse_categorical_accuracy: 0.8596 [0.37198370695114136, 0.8595600128173828]","title":"Knowledge distillation exploration"},{"location":"large_scale_multilabelclassification/","text":"import numpy as np import tensorflow as tf from tensorflow.keras.layers.experimental import preprocessing !wget -O datasets.zip http://nlp.cs.aueb.gr/software_and_datasets/EURLEX57K/datasets.zip --2020-08-23 11:37:34-- http://nlp.cs.aueb.gr/software_and_datasets/EURLEX57K/datasets.zip Resolving nlp.cs.aueb.gr (nlp.cs.aueb.gr)... 195.251.248.252 Connecting to nlp.cs.aueb.gr (nlp.cs.aueb.gr)|195.251.248.252|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 135996905 (130M) [application/zip] Saving to: \u2018datasets.zip\u2019 datasets.zip 100%[===================>] 129.70M 2.29MB/s in 2m 12s 2020-08-23 11:39:48 (1006 KB/s) - \u2018datasets.zip\u2019 saved [135996905/135996905] !unzip datasets.zip -d EURLEX57K \u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m inflating: EURLEX57K/dataset/dev/32004R1662.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1662.json inflating: EURLEX57K/dataset/dev/32004R0970.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0970.json inflating: EURLEX57K/dataset/dev/31999R0014.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0014.json inflating: EURLEX57K/dataset/dev/32007R1267.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1267.json inflating: EURLEX57K/dataset/dev/31987R3625.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R3625.json inflating: EURLEX57K/dataset/dev/31999R2183.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R2183.json inflating: EURLEX57K/dataset/dev/32012R0997.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0997.json inflating: EURLEX57K/dataset/dev/31994R2926.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2926.json inflating: EURLEX57K/dataset/dev/31986R1595.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1595.json inflating: EURLEX57K/dataset/dev/32001R1381.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1381.json inflating: EURLEX57K/dataset/dev/32011R1280.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1280.json inflating: EURLEX57K/dataset/dev/32007R1322.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1322.json inflating: EURLEX57K/dataset/dev/32005R0254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0254.json inflating: EURLEX57K/dataset/dev/32004R0136.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0136.json inflating: EURLEX57K/dataset/dev/31993D0332.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0332.json inflating: EURLEX57K/dataset/dev/32010R0209.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0209.json inflating: EURLEX57K/dataset/dev/31990R1964.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1964.json inflating: EURLEX57K/dataset/dev/31991D0005.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0005.json inflating: EURLEX57K/dataset/dev/31995R1838.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1838.json inflating: EURLEX57K/dataset/dev/31992D0050.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0050.json inflating: EURLEX57K/dataset/dev/31989D0668.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0668.json inflating: EURLEX57K/dataset/dev/31989R0379.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R0379.json inflating: EURLEX57K/dataset/dev/31994D1031.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D1031.json inflating: EURLEX57K/dataset/dev/32001R1552.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1552.json inflating: EURLEX57K/dataset/dev/31996R0905.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0905.json inflating: EURLEX57K/dataset/dev/32008D0876.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0876.json inflating: EURLEX57K/dataset/dev/32001D0202.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0202.json inflating: EURLEX57K/dataset/dev/32006R0128.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0128.json inflating: EURLEX57K/dataset/dev/31994D0620.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0620.json inflating: EURLEX57K/dataset/dev/32001D0652.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0652.json inflating: EURLEX57K/dataset/dev/32005R0487.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0487.json inflating: EURLEX57K/dataset/dev/31987R1927.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1927.json inflating: EURLEX57K/dataset/dev/31996D0517.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0517.json inflating: EURLEX57K/dataset/dev/32003R0074.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0074.json inflating: EURLEX57K/dataset/dev/31982D0297.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0297.json inflating: EURLEX57K/dataset/dev/32010R0065.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0065.json inflating: EURLEX57K/dataset/dev/32015D0078.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0078.json inflating: EURLEX57K/dataset/dev/31985R2180.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2180.json inflating: EURLEX57K/dataset/dev/31994R2249.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2249.json inflating: EURLEX57K/dataset/dev/32013D0171.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0171.json inflating: EURLEX57K/dataset/dev/31993R2022.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2022.json inflating: EURLEX57K/dataset/dev/32005R0891.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0891.json inflating: EURLEX57K/dataset/dev/32003L0126.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003L0126.json inflating: EURLEX57K/dataset/dev/32011R1146.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1146.json inflating: EURLEX57K/dataset/dev/32006R1386.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1386.json inflating: EURLEX57K/dataset/dev/31989D0490.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0490.json inflating: EURLEX57K/dataset/dev/32001R1250.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1250.json inflating: EURLEX57K/dataset/dev/31985R1942.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1942.json inflating: EURLEX57K/dataset/dev/31979R2968.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R2968.json inflating: EURLEX57K/dataset/dev/31987D0076.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0076.json inflating: EURLEX57K/dataset/dev/31996R0354.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0354.json inflating: EURLEX57K/dataset/dev/31988R3565.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R3565.json inflating: EURLEX57K/dataset/dev/31986R0255.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0255.json inflating: EURLEX57K/dataset/dev/31998R1009.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1009.json inflating: EURLEX57K/dataset/dev/32011D0114.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0114.json inflating: EURLEX57K/dataset/dev/31995R2783.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2783.json inflating: EURLEX57K/dataset/dev/31989R2850.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2850.json inflating: EURLEX57K/dataset/dev/32002R0551.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0551.json inflating: EURLEX57K/dataset/dev/32014R0109.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0109.json inflating: EURLEX57K/dataset/dev/31970D0325.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31970D0325.json inflating: EURLEX57K/dataset/dev/32013D0629(02).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0629(02).json inflating: EURLEX57K/dataset/dev/32011L0042.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011L0042.json inflating: EURLEX57K/dataset/dev/32009D0450.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0450.json inflating: EURLEX57K/dataset/dev/32008R0223.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0223.json inflating: EURLEX57K/dataset/dev/32010R1176.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R1176.json inflating: EURLEX57K/dataset/dev/32013R1089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1089.json inflating: EURLEX57K/dataset/dev/32009R1029.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1029.json inflating: EURLEX57K/dataset/dev/31996D0079.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0079.json inflating: EURLEX57K/dataset/dev/32004R1863.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1863.json inflating: EURLEX57K/dataset/dev/32014R0220.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0220.json inflating: EURLEX57K/dataset/dev/32002R0678.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0678.json inflating: EURLEX57K/dataset/dev/31994R2662.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2662.json inflating: EURLEX57K/dataset/dev/32007D0665.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0665.json inflating: EURLEX57K/dataset/dev/31993R2059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2059.json inflating: EURLEX57K/dataset/dev/32002R0228.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0228.json inflating: EURLEX57K/dataset/dev/31976D0806.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976D0806.json inflating: EURLEX57K/dataset/dev/31988R2118.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R2118.json inflating: EURLEX57K/dataset/dev/31994R3073.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R3073.json inflating: EURLEX57K/dataset/dev/32006L0001.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006L0001.json inflating: EURLEX57K/dataset/dev/32003R0849.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0849.json inflating: EURLEX57K/dataset/dev/32006R1607.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1607.json inflating: EURLEX57K/dataset/dev/31981R1013.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R1013.json inflating: EURLEX57K/dataset/dev/31985R1738.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1738.json inflating: EURLEX57K/dataset/dev/32004R1025.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1025.json inflating: EURLEX57K/dataset/dev/31988D0121.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0121.json inflating: EURLEX57K/dataset/dev/31996R1396.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1396.json inflating: EURLEX57K/dataset/dev/32002R1086.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1086.json inflating: EURLEX57K/dataset/dev/32001D0279.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0279.json inflating: EURLEX57K/dataset/dev/32003R2198.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2198.json inflating: EURLEX57K/dataset/dev/32007R0661.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0661.json inflating: EURLEX57K/dataset/dev/32014D0224.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0224.json inflating: EURLEX57K/dataset/dev/31975R0154.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31975R0154.json inflating: EURLEX57K/dataset/dev/31996D0242.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0242.json inflating: EURLEX57K/dataset/dev/32009D0811.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0811.json inflating: EURLEX57K/dataset/dev/32001D0107.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0107.json inflating: EURLEX57K/dataset/dev/31987R0530.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R0530.json inflating: EURLEX57K/dataset/dev/32002D0502.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0502.json inflating: EURLEX57K/dataset/dev/31991D0183.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0183.json inflating: EURLEX57K/dataset/dev/31997D0570.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0570.json inflating: EURLEX57K/dataset/dev/32008R0761.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0761.json inflating: EURLEX57K/dataset/dev/32001R1207.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1207.json inflating: EURLEX57K/dataset/dev/31999D0196.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0196.json inflating: EURLEX57K/dataset/dev/31995R3080.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R3080.json inflating: EURLEX57K/dataset/dev/32008D0620.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0620.json inflating: EURLEX57K/dataset/dev/31994R0937.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0937.json inflating: EURLEX57K/dataset/dev/32008R0624.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0624.json inflating: EURLEX57K/dataset/dev/32005R1486.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1486.json inflating: EURLEX57K/dataset/dev/32000R1470.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1470.json inflating: EURLEX57K/dataset/dev/31998R2367.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2367.json inflating: EURLEX57K/dataset/dev/32002D0017.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0017.json inflating: EURLEX57K/dataset/dev/32005R0297.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0297.json inflating: EURLEX57K/dataset/dev/31985R1850.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1850.json inflating: EURLEX57K/dataset/dev/31985D0253.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0253.json inflating: EURLEX57K/dataset/dev/31992D0093.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0093.json inflating: EURLEX57K/dataset/dev/32003R2209.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2209.json inflating: EURLEX57K/dataset/dev/32006R0942.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0942.json inflating: EURLEX57K/dataset/dev/32007R1532.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1532.json inflating: EURLEX57K/dataset/dev/31981R3583.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R3583.json inflating: EURLEX57K/dataset/dev/32009R0395.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0395.json inflating: EURLEX57K/dataset/dev/31988D0199.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0199.json inflating: EURLEX57K/dataset/dev/32001R2217.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2217.json inflating: EURLEX57K/dataset/dev/32005R0014.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0014.json inflating: EURLEX57K/dataset/dev/32010R0419.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0419.json inflating: EURLEX57K/dataset/dev/32011R0281.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0281.json inflating: EURLEX57K/dataset/dev/32006R0411.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0411.json inflating: EURLEX57K/dataset/dev/32015R0115.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0115.json inflating: EURLEX57K/dataset/dev/32007R0266.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0266.json inflating: EURLEX57K/dataset/dev/31988R4074.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R4074.json inflating: EURLEX57K/dataset/dev/31989L0342.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989L0342.json inflating: EURLEX57K/dataset/dev/31992R2229.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2229.json inflating: EURLEX57K/dataset/dev/32005R0151.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0151.json inflating: EURLEX57K/dataset/dev/31981R0710.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R0710.json inflating: EURLEX57K/dataset/dev/32009R1091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1091.json inflating: EURLEX57K/dataset/dev/32002R2307.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2307.json inflating: EURLEX57K/dataset/dev/31991R2786.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R2786.json inflating: EURLEX57K/dataset/dev/32004R1971.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1971.json inflating: EURLEX57K/dataset/dev/31990R0573.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R0573.json inflating: EURLEX57K/dataset/dev/31993R2848.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2848.json inflating: EURLEX57K/dataset/dev/32006R1715.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1715.json inflating: EURLEX57K/dataset/dev/32008R0018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0018.json inflating: EURLEX57K/dataset/dev/32001R1641.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1641.json inflating: EURLEX57K/dataset/dev/32013R1027.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1027.json inflating: EURLEX57K/dataset/dev/31986R3592.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3592.json inflating: EURLEX57K/dataset/dev/32011R1310.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1310.json inflating: EURLEX57K/dataset/dev/32001R1211.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1211.json inflating: EURLEX57K/dataset/dev/31994L0035.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994L0035.json inflating: EURLEX57K/dataset/dev/32012R0807.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0807.json inflating: EURLEX57K/dataset/dev/31996R1154.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1154.json inflating: EURLEX57K/dataset/dev/32001D0541.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0541.json inflating: EURLEX57K/dataset/dev/32013D0327.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0327.json inflating: EURLEX57K/dataset/dev/32002R0005.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0005.json inflating: EURLEX57K/dataset/dev/32002R2038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2038.json inflating: EURLEX57K/dataset/dev/32010R0399.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0399.json inflating: EURLEX57K/dataset/dev/32004R2131.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2131.json inflating: EURLEX57K/dataset/dev/31998R1048.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1048.json inflating: EURLEX57K/dataset/dev/32011D0440.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0440.json inflating: EURLEX57K/dataset/dev/32003R0222.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0222.json inflating: EURLEX57K/dataset/dev/32010R0726.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0726.json inflating: EURLEX57K/dataset/dev/32001R2128.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2128.json inflating: EURLEX57K/dataset/dev/32010D0237.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0237.json inflating: EURLEX57K/dataset/dev/31995D0344.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0344.json inflating: EURLEX57K/dataset/dev/32011R0014.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0014.json inflating: EURLEX57K/dataset/dev/31997D0073.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0073.json inflating: EURLEX57K/dataset/dev/31990R0709.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R0709.json inflating: EURLEX57K/dataset/dev/32009D0411.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0411.json inflating: EURLEX57K/dataset/dev/32001R1704.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1704.json inflating: EURLEX57K/dataset/dev/32014R1309.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1309.json inflating: EURLEX57K/dataset/dev/32009D0041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0041.json inflating: EURLEX57K/dataset/dev/31984R0965.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0965.json inflating: EURLEX57K/dataset/dev/31999D0495.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0495.json inflating: EURLEX57K/dataset/dev/31996R2514.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2514.json inflating: EURLEX57K/dataset/dev/31992R3838.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3838.json inflating: EURLEX57K/dataset/dev/32000R2533.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2533.json inflating: EURLEX57K/dataset/dev/31986R0582.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0582.json inflating: EURLEX57K/dataset/dev/32012R0368.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0368.json inflating: EURLEX57K/dataset/dev/32012D0383.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0383.json inflating: EURLEX57K/dataset/dev/32004R0360.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0360.json inflating: EURLEX57K/dataset/dev/32002R2254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2254.json inflating: EURLEX57K/dataset/dev/32000R2499.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2499.json inflating: EURLEX57K/dataset/dev/31993R0025.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0025.json inflating: EURLEX57K/dataset/dev/32014R0631.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0631.json inflating: EURLEX57K/dataset/dev/31986R2803.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2803.json inflating: EURLEX57K/dataset/dev/31999D0753.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0753.json inflating: EURLEX57K/dataset/dev/31988R0471.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0471.json inflating: EURLEX57K/dataset/dev/31992R1413.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1413.json inflating: EURLEX57K/dataset/dev/31996D0487.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0487.json inflating: EURLEX57K/dataset/dev/32005D0006.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0006.json inflating: EURLEX57K/dataset/dev/32011R0278.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0278.json inflating: EURLEX57K/dataset/dev/31992R1940.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1940.json inflating: EURLEX57K/dataset/dev/31998R1361.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1361.json inflating: EURLEX57K/dataset/dev/31995D0128.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0128.json inflating: EURLEX57K/dataset/dev/32006R0112.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0112.json inflating: EURLEX57K/dataset/dev/31988R1260.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1260.json inflating: EURLEX57K/dataset/dev/32001R2344.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2344.json inflating: EURLEX57K/dataset/dev/32001R1991.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1991.json inflating: EURLEX57K/dataset/dev/32010R0264.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0264.json inflating: EURLEX57K/dataset/dev/31978L0765.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978L0765.json inflating: EURLEX57K/dataset/dev/32000R2358.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2358.json inflating: EURLEX57K/dataset/dev/32011D0047.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0047.json inflating: EURLEX57K/dataset/dev/32004D0919.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0919.json inflating: EURLEX57K/dataset/dev/31994R3209.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R3209.json inflating: EURLEX57K/dataset/dev/32009R0012.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0012.json inflating: EURLEX57K/dataset/dev/32008D0661.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0661.json inflating: EURLEX57K/dataset/dev/31979D0044.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979D0044.json inflating: EURLEX57K/dataset/dev/32012R0915.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0915.json inflating: EURLEX57K/dataset/dev/31991R1387.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1387.json inflating: EURLEX57K/dataset/dev/32009D0446.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0446.json inflating: EURLEX57K/dataset/dev/31991R3640.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3640.json inflating: EURLEX57K/dataset/dev/32003R1464.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1464.json inflating: EURLEX57K/dataset/dev/32000R1431.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1431.json inflating: EURLEX57K/dataset/dev/32005R1097.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1097.json inflating: EURLEX57K/dataset/dev/31982D0429.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0429.json inflating: EURLEX57K/dataset/dev/32006D0268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0268.json inflating: EURLEX57K/dataset/dev/31986R3879.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3879.json inflating: EURLEX57K/dataset/dev/31991D0087.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0087.json inflating: EURLEX57K/dataset/dev/32009R0854.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0854.json inflating: EURLEX57K/dataset/dev/32001R2485.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2485.json inflating: EURLEX57K/dataset/dev/31992D0528.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0528.json inflating: EURLEX57K/dataset/dev/31979R2211.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R2211.json inflating: EURLEX57K/dataset/dev/31996L0093.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996L0093.json inflating: EURLEX57K/dataset/dev/31993R1663.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1663.json inflating: EURLEX57K/dataset/dev/31999R0350.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0350.json inflating: EURLEX57K/dataset/dev/32001R1180.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1180.json inflating: EURLEX57K/dataset/dev/31998R0032.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0032.json inflating: EURLEX57K/dataset/dev/32004R1526.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1526.json inflating: EURLEX57K/dataset/dev/32007R1573.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1573.json inflating: EURLEX57K/dataset/dev/32009R1195.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1195.json inflating: EURLEX57K/dataset/dev/32007R0698.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0698.json inflating: EURLEX57K/dataset/dev/31982R1953.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982R1953.json inflating: EURLEX57K/dataset/dev/32012D0706(02).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0706(02).json inflating: EURLEX57K/dataset/dev/31985R2047.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2047.json inflating: EURLEX57K/dataset/dev/31988R0830.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0830.json inflating: EURLEX57K/dataset/dev/32014D0727.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0727.json inflating: EURLEX57K/dataset/dev/32002R1993.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1993.json inflating: EURLEX57K/dataset/dev/32003R0019.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0019.json inflating: EURLEX57K/dataset/dev/31992R0605.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0605.json inflating: EURLEX57K/dataset/dev/32014R0373.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0373.json inflating: EURLEX57K/dataset/dev/32005R0540.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0540.json inflating: EURLEX57K/dataset/dev/32008R0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0059.json inflating: EURLEX57K/dataset/dev/32006R0846.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0846.json inflating: EURLEX57K/dataset/dev/31985L0578.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985L0578.json inflating: EURLEX57K/dataset/dev/32014R1076.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1076.json inflating: EURLEX57K/dataset/dev/32007R1088.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1088.json inflating: EURLEX57K/dataset/dev/31995R0879.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R0879.json inflating: EURLEX57K/dataset/dev/31993R1232.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1232.json inflating: EURLEX57K/dataset/dev/32004R1177.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1177.json inflating: EURLEX57K/dataset/dev/32007R0699.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0699.json inflating: EURLEX57K/dataset/dev/32007R0363.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0363.json inflating: EURLEX57K/dataset/dev/32001R1882.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1882.json inflating: EURLEX57K/dataset/dev/31996D0594.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0594.json inflating: EURLEX57K/dataset/dev/31977D0270.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977D0270.json inflating: EURLEX57K/dataset/dev/32000D0049.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0049.json inflating: EURLEX57K/dataset/dev/31983D0176.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0176.json inflating: EURLEX57K/dataset/dev/31988R1236.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1236.json inflating: EURLEX57K/dataset/dev/31992R2793.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2793.json inflating: EURLEX57K/dataset/dev/31996R2407.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2407.json inflating: EURLEX57K/dataset/dev/32004D0332.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0332.json inflating: EURLEX57K/dataset/dev/31993D0077.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0077.json inflating: EURLEX57K/dataset/dev/32013R1358.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1358.json inflating: EURLEX57K/dataset/dev/32014R1099.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1099.json inflating: EURLEX57K/dataset/dev/31986R0868.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0868.json inflating: EURLEX57K/dataset/dev/32003R0761.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0761.json inflating: EURLEX57K/dataset/dev/32010R0265.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0265.json inflating: EURLEX57K/dataset/dev/32000R2359.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2359.json inflating: EURLEX57K/dataset/dev/32015D0278.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0278.json inflating: EURLEX57K/dataset/dev/31981D0492.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0492.json inflating: EURLEX57K/dataset/dev/32013D0371.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0371.json inflating: EURLEX57K/dataset/dev/32010R0635.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0635.json inflating: EURLEX57K/dataset/dev/32012R0851.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0851.json inflating: EURLEX57K/dataset/dev/32001L0011.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001L0011.json inflating: EURLEX57K/dataset/dev/31998R2262.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2262.json inflating: EURLEX57K/dataset/dev/31995R1507.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1507.json inflating: EURLEX57K/dataset/dev/32006R1186.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1186.json inflating: EURLEX57K/dataset/dev/31987R1674.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1674.json inflating: EURLEX57K/dataset/dev/32008D0725.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0725.json inflating: EURLEX57K/dataset/dev/31989D0038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0038.json inflating: EURLEX57K/dataset/dev/31994D0070.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0070.json inflating: EURLEX57K/dataset/dev/32006D0024(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0024(01).json inflating: EURLEX57K/dataset/dev/32004R2022.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2022.json inflating: EURLEX57K/dataset/dev/31980D0461.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980D0461.json inflating: EURLEX57K/dataset/dev/32004R1823.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1823.json inflating: EURLEX57K/dataset/dev/32000R2532.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2532.json inflating: EURLEX57K/dataset/dev/31987R2326.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2326.json inflating: EURLEX57K/dataset/dev/31990D0130.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0130.json inflating: EURLEX57K/dataset/dev/31994R2622.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2622.json inflating: EURLEX57K/dataset/dev/32014R0630.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0630.json inflating: EURLEX57K/dataset/dev/31990D0560.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0560.json inflating: EURLEX57K/dataset/dev/32008R0849.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0849.json inflating: EURLEX57K/dataset/dev/31986L0594.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986L0594.json inflating: EURLEX57K/dataset/dev/31992R3090.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3090.json inflating: EURLEX57K/dataset/dev/31993R1320.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1320.json inflating: EURLEX57K/dataset/dev/31999D0302.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0302.json inflating: EURLEX57K/dataset/dev/32002R1496.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1496.json inflating: EURLEX57K/dataset/dev/31998D0060.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0060.json inflating: EURLEX57K/dataset/dev/32005R1757.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1757.json inflating: EURLEX57K/dataset/dev/31998R0988.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0988.json inflating: EURLEX57K/dataset/dev/31991D0317.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0317.json inflating: EURLEX57K/dataset/dev/31987R2263.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2263.json inflating: EURLEX57K/dataset/dev/32015R0047.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0047.json inflating: EURLEX57K/dataset/dev/31982R0752.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982R0752.json inflating: EURLEX57K/dataset/dev/31999R1452.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1452.json inflating: EURLEX57K/dataset/dev/31985D0028.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0028.json inflating: EURLEX57K/dataset/dev/31986R0096.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0096.json inflating: EURLEX57K/dataset/dev/31988R4063.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R4063.json inflating: EURLEX57K/dataset/dev/32011R0629.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0629.json inflating: EURLEX57K/dataset/dev/31993R3464.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3464.json inflating: EURLEX57K/dataset/dev/31998R2235.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2235.json inflating: EURLEX57K/dataset/dev/31995R1550.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1550.json inflating: EURLEX57K/dataset/dev/31994D0861.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0861.json inflating: EURLEX57K/dataset/dev/31989D0080.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0080.json inflating: EURLEX57K/dataset/dev/31997D0834.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0834.json inflating: EURLEX57K/dataset/dev/31984R1733.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1733.json inflating: EURLEX57K/dataset/dev/32006R1581.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1581.json inflating: EURLEX57K/dataset/dev/31984R0088.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0088.json inflating: EURLEX57K/dataset/dev/32013R0637.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0637.json inflating: EURLEX57K/dataset/dev/31997D0567.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0567.json inflating: EURLEX57K/dataset/dev/32004R2130.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2130.json inflating: EURLEX57K/dataset/dev/31971D0142.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31971D0142.json inflating: EURLEX57K/dataset/dev/32009D0806.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0806.json inflating: EURLEX57K/dataset/dev/32009R0802.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0802.json inflating: EURLEX57K/dataset/dev/32010R0727.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0727.json inflating: EURLEX57K/dataset/dev/32010D0236.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0236.json inflating: EURLEX57K/dataset/dev/31987D0523.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0523.json inflating: EURLEX57K/dataset/dev/31995D0345.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0345.json inflating: EURLEX57K/dataset/dev/32003R1961.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1961.json inflating: EURLEX57K/dataset/dev/32009D0410.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0410.json inflating: EURLEX57K/dataset/dev/31990R3574.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3574.json inflating: EURLEX57K/dataset/dev/31992R3213.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3213.json inflating: EURLEX57K/dataset/dev/31991R3246.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3246.json inflating: EURLEX57K/dataset/dev/32003R1062.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1062.json inflating: EURLEX57K/dataset/dev/31984R0964.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0964.json inflating: EURLEX57K/dataset/dev/32008D0322.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0322.json inflating: EURLEX57K/dataset/dev/32002R1300.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1300.json inflating: EURLEX57K/dataset/dev/31999D0601.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0601.json inflating: EURLEX57K/dataset/dev/31998D0099.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0099.json inflating: EURLEX57K/dataset/dev/31995R2805.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2805.json inflating: EURLEX57K/dataset/dev/31994R1018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1018.json inflating: EURLEX57K/dataset/dev/32012R1094.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1094.json inflating: EURLEX57K/dataset/dev/31983R1722.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983R1722.json inflating: EURLEX57K/dataset/dev/31981R2669.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2669.json inflating: EURLEX57K/dataset/dev/31994R2264.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2264.json inflating: EURLEX57K/dataset/dev/32002R2243.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2243.json inflating: EURLEX57K/dataset/dev/31996R2153.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2153.json inflating: EURLEX57K/dataset/dev/32002R0784.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0784.json inflating: EURLEX57K/dataset/dev/31991D0614.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0614.json inflating: EURLEX57K/dataset/dev/31978L0549.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978L0549.json inflating: EURLEX57K/dataset/dev/31994R0609.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0609.json inflating: EURLEX57K/dataset/dev/32005D0154.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0154.json inflating: EURLEX57K/dataset/dev/32015R0114.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0114.json inflating: EURLEX57K/dataset/dev/32014D0788.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0788.json inflating: EURLEX57K/dataset/dev/32003R2064.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2064.json inflating: EURLEX57K/dataset/dev/32002R2306.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2306.json inflating: EURLEX57K/dataset/dev/32014D0272.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0272.json inflating: EURLEX57K/dataset/dev/32015R0051.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0051.json inflating: EURLEX57K/dataset/dev/31992D0354.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0354.json inflating: EURLEX57K/dataset/dev/32008R1258.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1258.json inflating: EURLEX57K/dataset/dev/32005R0500.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0500.json inflating: EURLEX57K/dataset/dev/32004D0689.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0689.json inflating: EURLEX57K/dataset/dev/32008D0029(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0029(01).json inflating: EURLEX57K/dataset/dev/31997D0208.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0208.json inflating: EURLEX57K/dataset/dev/32001R1085.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1085.json inflating: EURLEX57K/dataset/dev/31996R0928.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0928.json inflating: EURLEX57K/dataset/dev/32007R1476.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1476.json inflating: EURLEX57K/dataset/dev/31993R2849.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2849.json inflating: EURLEX57K/dataset/dev/32002R1480.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1480.json inflating: EURLEX57K/dataset/dev/32006R0386.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0386.json inflating: EURLEX57K/dataset/dev/32012D0402.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0402.json inflating: EURLEX57K/dataset/dev/31984R0134.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0134.json inflating: EURLEX57K/dataset/dev/31997D0121.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0121.json inflating: EURLEX57K/dataset/dev/31991D0182.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0182.json inflating: EURLEX57K/dataset/dev/31994D0524.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0524.json inflating: EURLEX57K/dataset/dev/32003R1131.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1131.json inflating: EURLEX57K/dataset/dev/31991R1328.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1328.json inflating: EURLEX57K/dataset/dev/32005R1038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1038.json inflating: EURLEX57K/dataset/dev/31995R1546.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1546.json inflating: EURLEX57K/dataset/dev/32005R1192.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1192.json inflating: EURLEX57K/dataset/dev/31994R0936.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0936.json inflating: EURLEX57K/dataset/dev/31979D0511.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979D0511.json inflating: EURLEX57K/dataset/dev/31988R2267.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R2267.json inflating: EURLEX57K/dataset/dev/32003R1074.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1074.json inflating: EURLEX57K/dataset/dev/31987R4067.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R4067.json inflating: EURLEX57K/dataset/dev/32002R0854.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0854.json inflating: EURLEX57K/dataset/dev/32013R1175.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1175.json inflating: EURLEX57K/dataset/dev/31989R2555.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2555.json inflating: EURLEX57K/dataset/dev/32009L0100.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009L0100.json inflating: EURLEX57K/dataset/dev/32003R0665.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0665.json inflating: EURLEX57K/dataset/dev/31989R3714.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R3714.json inflating: EURLEX57K/dataset/dev/31985R0743.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0743.json inflating: EURLEX57K/dataset/dev/32012D0117.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0117.json inflating: EURLEX57K/dataset/dev/31984D0130.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0130.json inflating: EURLEX57K/dataset/dev/32012R0406.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0406.json inflating: EURLEX57K/dataset/dev/31988R4219.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R4219.json inflating: EURLEX57K/dataset/dev/31998R1264.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1264.json inflating: EURLEX57K/dataset/dev/31985R0597.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0597.json inflating: EURLEX57K/dataset/dev/31992D0616.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0616.json inflating: EURLEX57K/dataset/dev/32002R2214.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2214.json inflating: EURLEX57K/dataset/dev/31982R1944.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982R1944.json inflating: EURLEX57K/dataset/dev/31999R1556.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1556.json inflating: EURLEX57K/dataset/dev/32007R1134.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1134.json inflating: EURLEX57K/dataset/dev/32009R0269.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0269.json inflating: EURLEX57K/dataset/dev/32010R0859.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0859.json inflating: EURLEX57K/dataset/dev/31998D0021.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0021.json inflating: EURLEX57K/dataset/dev/32004R1474.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1474.json inflating: EURLEX57K/dataset/dev/32011R1079.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1079.json inflating: EURLEX57K/dataset/dev/32002R1087.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1087.json inflating: EURLEX57K/dataset/dev/32007R1421.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1421.json inflating: EURLEX57K/dataset/dev/31991R2380.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R2380.json inflating: EURLEX57K/dataset/dev/32012R0797.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0797.json inflating: EURLEX57K/dataset/dev/31986R0487.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0487.json inflating: EURLEX57K/dataset/dev/31983D0530.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0530.json inflating: EURLEX57K/dataset/dev/32005R2090.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R2090.json inflating: EURLEX57K/dataset/dev/31985D0069.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0069.json inflating: EURLEX57K/dataset/dev/32005D0416.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0416.json inflating: EURLEX57K/dataset/dev/32014D0225.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0225.json inflating: EURLEX57K/dataset/dev/31995R2014.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2014.json inflating: EURLEX57K/dataset/dev/31997D0102(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0102(01).json inflating: EURLEX57K/dataset/dev/32006R1190.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1190.json inflating: EURLEX57K/dataset/dev/32001D0852.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0852.json inflating: EURLEX57K/dataset/dev/31986R3478.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3478.json inflating: EURLEX57K/dataset/dev/31996R1114.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1114.json inflating: EURLEX57K/dataset/dev/31993R0209.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0209.json inflating: EURLEX57K/dataset/dev/31993D0718.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0718.json inflating: EURLEX57K/dataset/dev/32002R0045.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0045.json inflating: EURLEX57K/dataset/dev/32006D0290.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0290.json inflating: EURLEX57K/dataset/dev/32007R0119.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0119.json inflating: EURLEX57K/dataset/dev/32002R0415.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0415.json inflating: EURLEX57K/dataset/dev/32007D0058.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0058.json inflating: EURLEX57K/dataset/dev/32011D0400.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0400.json inflating: EURLEX57K/dataset/dev/32001R2492.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2492.json inflating: EURLEX57K/dataset/dev/31991D0090.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0090.json inflating: EURLEX57K/dataset/dev/32003D0323.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0323.json inflating: EURLEX57K/dataset/dev/31996R1952.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1952.json inflating: EURLEX57K/dataset/dev/32004D0148.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0148.json inflating: EURLEX57K/dataset/dev/31989R2851.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2851.json inflating: EURLEX57K/dataset/dev/32010R0336.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0336.json inflating: EURLEX57K/dataset/dev/31990R2774.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2774.json inflating: EURLEX57K/dataset/dev/31998R2331.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2331.json inflating: EURLEX57K/dataset/dev/31981D0938.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0938.json inflating: EURLEX57K/dataset/dev/31991R3207.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3207.json inflating: EURLEX57K/dataset/dev/31993R3130.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3130.json inflating: EURLEX57K/dataset/dev/31990R3372.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3372.json inflating: EURLEX57K/dataset/dev/31986R1347.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1347.json inflating: EURLEX57K/dataset/dev/32001R1553.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1553.json inflating: EURLEX57K/dataset/dev/32011D0621(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0621(01).json inflating: EURLEX57K/dataset/dev/32003R1664.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1664.json inflating: EURLEX57K/dataset/dev/32010D0429(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0429(01).json inflating: EURLEX57K/dataset/dev/31989R0728.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R0728.json inflating: EURLEX57K/dataset/dev/32005R1297.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1297.json inflating: EURLEX57K/dataset/dev/32010R0988.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0988.json inflating: EURLEX57K/dataset/dev/32003R0425.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0425.json inflating: EURLEX57K/dataset/dev/32005R0486.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0486.json inflating: EURLEX57K/dataset/dev/31975R2113.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31975R2113.json inflating: EURLEX57K/dataset/dev/32000R0020.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R0020.json inflating: EURLEX57K/dataset/dev/31991R0329.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0329.json inflating: EURLEX57K/dataset/dev/32010R0434.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0434.json inflating: EURLEX57K/dataset/dev/31996R2485.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2485.json inflating: EURLEX57K/dataset/dev/32006R1387.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1387.json inflating: EURLEX57K/dataset/dev/32013R1270.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1270.json inflating: EURLEX57K/dataset/dev/31990R2975.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2975.json inflating: EURLEX57K/dataset/dev/32004R0072.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0072.json inflating: EURLEX57K/dataset/dev/31992D0544.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0544.json inflating: EURLEX57K/dataset/dev/31978R3077.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978R3077.json inflating: EURLEX57K/dataset/dev/32006R1807.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1807.json inflating: EURLEX57K/dataset/dev/32014D0462.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0462.json inflating: EURLEX57K/dataset/dev/31997D0048.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0048.json inflating: EURLEX57K/dataset/dev/32001R2543.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2543.json inflating: EURLEX57K/dataset/dev/31989D0055.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0055.json inflating: EURLEX57K/dataset/dev/32002R1290.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1290.json inflating: EURLEX57K/dataset/dev/31977R1664.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977R1664.json inflating: EURLEX57K/dataset/dev/31985L0328.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985L0328.json inflating: EURLEX57K/dataset/dev/31993R1526.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1526.json inflating: EURLEX57K/dataset/dev/31995R1585.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1585.json inflating: EURLEX57K/dataset/dev/32005R1151.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1151.json inflating: EURLEX57K/dataset/dev/31990L0660.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990L0660.json inflating: EURLEX57K/dataset/dev/32006R1041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1041.json inflating: EURLEX57K/dataset/dev/32002R2003.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2003.json inflating: EURLEX57K/dataset/dev/32001R0591.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0591.json inflating: EURLEX57K/dataset/dev/31991D0454.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0454.json inflating: EURLEX57K/dataset/dev/31993R1930.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1930.json inflating: EURLEX57K/dataset/dev/31979R1543.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R1543.json inflating: EURLEX57K/dataset/dev/31990R2164.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2164.json inflating: EURLEX57K/dataset/dev/31999R1595.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1595.json inflating: EURLEX57K/dataset/dev/31996D0111.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0111.json inflating: EURLEX57K/dataset/dev/32000D0136.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0136.json inflating: EURLEX57K/dataset/dev/31975D0038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31975D0038.json inflating: EURLEX57K/dataset/dev/31993R2561.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2561.json inflating: EURLEX57K/dataset/dev/32003R1633.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1633.json inflating: EURLEX57K/dataset/dev/32003R0921.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0921.json inflating: EURLEX57K/dataset/dev/31987R1588.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1588.json inflating: EURLEX57K/dataset/dev/31994D1067.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D1067.json inflating: EURLEX57K/dataset/dev/31994R1126.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1126.json inflating: EURLEX57K/dataset/dev/31997R1036.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1036.json inflating: EURLEX57K/dataset/dev/31979R2380.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R2380.json inflating: EURLEX57K/dataset/dev/32000R1373.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1373.json inflating: EURLEX57K/dataset/dev/31985D0100.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0100.json inflating: EURLEX57K/dataset/dev/32002D0714.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0714.json inflating: EURLEX57K/dataset/dev/31984R0289.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0289.json inflating: EURLEX57K/dataset/dev/32000D0423.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0423.json inflating: EURLEX57K/dataset/dev/31999R1080.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1080.json inflating: EURLEX57K/dataset/dev/32005D0085.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0085.json inflating: EURLEX57K/dataset/dev/31986R2029.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2029.json inflating: EURLEX57K/dataset/dev/32001R0600.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0600.json inflating: EURLEX57K/dataset/dev/31994D0699.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0699.json inflating: EURLEX57K/dataset/dev/31992R1869.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1869.json inflating: EURLEX57K/dataset/dev/32011R0351.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0351.json inflating: EURLEX57K/dataset/dev/31999D0103.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0103.json inflating: EURLEX57K/dataset/dev/32004L0032.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004L0032.json inflating: EURLEX57K/dataset/dev/31984R0909.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0909.json inflating: EURLEX57K/dataset/dev/32007R0973.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0973.json inflating: EURLEX57K/dataset/dev/31994R2566.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2566.json inflating: EURLEX57K/dataset/dev/31990D0274.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0274.json inflating: EURLEX57K/dataset/dev/32014D0435.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0435.json inflating: EURLEX57K/dataset/dev/31996R0396.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0396.json inflating: EURLEX57K/dataset/dev/32005D0206.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0206.json inflating: EURLEX57K/dataset/dev/32007D0131.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0131.json inflating: EURLEX57K/dataset/dev/31993D0671.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0671.json inflating: EURLEX57K/dataset/dev/32005R0347.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0347.json inflating: EURLEX57K/dataset/dev/31983D0320.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0320.json inflating: EURLEX57K/dataset/dev/31992R0517.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0517.json inflating: EURLEX57K/dataset/dev/32004R0530.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0530.json inflating: EURLEX57K/dataset/dev/31972R2456.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31972R2456.json inflating: EURLEX57K/dataset/dev/31985R2210.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2210.json inflating: EURLEX57K/dataset/dev/31991R2085.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R2085.json inflating: EURLEX57K/dataset/dev/31990D0331.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0331.json inflating: EURLEX57K/dataset/dev/32005R1940.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1940.json inflating: EURLEX57K/dataset/dev/32004R0160.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0160.json inflating: EURLEX57K/dataset/dev/32007R1374.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1374.json inflating: EURLEX57K/dataset/dev/32009R0183.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0183.json inflating: EURLEX57K/dataset/dev/31997D0859.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0859.json inflating: EURLEX57K/dataset/dev/31969R2517.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31969R2517.json inflating: EURLEX57K/dataset/dev/32001R1142.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1142.json inflating: EURLEX57K/dataset/dev/32009R0746.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0746.json inflating: EURLEX57K/dataset/dev/32006R1779.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1779.json inflating: EURLEX57K/dataset/dev/31996R1657.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1657.json inflating: EURLEX57K/dataset/dev/31989R0769.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R0769.json inflating: EURLEX57K/dataset/dev/32013D0424.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0424.json inflating: EURLEX57K/dataset/dev/32010R0160.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0160.json inflating: EURLEX57K/dataset/dev/32006D0479.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0479.json inflating: EURLEX57K/dataset/dev/32003D0525.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0525.json inflating: EURLEX57K/dataset/dev/32003R0034.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0034.json inflating: EURLEX57K/dataset/dev/31994R1999.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1999.json inflating: EURLEX57K/dataset/dev/32009R1012.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1012.json inflating: EURLEX57K/dataset/dev/32001R1904.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1904.json inflating: EURLEX57K/dataset/dev/31999R1096.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1096.json inflating: EURLEX57K/dataset/dev/32005D0093.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0093.json inflating: EURLEX57K/dataset/dev/32005R0582.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0582.json inflating: EURLEX57K/dataset/dev/31995D0447.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0447.json inflating: EURLEX57K/dataset/dev/32006R2010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R2010.json inflating: EURLEX57K/dataset/dev/32000R1365.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1365.json inflating: EURLEX57K/dataset/dev/32008D0070.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0070.json inflating: EURLEX57K/dataset/dev/32005R1239.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1239.json inflating: EURLEX57K/dataset/dev/32009R0879.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0879.json inflating: EURLEX57K/dataset/dev/32004D0522.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0522.json inflating: EURLEX57K/dataset/dev/31977D0525.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977D0525.json inflating: EURLEX57K/dataset/dev/32005R1813.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1813.json inflating: EURLEX57K/dataset/dev/32001R2502.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2502.json inflating: EURLEX57K/dataset/dev/32014R0132.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0132.json inflating: EURLEX57K/dataset/dev/32014D0423.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0423.json inflating: EURLEX57K/dataset/dev/31988R3948.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R3948.json inflating: EURLEX57K/dataset/dev/31993R1567.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1567.json inflating: EURLEX57K/dataset/dev/32002R0993.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0993.json inflating: EURLEX57K/dataset/dev/31998D0277.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0277.json inflating: EURLEX57K/dataset/dev/32004R1788.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1788.json inflating: EURLEX57K/dataset/dev/32008R0648.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0648.json inflating: EURLEX57K/dataset/dev/32004R1767.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1767.json inflating: EURLEX57K/dataset/dev/32005R1055.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1055.json inflating: EURLEX57K/dataset/dev/31994R3224.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R3224.json inflating: EURLEX57K/dataset/dev/31998R1198.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1198.json inflating: EURLEX57K/dataset/dev/32010D0758.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0758.json inflating: EURLEX57K/dataset/dev/32007R0089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0089.json inflating: EURLEX57K/dataset/dev/32007D0598.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0598.json inflating: EURLEX57K/dataset/dev/32011R0605.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0605.json inflating: EURLEX57K/dataset/dev/32003R0063.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0063.json inflating: EURLEX57K/dataset/dev/32008R0920.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0920.json inflating: EURLEX57K/dataset/dev/32001D0645.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0645.json inflating: EURLEX57K/dataset/dev/32001R2369.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2369.json inflating: EURLEX57K/dataset/dev/32011R0255.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0255.json inflating: EURLEX57K/dataset/dev/32006D0584.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0584.json inflating: EURLEX57K/dataset/dev/31995D0105.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0105.json inflating: EURLEX57K/dataset/dev/32012D0711.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0711.json inflating: EURLEX57K/dataset/dev/32010R0137.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0137.json inflating: EURLEX57K/dataset/dev/32006L0082.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006L0082.json inflating: EURLEX57K/dataset/dev/32012R1041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1041.json inflating: EURLEX57K/dataset/dev/32003R1388.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1388.json inflating: EURLEX57K/dataset/dev/31989R2353.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2353.json inflating: EURLEX57K/dataset/dev/32000L0021.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000L0021.json inflating: EURLEX57K/dataset/dev/31987R1463.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1463.json inflating: EURLEX57K/dataset/dev/31998R2560.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2560.json inflating: EURLEX57K/dataset/dev/32009R0711.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0711.json inflating: EURLEX57K/dataset/dev/31985R2894.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2894.json inflating: EURLEX57K/dataset/dev/32007R1159.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1159.json inflating: EURLEX57K/dataset/dev/31985R0450.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0450.json inflating: EURLEX57K/dataset/dev/32002R2279.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2279.json inflating: EURLEX57K/dataset/dev/32007R0748.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0748.json inflating: EURLEX57K/dataset/dev/31984R3667.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R3667.json inflating: EURLEX57K/dataset/dev/32007R1270.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1270.json inflating: EURLEX57K/dataset/dev/31998R0731.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0731.json inflating: EURLEX57K/dataset/dev/31988R1922.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1922.json inflating: EURLEX57K/dataset/dev/31988R1471.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1471.json inflating: EURLEX57K/dataset/dev/31969R1395.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31969R1395.json inflating: EURLEX57K/dataset/dev/31998D1022(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D1022(01).json inflating: EURLEX57K/dataset/dev/31995D0369.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0369.json inflating: EURLEX57K/dataset/dev/31983R0220.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983R0220.json inflating: EURLEX57K/dataset/dev/31993D0630.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0630.json inflating: EURLEX57K/dataset/dev/31985R0283.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0283.json inflating: EURLEX57K/dataset/dev/31980D0334.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980D0334.json inflating: EURLEX57K/dataset/dev/32002R0182.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0182.json inflating: EURLEX57K/dataset/dev/32007D0035.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0035.json inflating: EURLEX57K/dataset/dev/32001R2410.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2410.json inflating: EURLEX57K/dataset/dev/32014D0531.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0531.json inflating: EURLEX57K/dataset/dev/32012D0068.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0068.json inflating: EURLEX57K/dataset/dev/31999D0007.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0007.json inflating: EURLEX57K/dataset/dev/32006R1057.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1057.json inflating: EURLEX57K/dataset/dev/31991R1742.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1742.json inflating: EURLEX57K/dataset/dev/31988R0775.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0775.json inflating: EURLEX57K/dataset/dev/31999R0131.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0131.json inflating: EURLEX57K/dataset/dev/31988D0213.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0213.json inflating: EURLEX57K/dataset/dev/31994R2946.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2946.json inflating: EURLEX57K/dataset/dev/31976R1422.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976R1422.json inflating: EURLEX57K/dataset/dev/31999R0561.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0561.json inflating: EURLEX57K/dataset/dev/31975L0271.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31975L0271.json inflating: EURLEX57K/dataset/dev/31984D0038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0038.json inflating: EURLEX57K/dataset/dev/32013R0396.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0396.json inflating: EURLEX57K/dataset/dev/31994D0093.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0093.json inflating: EURLEX57K/dataset/dev/32013D0687.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0687.json inflating: EURLEX57K/dataset/dev/32001R2467.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2467.json inflating: EURLEX57K/dataset/dev/31988R3084.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R3084.json inflating: EURLEX57K/dataset/dev/31998R1012.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1012.json inflating: EURLEX57K/dataset/dev/32004R0013.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0013.json inflating: EURLEX57K/dataset/dev/31992D0525.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0525.json inflating: EURLEX57K/dataset/dev/32013D0392.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0392.json inflating: EURLEX57K/dataset/dev/32009D0918.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0918.json inflating: EURLEX57K/dataset/dev/32005R0721.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0721.json inflating: EURLEX57K/dataset/dev/32013R0379.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0379.json inflating: EURLEX57K/dataset/dev/31990R0753.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R0753.json inflating: EURLEX57K/dataset/dev/32011L0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011L0059.json inflating: EURLEX57K/dataset/dev/31995R3023.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R3023.json inflating: EURLEX57K/dataset/dev/31989D0464.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0464.json inflating: EURLEX57K/dataset/dev/31999D0135.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0135.json inflating: EURLEX57K/dataset/dev/32006R1165.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1165.json inflating: EURLEX57K/dataset/dev/32012R0332.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0332.json inflating: EURLEX57K/dataset/dev/31994R2383.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2383.json inflating: EURLEX57K/dataset/dev/32005R0408.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0408.json inflating: EURLEX57K/dataset/dev/31988R0997.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0997.json inflating: EURLEX57K/dataset/dev/32012R0762.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0762.json inflating: EURLEX57K/dataset/dev/32005R2065.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R2065.json inflating: EURLEX57K/dataset/dev/32013R0953.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0953.json inflating: EURLEX57K/dataset/dev/31974R1579.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31974R1579.json inflating: EURLEX57K/dataset/dev/32010R0843.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0843.json inflating: EURLEX57K/dataset/dev/32003R1605.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1605.json inflating: EURLEX57K/dataset/dev/32006D0009.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0009.json inflating: EURLEX57K/dataset/dev/31994D0640.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0640.json inflating: EURLEX57K/dataset/dev/32004R1897.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1897.json inflating: EURLEX57K/dataset/dev/32011D0699.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0699.json inflating: EURLEX57K/dataset/dev/31992R0258.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0258.json inflating: EURLEX57K/dataset/dev/32003D0505.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0505.json inflating: EURLEX57K/dataset/dev/32009R1209.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1209.json inflating: EURLEX57K/dataset/dev/32012D0048.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0048.json inflating: EURLEX57K/dataset/dev/32007D0445.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0445.json inflating: EURLEX57K/dataset/dev/32001R2060.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2060.json inflating: EURLEX57K/dataset/dev/32009R0418.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0418.json inflating: EURLEX57K/dataset/dev/32013R1180.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1180.json inflating: EURLEX57K/dataset/dev/31971L0018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31971L0018.json inflating: EURLEX57K/dataset/dev/32009D0559.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0559.json inflating: EURLEX57K/dataset/dev/32009R0048.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0048.json inflating: EURLEX57K/dataset/dev/32014R1241.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1241.json inflating: EURLEX57K/dataset/dev/31988D0244.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0244.json inflating: EURLEX57K/dataset/dev/31999D0162.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0162.json inflating: EURLEX57K/dataset/dev/32008R0795.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0795.json inflating: EURLEX57K/dataset/dev/31998D0650.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0650.json inflating: EURLEX57K/dataset/dev/31994R2507.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2507.json inflating: EURLEX57K/dataset/dev/31987D0485.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0485.json inflating: EURLEX57K/dataset/dev/32006D0662.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0662.json inflating: EURLEX57K/dataset/dev/32008R1184.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1184.json inflating: EURLEX57K/dataset/dev/32011D0158.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0158.json inflating: EURLEX57K/dataset/dev/31992R0063.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0063.json inflating: EURLEX57K/dataset/dev/32004R2079.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2079.json inflating: EURLEX57K/dataset/dev/32011R0449.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0449.json inflating: EURLEX57K/dataset/dev/32013L0010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013L0010.json inflating: EURLEX57K/dataset/dev/32008R0516.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0516.json inflating: EURLEX57K/dataset/dev/32002R2259.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2259.json inflating: EURLEX57K/dataset/dev/32013R0457.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0457.json inflating: EURLEX57K/dataset/dev/32002D0775.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0775.json inflating: EURLEX57K/dataset/dev/32012R0735.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0735.json inflating: EURLEX57K/dataset/dev/31990D0096.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0096.json inflating: EURLEX57K/dataset/dev/32013R0007.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0007.json inflating: EURLEX57K/dataset/dev/32003R2091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2091.json inflating: EURLEX57K/dataset/dev/31994D0302.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0302.json inflating: EURLEX57K/dataset/dev/31996R0524.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0524.json inflating: EURLEX57K/dataset/dev/32014D0268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0268.json inflating: EURLEX57K/dataset/dev/32014R0779.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0779.json inflating: EURLEX57K/dataset/dev/32001R2349.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2349.json inflating: EURLEX57K/dataset/dev/32003R0413.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0413.json inflating: EURLEX57K/dataset/dev/31991R3925.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3925.json inflating: EURLEX57K/dataset/dev/32002D0260.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0260.json inflating: EURLEX57K/dataset/dev/32002R0771.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0771.json inflating: EURLEX57K/dataset/dev/31993R2500.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2500.json inflating: EURLEX57K/dataset/dev/32004R0382.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0382.json inflating: EURLEX57K/dataset/dev/31984R3581.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R3581.json inflating: EURLEX57K/dataset/dev/31994D1006.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D1006.json inflating: EURLEX57K/dataset/dev/31991R3026.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3026.json inflating: EURLEX57K/dataset/dev/32009R0731.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0731.json inflating: EURLEX57K/dataset/dev/31995R1225.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1225.json inflating: EURLEX57K/dataset/dev/32006L0108.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006L0108.json inflating: EURLEX57K/dataset/dev/32002R1160.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1160.json inflating: EURLEX57K/dataset/dev/32001R1135.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1135.json inflating: EURLEX57K/dataset/dev/31998D0596.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0596.json inflating: EURLEX57K/dataset/dev/31988R0743.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0743.json inflating: EURLEX57K/dataset/dev/32000R1492.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1492.json inflating: EURLEX57K/dataset/dev/31981D0125.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0125.json inflating: EURLEX57K/dataset/dev/32002R2023.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2023.json inflating: EURLEX57K/dataset/dev/32014D0157.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0157.json inflating: EURLEX57K/dataset/dev/31993D0743.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0743.json inflating: EURLEX57K/dataset/dev/32000R2314.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2314.json inflating: EURLEX57K/dataset/dev/32007D0453.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0453.json inflating: EURLEX57K/dataset/dev/32007R0142.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0142.json inflating: EURLEX57K/dataset/dev/32006R0670.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0670.json inflating: EURLEX57K/dataset/dev/32013R0787.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0787.json inflating: EURLEX57K/dataset/dev/31989R1375.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1375.json inflating: EURLEX57K/dataset/dev/32004R0052.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0052.json inflating: EURLEX57K/dataset/dev/31987D0169.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0169.json inflating: EURLEX57K/dataset/dev/32004D0543.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0543.json inflating: EURLEX57K/dataset/dev/32001R2133.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2133.json inflating: EURLEX57K/dataset/dev/32003D0378.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0378.json inflating: EURLEX57K/dataset/dev/32014D0442.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0442.json inflating: EURLEX57K/dataset/dev/32005R0760.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0760.json inflating: EURLEX57K/dataset/dev/31983L0201.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983L0201.json inflating: EURLEX57K/dataset/dev/32012D0818.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0818.json inflating: EURLEX57K/dataset/dev/32008R0279.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0279.json inflating: EURLEX57K/dataset/dev/31988R1914.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1914.json inflating: EURLEX57K/dataset/dev/32004R1213.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1213.json inflating: EURLEX57K/dataset/dev/31991R3876.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3876.json inflating: EURLEX57K/dataset/dev/32012D0662.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0662.json inflating: EURLEX57K/dataset/dev/32000D0004.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0004.json inflating: EURLEX57K/dataset/dev/31988R1294.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1294.json inflating: EURLEX57K/dataset/dev/31993D0485.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0485.json inflating: EURLEX57K/dataset/dev/32010R0044.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0044.json inflating: EURLEX57K/dataset/dev/32001D0366.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0366.json inflating: EURLEX57K/dataset/dev/32008R1311.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1311.json inflating: EURLEX57K/dataset/dev/32002R0788.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0788.json inflating: EURLEX57K/dataset/dev/32011D0267.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0267.json inflating: EURLEX57K/dataset/dev/31996D0473.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0473.json inflating: EURLEX57K/dataset/dev/31997D0711.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0711.json inflating: EURLEX57K/dataset/dev/32010R0414.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0414.json inflating: EURLEX57K/dataset/dev/31986R1388.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1388.json inflating: EURLEX57K/dataset/dev/32010R0947.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0947.json inflating: EURLEX57K/dataset/dev/32009R0398.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0398.json inflating: EURLEX57K/dataset/dev/32012R1098.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1098.json inflating: EURLEX57K/dataset/dev/32009R0232.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0232.json inflating: EURLEX57K/dataset/dev/31996R1289.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1289.json inflating: EURLEX57K/dataset/dev/32001R1123.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1123.json inflating: EURLEX57K/dataset/dev/31998R2556.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2556.json inflating: EURLEX57K/dataset/dev/31982R3164.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982R3164.json inflating: EURLEX57K/dataset/dev/32008R0015.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0015.json inflating: EURLEX57K/dataset/dev/31994D1010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D1010.json inflating: EURLEX57K/dataset/dev/32009R0377.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0377.json inflating: EURLEX57K/dataset/dev/32010R0101.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0101.json inflating: EURLEX57K/dataset/dev/32008L0002.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008L0002.json inflating: EURLEX57K/dataset/dev/32008R1254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1254.json inflating: EURLEX57K/dataset/dev/31981R2370.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2370.json inflating: EURLEX57K/dataset/dev/32013R0504.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0504.json inflating: EURLEX57K/dataset/dev/32006R0109.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0109.json inflating: EURLEX57K/dataset/dev/31989R1119.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1119.json inflating: EURLEX57K/dataset/dev/32011D0322.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0322.json inflating: EURLEX57K/dataset/dev/32003R1869.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1869.json inflating: EURLEX57K/dataset/dev/32012R0148.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0148.json inflating: EURLEX57K/dataset/dev/32002R0419.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0419.json inflating: EURLEX57K/dataset/dev/31983D0245.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0245.json inflating: EURLEX57K/dataset/dev/32013R0380.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0380.json inflating: EURLEX57K/dataset/dev/32002D0558.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0558.json inflating: EURLEX57K/dataset/dev/31987D0581.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0581.json inflating: EURLEX57K/dataset/dev/32004R1301.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1301.json inflating: EURLEX57K/dataset/dev/32005R1599.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1599.json inflating: EURLEX57K/dataset/dev/32007R1354.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1354.json inflating: EURLEX57K/dataset/dev/31991R1723.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1723.json inflating: EURLEX57K/dataset/dev/32006R1466.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1466.json inflating: EURLEX57K/dataset/dev/31998R2782.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2782.json inflating: EURLEX57K/dataset/dev/32001R1318.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1318.json inflating: EURLEX57K/dataset/dev/32003R1185.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1185.json inflating: EURLEX57K/dataset/dev/32001D0018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0018.json inflating: EURLEX57K/dataset/dev/32000D0680.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0680.json inflating: EURLEX57K/dataset/dev/32001R0509.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0509.json inflating: EURLEX57K/dataset/dev/31998R1141.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1141.json inflating: EURLEX57K/dataset/dev/32011R0058.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0058.json inflating: EURLEX57K/dataset/dev/32011R0408.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0408.json inflating: EURLEX57K/dataset/dev/32010R0290.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0290.json inflating: EURLEX57K/dataset/dev/31991D0566.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0566.json inflating: EURLEX57K/dataset/dev/32010L0004.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010L0004.json inflating: EURLEX57K/dataset/dev/32009R0265.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0265.json inflating: EURLEX57K/dataset/dev/31992R3527.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3527.json inflating: EURLEX57K/dataset/dev/32005L0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005L0059.json inflating: EURLEX57K/dataset/dev/32008D0046.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0046.json inflating: EURLEX57K/dataset/dev/32009R0635.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0635.json inflating: EURLEX57K/dataset/dev/31985D0120.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0120.json inflating: EURLEX57K/dataset/dev/32007D0392.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0392.json inflating: EURLEX57K/dataset/dev/31995D0471.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0471.json inflating: EURLEX57K/dataset/dev/31988R1693.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1693.json inflating: EURLEX57K/dataset/dev/32010R0013.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0013.json inflating: EURLEX57K/dataset/dev/31997D0316.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0316.json inflating: EURLEX57K/dataset/dev/31985R0061.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0061.json inflating: EURLEX57K/dataset/dev/32002R1967.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1967.json inflating: EURLEX57K/dataset/dev/32007R0729.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0729.json inflating: EURLEX57K/dataset/dev/31988L0095.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988L0095.json inflating: EURLEX57K/dataset/dev/32003D0513.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0513.json inflating: EURLEX57K/dataset/dev/32014R0738.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0738.json inflating: EURLEX57K/dataset/dev/31985D0065.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0065.json inflating: EURLEX57K/dataset/dev/31990R2144.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2144.json inflating: EURLEX57K/dataset/dev/31976R2561.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976R2561.json inflating: EURLEX57K/dataset/dev/31993R0086.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0086.json inflating: EURLEX57K/dataset/dev/31997R1850.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1850.json inflating: EURLEX57K/dataset/dev/31985R2119.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2119.json inflating: EURLEX57K/dataset/dev/32003D0840.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0840.json inflating: EURLEX57K/dataset/dev/32001R1174.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1174.json inflating: EURLEX57K/dataset/dev/32002R1121.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1121.json inflating: EURLEX57K/dataset/dev/32006R1108.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1108.json inflating: EURLEX57K/dataset/dev/32007D0869.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0869.json inflating: EURLEX57K/dataset/dev/32011R0920.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0920.json inflating: EURLEX57K/dataset/dev/32004L0069.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004L0069.json inflating: EURLEX57K/dataset/dev/31998D0390.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0390.json inflating: EURLEX57K/dataset/dev/32013R1155.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1155.json inflating: EURLEX57K/dataset/dev/31985R1172.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1172.json inflating: EURLEX57K/dataset/dev/32009R0137.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0137.json inflating: EURLEX57K/dataset/dev/31993R1879.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1879.json inflating: EURLEX57K/dataset/dev/31986R0366.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0366.json inflating: EURLEX57K/dataset/dev/32006D0208.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0208.json inflating: EURLEX57K/dataset/dev/32009D0975.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0975.json inflating: EURLEX57K/dataset/dev/31987R2039.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2039.json inflating: EURLEX57K/dataset/dev/32001R0088.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0088.json inflating: EURLEX57K/dataset/dev/31986D0227.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0227.json inflating: EURLEX57K/dataset/dev/31992R0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0059.json inflating: EURLEX57K/dataset/dev/32003R1812.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1812.json inflating: EURLEX57K/dataset/dev/32003R0700.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0700.json inflating: EURLEX57K/dataset/dev/31994D0154.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0154.json inflating: EURLEX57K/dataset/dev/32005R0259.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0259.json inflating: EURLEX57K/dataset/dev/31994R2028.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2028.json inflating: EURLEX57K/dataset/dev/32003R1111.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1111.json inflating: EURLEX57K/dataset/dev/32008R0740.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0740.json inflating: EURLEX57K/dataset/dev/32011R1327.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1327.json inflating: EURLEX57K/dataset/dev/32004R0838.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0838.json inflating: EURLEX57K/dataset/dev/32004L0086.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004L0086.json inflating: EURLEX57K/dataset/dev/32007R1285.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1285.json inflating: EURLEX57K/dataset/dev/32004R0212.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0212.json inflating: EURLEX57K/dataset/dev/31997R0693.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0693.json inflating: EURLEX57K/dataset/dev/31997D0678.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0678.json inflating: EURLEX57K/dataset/dev/32005R0170.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0170.json inflating: EURLEX57K/dataset/dev/32004R1950.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1950.json inflating: EURLEX57K/dataset/dev/32000D0497.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0497.json inflating: EURLEX57K/dataset/dev/32004R1403.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1403.json inflating: EURLEX57K/dataset/dev/31979R0309.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R0309.json inflating: EURLEX57K/dataset/dev/31976L0160.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976L0160.json inflating: EURLEX57K/dataset/dev/31994R3140.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R3140.json inflating: EURLEX57K/dataset/dev/31999R0330.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0330.json inflating: EURLEX57K/dataset/dev/31998D0113.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0113.json inflating: EURLEX57K/dataset/dev/31997R1597.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1597.json inflating: EURLEX57K/dataset/dev/32006D0571.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0571.json inflating: EURLEX57K/dataset/dev/31991R0325.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0325.json inflating: EURLEX57K/dataset/dev/32005R0465.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0465.json inflating: EURLEX57K/dataset/dev/31970R2556.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31970R2556.json inflating: EURLEX57K/dataset/dev/31997R0152.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0152.json inflating: EURLEX57K/dataset/dev/31984R2191.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R2191.json inflating: EURLEX57K/dataset/dev/32012D0530.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0530.json inflating: EURLEX57K/dataset/dev/32001D0034.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0034.json inflating: EURLEX57K/dataset/dev/31970D0304.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31970D0304.json inflating: EURLEX57K/dataset/dev/31994D0046.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0046.json inflating: EURLEX57K/dataset/dev/32009D0471.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0471.json inflating: EURLEX57K/dataset/dev/32014R0981.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0981.json inflating: EURLEX57K/dataset/dev/31979D0423.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979D0423.json inflating: EURLEX57K/dataset/dev/31995R3019.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R3019.json inflating: EURLEX57K/dataset/dev/32011R0977.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0977.json inflating: EURLEX57K/dataset/dev/32004R1268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1268.json inflating: EURLEX57K/dataset/dev/31987R3385.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R3385.json inflating: EURLEX57K/dataset/dev/32013R1047.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1047.json inflating: EURLEX57K/dataset/dev/31990R1197.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1197.json inflating: EURLEX57K/dataset/dev/32003R1516.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1516.json inflating: EURLEX57K/dataset/dev/31992R2875.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2875.json inflating: EURLEX57K/dataset/dev/31992R0848.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0848.json inflating: EURLEX57K/dataset/dev/32002R1224.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1224.json inflating: EURLEX57K/dataset/dev/31984R1752.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1752.json inflating: EURLEX57K/dataset/dev/31978D0254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978D0254.json inflating: EURLEX57K/dataset/dev/32004D0187.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0187.json inflating: EURLEX57K/dataset/dev/31981R2188.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2188.json inflating: EURLEX57K/dataset/dev/32002R0435.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0435.json inflating: EURLEX57K/dataset/dev/32007R0569.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0569.json inflating: EURLEX57K/dataset/dev/32003D0246.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0246.json inflating: EURLEX57K/dataset/dev/32013D0718(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0718(01).json inflating: EURLEX57K/dataset/dev/31985R2836.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2836.json inflating: EURLEX57K/dataset/dev/31999D0363.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0363.json inflating: EURLEX57K/dataset/dev/32014D0008(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0008(01).json inflating: EURLEX57K/dataset/dev/31978R2451.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978R2451.json inflating: EURLEX57K/dataset/dev/31998R2487.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2487.json inflating: EURLEX57K/dataset/dev/32005R1736.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1736.json inflating: EURLEX57K/dataset/dev/31993R0803.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0803.json inflating: EURLEX57K/dataset/dev/32013R0986.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0986.json inflating: EURLEX57K/dataset/dev/31990D0014.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0014.json inflating: EURLEX57K/dataset/dev/31995R2464.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2464.json inflating: EURLEX57K/dataset/dev/31985D0049.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0049.json inflating: EURLEX57K/dataset/dev/32006R0488.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0488.json inflating: EURLEX57K/dataset/dev/32013R0085.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0085.json inflating: EURLEX57K/dataset/dev/31996D0058.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0058.json inflating: EURLEX57K/dataset/dev/32006R0467.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0467.json inflating: EURLEX57K/dataset/dev/32002D0348.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0348.json inflating: EURLEX57K/dataset/dev/32005R0432.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0432.json inflating: EURLEX57K/dataset/dev/31997R1939.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1939.json inflating: EURLEX57K/dataset/dev/31983R2529.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983R2529.json inflating: EURLEX57K/dataset/dev/32010D0484.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0484.json inflating: EURLEX57K/dataset/dev/32015R0533.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0533.json inflating: EURLEX57K/dataset/dev/31993R2582.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2582.json inflating: EURLEX57K/dataset/dev/32007R0355.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0355.json inflating: EURLEX57K/dataset/dev/31995R2171.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2171.json inflating: EURLEX57K/dataset/dev/32008R0828.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0828.json inflating: EURLEX57K/dataset/dev/31988R1345.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1345.json inflating: EURLEX57K/dataset/dev/32007R1114.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1114.json inflating: EURLEX57K/dataset/dev/32013R0969.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0969.json inflating: EURLEX57K/dataset/dev/31997R1305.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1305.json inflating: EURLEX57K/dataset/dev/32011R0961.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0961.json inflating: EURLEX57K/dataset/dev/32006D1008.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D1008.json inflating: EURLEX57K/dataset/dev/32006R1519.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1519.json inflating: EURLEX57K/dataset/dev/31999R0058.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0058.json inflating: EURLEX57K/dataset/dev/32011D0089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0089.json inflating: EURLEX57K/dataset/dev/32014R0094.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0094.json inflating: EURLEX57K/dataset/dev/31992R0018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0018.json inflating: EURLEX57K/dataset/dev/31993R1992.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1992.json inflating: EURLEX57K/dataset/dev/32003R2269.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2269.json inflating: EURLEX57K/dataset/dev/31986D0289.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0289.json inflating: EURLEX57K/dataset/dev/31994R2439.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2439.json inflating: EURLEX57K/dataset/dev/32007R0085.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0085.json inflating: EURLEX57K/dataset/dev/31997D0140.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0140.json inflating: EURLEX57K/dataset/dev/32004R2147.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2147.json inflating: EURLEX57K/dataset/dev/31981R2464.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2464.json inflating: EURLEX57K/dataset/dev/31988R1996.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1996.json inflating: EURLEX57K/dataset/dev/31986R1473.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1473.json inflating: EURLEX57K/dataset/dev/32001R1267.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1267.json inflating: EURLEX57K/dataset/dev/32005R1059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1059.json inflating: EURLEX57K/dataset/dev/31997D0843.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0843.json inflating: EURLEX57K/dataset/dev/31990R1181.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1181.json inflating: EURLEX57K/dataset/dev/31994R0957.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0957.json inflating: EURLEX57K/dataset/dev/32005R1409.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1409.json inflating: EURLEX57K/dataset/dev/32001R1637.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1637.json inflating: EURLEX57K/dataset/dev/31998R0855.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0855.json inflating: EURLEX57K/dataset/dev/32014D0213.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0213.json inflating: EURLEX57K/dataset/dev/32005R0131.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0131.json inflating: EURLEX57K/dataset/dev/31982R1837.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982R1837.json inflating: EURLEX57K/dataset/dev/32001R2332.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2332.json inflating: EURLEX57K/dataset/dev/31978D0868.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978D0868.json inflating: EURLEX57K/dataset/dev/32004R0603.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0603.json inflating: EURLEX57K/dataset/dev/32015R0030.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0030.json inflating: EURLEX57K/dataset/dev/31992R3458.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3458.json inflating: EURLEX57K/dataset/dev/32006R1775.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1775.json inflating: EURLEX57K/dataset/dev/31989D0624.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0624.json inflating: EURLEX57K/dataset/dev/31993R3690.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3690.json inflating: EURLEX57K/dataset/dev/31999R1976.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1976.json inflating: EURLEX57K/dataset/dev/32005R0977.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0977.json inflating: EURLEX57K/dataset/dev/31998D0152.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0152.json inflating: EURLEX57K/dataset/dev/32000R1369.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1369.json inflating: EURLEX57K/dataset/dev/32006R0021.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0021.json inflating: EURLEX57K/dataset/dev/32012R0430.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0430.json inflating: EURLEX57K/dataset/dev/32010R0707.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0707.json inflating: EURLEX57K/dataset/dev/32010D0216.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0216.json inflating: EURLEX57K/dataset/dev/32003R0653.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0653.json inflating: EURLEX57K/dataset/dev/32013R0302.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0302.json inflating: EURLEX57K/dataset/dev/31986D0661.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0661.json inflating: EURLEX57K/dataset/dev/32009D0430.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0430.json inflating: EURLEX57K/dataset/dev/31991R0919.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0919.json inflating: EURLEX57K/dataset/dev/32002R0862.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0862.json inflating: EURLEX57K/dataset/dev/31998D0386.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0386.json inflating: EURLEX57K/dataset/dev/32008D0302.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0302.json inflating: EURLEX57K/dataset/dev/31980R1192.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980R1192.json inflating: EURLEX57K/dataset/dev/32001R0972.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0972.json inflating: EURLEX57K/dataset/dev/31987L0140.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987L0140.json inflating: EURLEX57K/dataset/dev/31992R2834.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2834.json inflating: EURLEX57K/dataset/dev/31998R0382.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0382.json inflating: EURLEX57K/dataset/dev/31996R1175.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1175.json inflating: EURLEX57K/dataset/dev/32003R1107.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1107.json inflating: EURLEX57K/dataset/dev/31981R3388.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R3388.json inflating: EURLEX57K/dataset/dev/31990R2200.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2200.json inflating: EURLEX57K/dataset/dev/31999R0849.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0849.json inflating: EURLEX57K/dataset/dev/31992R2137.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2137.json inflating: EURLEX57K/dataset/dev/32000D0602.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0602.json inflating: EURLEX57K/dataset/dev/32007D0039.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0039.json inflating: EURLEX57K/dataset/dev/32007R1010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1010.json inflating: EURLEX57K/dataset/dev/31998D0040.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0040.json inflating: EURLEX57K/dataset/dev/31988D0141.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0141.json inflating: EURLEX57K/dataset/dev/32000R2904.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2904.json inflating: EURLEX57K/dataset/dev/32013R0494.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0494.json inflating: EURLEX57K/dataset/dev/32008R0886.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0886.json inflating: EURLEX57K/dataset/dev/32002R2330.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2330.json inflating: EURLEX57K/dataset/dev/32010D0580.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0580.json inflating: EURLEX57K/dataset/dev/31992D0227.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0227.json inflating: EURLEX57K/dataset/dev/31977D0207.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977D0207.json inflating: EURLEX57K/dataset/dev/32003R2117.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2117.json inflating: EURLEX57K/dataset/dev/32008R1281.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1281.json inflating: EURLEX57K/dataset/dev/32004D0650.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0650.json inflating: EURLEX57K/dataset/dev/32007R1505.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1505.json inflating: EURLEX57K/dataset/dev/32003R0829.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0829.json inflating: EURLEX57K/dataset/dev/31993R3628.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3628.json inflating: EURLEX57K/dataset/dev/32012D0080.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0080.json inflating: EURLEX57K/dataset/dev/32005D0240.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0240.json inflating: EURLEX57K/dataset/dev/32005R0751.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0751.json inflating: EURLEX57K/dataset/dev/32007R0036.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0036.json inflating: EURLEX57K/dataset/dev/32007D0527.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0527.json inflating: EURLEX57K/dataset/dev/32004R0599.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0599.json inflating: EURLEX57K/dataset/dev/32003R1049.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1049.json inflating: EURLEX57K/dataset/dev/32008R0618.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0618.json inflating: EURLEX57K/dataset/dev/31998D0677.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0677.json inflating: EURLEX57K/dataset/dev/31997R1709.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1709.json inflating: EURLEX57K/dataset/dev/31999D0515.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0515.json inflating: EURLEX57K/dataset/dev/32000R1509.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1509.json inflating: EURLEX57K/dataset/dev/32006R0641.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0641.json inflating: EURLEX57K/dataset/dev/31982D0511.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0511.json inflating: EURLEX57K/dataset/dev/32001R2417.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2417.json inflating: EURLEX57K/dataset/dev/32011R0481.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0481.json inflating: EURLEX57K/dataset/dev/31992R0551.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0551.json inflating: EURLEX57K/dataset/dev/31991D0015.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0015.json inflating: EURLEX57K/dataset/dev/32007R0523.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0523.json inflating: EURLEX57K/dataset/dev/31995D0381.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0381.json inflating: EURLEX57K/dataset/dev/32002R2012.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2012.json inflating: EURLEX57K/dataset/dev/31979R2903.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R2903.json inflating: EURLEX57K/dataset/dev/31978R3089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978R3089.json inflating: EURLEX57K/dataset/dev/31986R3941.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3941.json inflating: EURLEX57K/dataset/dev/32007R0173.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0173.json inflating: EURLEX57K/dataset/dev/32001R1542.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1542.json inflating: EURLEX57K/dataset/dev/32006R1729.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1729.json inflating: EURLEX57K/dataset/dev/32006R1379.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1379.json inflating: EURLEX57K/dataset/dev/31996L0001.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996L0001.json inflating: EURLEX57K/dataset/dev/32010R0560.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0560.json inflating: EURLEX57K/dataset/dev/32006D0429.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0429.json inflating: EURLEX57K/dataset/dev/32008R0927.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0927.json inflating: EURLEX57K/dataset/dev/31984R1962.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1962.json inflating: EURLEX57K/dataset/dev/31996D0507.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0507.json inflating: EURLEX57K/dataset/dev/32001R1811.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1811.json inflating: EURLEX57K/dataset/dev/32013D0474.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0474.json inflating: EURLEX57K/dataset/dev/32008D0866.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0866.json inflating: EURLEX57K/dataset/dev/31984R0220.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0220.json inflating: EURLEX57K/dataset/dev/32003D0125.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0125.json inflating: EURLEX57K/dataset/dev/32008D0889.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0889.json inflating: EURLEX57K/dataset/dev/32006D0096.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0096.json inflating: EURLEX57K/dataset/dev/32012D0203.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0203.json inflating: EURLEX57K/dataset/dev/32002R0243.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0243.json inflating: EURLEX57K/dataset/dev/32009R1042.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1042.json inflating: EURLEX57K/dataset/dev/32012R0342.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0342.json inflating: EURLEX57K/dataset/dev/32001R2381.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2381.json inflating: EURLEX57K/dataset/dev/32001R0646.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0646.json inflating: EURLEX57K/dataset/dev/31997R0661.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0661.json inflating: EURLEX57K/dataset/dev/31998R2072.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2072.json inflating: EURLEX57K/dataset/dev/31998R2588.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2588.json inflating: EURLEX57K/dataset/dev/32013R1261.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1261.json inflating: EURLEX57K/dataset/dev/31994R1475.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1475.json inflating: EURLEX57K/dataset/dev/31995R2992.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2992.json inflating: EURLEX57K/dataset/dev/31999R0053.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0053.json inflating: EURLEX57K/dataset/dev/32005R1117.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1117.json inflating: EURLEX57K/dataset/dev/31992L0004.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992L0004.json inflating: EURLEX57K/dataset/dev/32014D0215(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0215(01).json inflating: EURLEX57K/dataset/dev/31985R0683.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0683.json inflating: EURLEX57K/dataset/dev/31995R0628.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R0628.json inflating: EURLEX57K/dataset/dev/31985R2651.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2651.json inflating: EURLEX57K/dataset/dev/31986D0328.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0328.json inflating: EURLEX57K/dataset/dev/32001R2010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2010.json inflating: EURLEX57K/dataset/dev/32004R0521.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0521.json inflating: EURLEX57K/dataset/dev/31995R2750.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2750.json inflating: EURLEX57K/dataset/dev/32005R1402.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1402.json inflating: EURLEX57K/dataset/dev/31987R4148.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R4148.json inflating: EURLEX57K/dataset/dev/31998R0624.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0624.json inflating: EURLEX57K/dataset/dev/31993R3048.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3048.json inflating: EURLEX57K/dataset/dev/32011L0091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011L0091.json inflating: EURLEX57K/dataset/dev/32014R0973.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0973.json inflating: EURLEX57K/dataset/dev/32009R1150.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1150.json inflating: EURLEX57K/dataset/dev/32004R0608.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0608.json inflating: EURLEX57K/dataset/dev/31996D0100.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0100.json inflating: EURLEX57K/dataset/dev/32014D0648.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0648.json inflating: EURLEX57K/dataset/dev/32006R0495.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0495.json inflating: EURLEX57K/dataset/dev/32012D0311.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0311.json inflating: EURLEX57K/dataset/dev/32013R0562.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0562.json inflating: EURLEX57K/dataset/dev/31997R1498.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1498.json inflating: EURLEX57K/dataset/dev/31996L0056.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996L0056.json inflating: EURLEX57K/dataset/dev/32011D1105.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D1105.json inflating: EURLEX57K/dataset/dev/32005R0993.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0993.json inflating: EURLEX57K/dataset/dev/31995R1605.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1605.json inflating: EURLEX57K/dataset/dev/31996D0719(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0719(01).json inflating: EURLEX57K/dataset/dev/31999R2717.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R2717.json inflating: EURLEX57K/dataset/dev/32001R1450.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1450.json inflating: EURLEX57K/dataset/dev/32010R0022.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0022.json inflating: EURLEX57K/dataset/dev/32001R1903.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1903.json inflating: EURLEX57K/dataset/dev/31994R1971.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1971.json inflating: EURLEX57K/dataset/dev/32000D0062.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0062.json inflating: EURLEX57K/dataset/dev/31982R1979.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982R1979.json inflating: EURLEX57K/dataset/dev/31986R2468.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2468.json inflating: EURLEX57K/dataset/dev/31994D0722.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0722.json inflating: EURLEX57K/dataset/dev/31995D0440.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0440.json inflating: EURLEX57K/dataset/dev/31985R1987.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1987.json inflating: EURLEX57K/dataset/dev/32004R0472.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0472.json inflating: EURLEX57K/dataset/dev/31993R1825.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1825.json inflating: EURLEX57K/dataset/dev/32006D0254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0254.json inflating: EURLEX57K/dataset/dev/31999R1654.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1654.json inflating: EURLEX57K/dataset/dev/31997D0448.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0448.json inflating: EURLEX57K/dataset/dev/32005R0340.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0340.json inflating: EURLEX57K/dataset/dev/32002R1690.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1690.json inflating: EURLEX57K/dataset/dev/32003R1008.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1008.json inflating: EURLEX57K/dataset/dev/32004R1263.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1263.json inflating: EURLEX57K/dataset/dev/31996R1580.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1580.json inflating: EURLEX57K/dataset/dev/31994R3370.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R3370.json inflating: EURLEX57K/dataset/dev/31999D0041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0041.json inflating: EURLEX57K/dataset/dev/31993R1433.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1433.json inflating: EURLEX57K/dataset/dev/32011R0993.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0993.json inflating: EURLEX57K/dataset/dev/32005R1414.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1414.json inflating: EURLEX57K/dataset/dev/31985R3056.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R3056.json inflating: EURLEX57K/dataset/dev/32012R0495.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0495.json inflating: EURLEX57K/dataset/dev/31989R1610.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1610.json inflating: EURLEX57K/dataset/dev/31989R2895.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2895.json inflating: EURLEX57K/dataset/dev/31986R3900.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3900.json inflating: EURLEX57K/dataset/dev/31988R2427.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R2427.json inflating: EURLEX57K/dataset/dev/31989R3857.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R3857.json inflating: EURLEX57K/dataset/dev/32012R1007.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1007.json inflating: EURLEX57K/dataset/dev/32002R1106.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1106.json inflating: EURLEX57K/dataset/dev/32013R0827.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0827.json inflating: EURLEX57K/dataset/dev/31999R2644.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R2644.json inflating: EURLEX57K/dataset/dev/32003R1264.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1264.json inflating: EURLEX57K/dataset/dev/32009R0757.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0757.json inflating: EURLEX57K/dataset/dev/32008R0966.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0966.json inflating: EURLEX57K/dataset/dev/32006R0179.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0179.json inflating: EURLEX57K/dataset/dev/31990R2163.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2163.json inflating: EURLEX57K/dataset/dev/31997R2308.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2308.json inflating: EURLEX57K/dataset/dev/32010R0521.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0521.json inflating: EURLEX57K/dataset/dev/31987D0375.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0375.json inflating: EURLEX57K/dataset/dev/32003R2018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2018.json inflating: EURLEX57K/dataset/dev/31984R1889.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1889.json inflating: EURLEX57K/dataset/dev/32002D0206.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0206.json inflating: EURLEX57K/dataset/dev/31987R2209.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2209.json inflating: EURLEX57K/dataset/dev/32003R0160.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0160.json inflating: EURLEX57K/dataset/dev/31991R2344.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R2344.json inflating: EURLEX57K/dataset/dev/32012R0753.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0753.json inflating: EURLEX57K/dataset/dev/32006R2001.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R2001.json inflating: EURLEX57K/dataset/dev/32010R0464.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0464.json inflating: EURLEX57K/dataset/dev/32004R1849.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1849.json inflating: EURLEX57K/dataset/dev/31981D0293.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0293.json inflating: EURLEX57K/dataset/dev/32012R1142.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1142.json inflating: EURLEX57K/dataset/dev/32005R1678.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1678.json inflating: EURLEX57K/dataset/dev/32010R0937.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0937.json inflating: EURLEX57K/dataset/dev/31985R1179.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1179.json inflating: EURLEX57K/dataset/dev/31987L0018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987L0018.json inflating: EURLEX57K/dataset/dev/31987R2931.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2931.json inflating: EURLEX57K/dataset/dev/31996D0287.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0287.json inflating: EURLEX57K/dataset/dev/32014D0035.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0035.json inflating: EURLEX57K/dataset/dev/32007D0531.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0531.json inflating: EURLEX57K/dataset/dev/31992D0543.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0543.json inflating: EURLEX57K/dataset/dev/32012D0096.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0096.json inflating: EURLEX57K/dataset/dev/32001D0592.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0592.json inflating: EURLEX57K/dataset/dev/32004R0130.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0130.json inflating: EURLEX57K/dataset/dev/32005D0743.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0743.json inflating: EURLEX57K/dataset/dev/32014R0031.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0031.json inflating: EURLEX57K/dataset/dev/32001R1387.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1387.json inflating: EURLEX57K/dataset/dev/32014D0873.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0873.json inflating: EURLEX57K/dataset/dev/31982D0854.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0854.json inflating: EURLEX57K/dataset/dev/31981R3395.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R3395.json inflating: EURLEX57K/dataset/dev/32011R1286.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1286.json inflating: EURLEX57K/dataset/dev/32009R1111.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1111.json inflating: EURLEX57K/dataset/dev/31991R2606.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R2606.json inflating: EURLEX57K/dataset/dev/32010D0437.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0437.json inflating: EURLEX57K/dataset/dev/31996R1241.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1241.json inflating: EURLEX57K/dataset/dev/31995L0042.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995L0042.json inflating: EURLEX57K/dataset/dev/31989R2712.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2712.json inflating: EURLEX57K/dataset/dev/31991R1180.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1180.json inflating: EURLEX57K/dataset/dev/31990R3230.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3230.json inflating: EURLEX57K/dataset/dev/32000D0023.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0023.json inflating: EURLEX57K/dataset/dev/32003D0076.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0076.json inflating: EURLEX57K/dataset/dev/31995D0051.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0051.json inflating: EURLEX57K/dataset/dev/32002R0255.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0255.json inflating: EURLEX57K/dataset/dev/31986R2429.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2429.json inflating: EURLEX57K/dataset/dev/32003R0137.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0137.json inflating: EURLEX57K/dataset/dev/32007D0618.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0618.json inflating: EURLEX57K/dataset/dev/32006R0591.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0591.json inflating: EURLEX57K/dataset/dev/31984D0025.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0025.json inflating: EURLEX57K/dataset/dev/32001D0506.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0506.json inflating: EURLEX57K/dataset/dev/32011R0116.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0116.json inflating: EURLEX57K/dataset/dev/31991R0093.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0093.json inflating: EURLEX57K/dataset/dev/32002R0412.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0412.json inflating: EURLEX57K/dataset/dev/31981R0038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R0038.json inflating: EURLEX57K/dataset/dev/32011R1357.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1357.json inflating: EURLEX57K/dataset/dev/32000R1564.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1564.json inflating: EURLEX57K/dataset/dev/31984R1325.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1325.json inflating: EURLEX57K/dataset/dev/31999R0069.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0069.json inflating: EURLEX57K/dataset/dev/31989D0029.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0029.json inflating: EURLEX57K/dataset/dev/32009R0147.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0147.json inflating: EURLEX57K/dataset/dev/32008R0675.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0675.json inflating: EURLEX57K/dataset/dev/32011R0950.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0950.json inflating: EURLEX57K/dataset/dev/32011R0403.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0403.json inflating: EURLEX57K/dataset/dev/32012R0006.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0006.json inflating: EURLEX57K/dataset/dev/31971R1592.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31971R1592.json inflating: EURLEX57K/dataset/dev/32005L0052.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005L0052.json inflating: EURLEX57K/dataset/dev/31995R1280.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1280.json inflating: EURLEX57K/dataset/dev/32007R1563.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1563.json inflating: EURLEX57K/dataset/dev/32011D0391.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0391.json inflating: EURLEX57K/dataset/dev/32005D0104.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0104.json inflating: EURLEX57K/dataset/dev/31990R0467.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R0467.json inflating: EURLEX57K/dataset/dev/31994D0348.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0348.json inflating: EURLEX57K/dataset/dev/32002R0384.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0384.json inflating: EURLEX57K/dataset/dev/32000D0058.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0058.json inflating: EURLEX57K/dataset/dev/32010R0448.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0448.json inflating: EURLEX57K/dataset/dev/31995D0180.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0180.json inflating: EURLEX57K/dataset/dev/32007R0667.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0667.json inflating: EURLEX57K/dataset/dev/32014R0699.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0699.json inflating: EURLEX57K/dataset/dev/32001D0785.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0785.json inflating: EURLEX57K/dataset/dev/31997R0749.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0749.json inflating: EURLEX57K/dataset/dev/31993R0577.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0577.json inflating: EURLEX57K/dataset/dev/32001R2303.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2303.json inflating: EURLEX57K/dataset/dev/32013D0049.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0049.json inflating: EURLEX57K/dataset/dev/32007R1076.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1076.json inflating: EURLEX57K/dataset/dev/32003R1248.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1248.json inflating: EURLEX57K/dataset/dev/32005R1341.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1341.json inflating: EURLEX57K/dataset/dev/32005D0942.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0942.json inflating: EURLEX57K/dataset/dev/32008D0558.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0558.json inflating: EURLEX57K/dataset/dev/31990R1763.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1763.json inflating: EURLEX57K/dataset/dev/31989D0091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0091.json inflating: EURLEX57K/dataset/dev/31977R0818.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977R0818.json inflating: EURLEX57K/dataset/dev/31992L0086.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992L0086.json inflating: EURLEX57K/dataset/dev/32003D0236.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0236.json inflating: EURLEX57K/dataset/dev/32007R0519.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0519.json inflating: EURLEX57K/dataset/dev/32006D0690.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0690.json inflating: EURLEX57K/dataset/dev/31985D0310.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0310.json inflating: EURLEX57K/dataset/dev/31986R0654.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0654.json inflating: EURLEX57K/dataset/dev/32005R1896.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1896.json inflating: EURLEX57K/dataset/dev/32003R1970.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1970.json inflating: EURLEX57K/dataset/dev/32003R0398.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0398.json inflating: EURLEX57K/dataset/dev/32008R1033.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1033.json inflating: EURLEX57K/dataset/dev/31995R2382.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2382.json inflating: EURLEX57K/dataset/dev/31997D0063.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0063.json inflating: EURLEX57K/dataset/dev/32002D0441.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0441.json inflating: EURLEX57K/dataset/dev/31994R0177.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0177.json inflating: EURLEX57K/dataset/dev/32010R0736.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0736.json inflating: EURLEX57K/dataset/dev/31989R1684.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1684.json inflating: EURLEX57K/dataset/dev/32005R1480.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1480.json inflating: EURLEX57K/dataset/dev/31981R2814.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2814.json inflating: EURLEX57K/dataset/dev/32011D0846.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0846.json inflating: EURLEX57K/dataset/dev/31985R3168.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R3168.json inflating: EURLEX57K/dataset/dev/32001R2211.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2211.json inflating: EURLEX57K/dataset/dev/32006R0047.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0047.json inflating: EURLEX57K/dataset/dev/32014D0611(02).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0611(02).json inflating: EURLEX57K/dataset/dev/32005R2185.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R2185.json inflating: EURLEX57K/dataset/dev/32014D0760.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0760.json inflating: EURLEX57K/dataset/dev/32012D0393.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0393.json inflating: EURLEX57K/dataset/dev/32012R0682.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0682.json inflating: EURLEX57K/dataset/dev/32009R0239.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0239.json inflating: EURLEX57K/dataset/dev/31995R2802.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2802.json inflating: EURLEX57K/dataset/dev/32009D0728.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0728.json inflating: EURLEX57K/dataset/dev/31985R0894.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0894.json inflating: EURLEX57K/dataset/dev/31989D0357.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0357.json inflating: EURLEX57K/dataset/dev/31986R3214.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3214.json inflating: EURLEX57K/dataset/dev/31989R2381.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2381.json inflating: EURLEX57K/dataset/dev/31990L0427.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990L0427.json inflating: EURLEX57K/dataset/dev/32013R0919.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0919.json inflating: EURLEX57K/dataset/dev/31995R0995.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R0995.json inflating: EURLEX57K/dataset/dev/32009R0686.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0686.json inflating: EURLEX57K/dataset/dev/32007R1471.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1471.json inflating: EURLEX57K/dataset/dev/31999D0313.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0313.json inflating: EURLEX57K/dataset/dev/32014D0275.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0275.json inflating: EURLEX57K/dataset/dev/31995D0092.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0092.json inflating: EURLEX57K/dataset/dev/31989R1112.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1112.json inflating: EURLEX57K/dataset/dev/32011R0638.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0638.json inflating: EURLEX57K/dataset/dev/32006R0552.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0552.json inflating: EURLEX57K/dataset/dev/32004R1977.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1977.json inflating: EURLEX57K/dataset/dev/31992D0353.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0353.json inflating: EURLEX57K/dataset/dev/32009R0940.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0940.json inflating: EURLEX57K/dataset/dev/31980R1859.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980R1859.json inflating: EURLEX57K/dataset/dev/32011R0507.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0507.json inflating: EURLEX57K/dataset/dev/31976R0795.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976R0795.json inflating: EURLEX57K/dataset/dev/31995D0207.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0207.json inflating: EURLEX57K/dataset/dev/32003R1823.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1823.json inflating: EURLEX57K/dataset/dev/32010R0235.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0235.json inflating: EURLEX57K/dataset/dev/32010R1074.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R1074.json inflating: EURLEX57K/dataset/dev/32014D0849.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0849.json inflating: EURLEX57K/dataset/dev/31981R3205.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R3205.json inflating: EURLEX57K/dataset/dev/31987R1761.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1761.json inflating: EURLEX57K/dataset/dev/31993R3033.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3033.json inflating: EURLEX57K/dataset/dev/32001R1702.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1702.json inflating: EURLEX57K/dataset/dev/32002R1757.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1757.json inflating: EURLEX57K/dataset/dev/31994R1770.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1770.json inflating: EURLEX57K/dataset/dev/32012L0050.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012L0050.json inflating: EURLEX57K/dataset/dev/31989D0068.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0068.json inflating: EURLEX57K/dataset/dev/32001R1352.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1352.json inflating: EURLEX57K/dataset/dev/32008R0634.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0634.json inflating: EURLEX57K/dataset/dev/31980R3472.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980R3472.json inflating: EURLEX57K/dataset/dev/32005R1496.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1496.json inflating: EURLEX57K/dataset/dev/31992D0083.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0083.json inflating: EURLEX57K/dataset/dev/32011D0153.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0153.json inflating: EURLEX57K/dataset/dev/32002R0516.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0516.json inflating: EURLEX57K/dataset/dev/32005R1245.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1245.json inflating: EURLEX57K/dataset/dev/31999D0240.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0240.json inflating: EURLEX57K/dataset/dev/32007R1522.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1522.json inflating: EURLEX57K/dataset/dev/32007D0272.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0272.json inflating: EURLEX57K/dataset/dev/32005R0004.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0004.json inflating: EURLEX57K/dataset/dev/32010D0118.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0118.json inflating: EURLEX57K/dataset/dev/32004R0366.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0366.json inflating: EURLEX57K/dataset/dev/32002R0795.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0795.json inflating: EURLEX57K/dataset/dev/31992D0345.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0345.json inflating: EURLEX57K/dataset/dev/32007R0626.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0626.json inflating: EURLEX57K/dataset/dev/31990D0588.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0588.json inflating: EURLEX57K/dataset/dev/32005R0511.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0511.json inflating: EURLEX57K/dataset/dev/32001R1997.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1997.json inflating: EURLEX57K/dataset/dev/31998D0067.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0067.json inflating: EURLEX57K/dataset/dev/31989R3990.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R3990.json inflating: EURLEX57K/dataset/dev/31999D0305.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0305.json inflating: EURLEX57K/dataset/dev/32004R1432.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1432.json inflating: EURLEX57K/dataset/dev/31995R2951.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2951.json inflating: EURLEX57K/dataset/dev/31981R1111.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R1111.json inflating: EURLEX57K/dataset/dev/32008R0726.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0726.json inflating: EURLEX57K/dataset/dev/31984L0535.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984L0535.json inflating: EURLEX57K/dataset/dev/31984R1763.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1763.json inflating: EURLEX57K/dataset/dev/31986R3193.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3193.json inflating: EURLEX57K/dataset/dev/32008D0667.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0667.json inflating: EURLEX57K/dataset/dev/31997D0167.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0167.json inflating: EURLEX57K/dataset/dev/32001D0140.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0140.json inflating: EURLEX57K/dataset/dev/32009L0003.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009L0003.json inflating: EURLEX57K/dataset/dev/32012D0444.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0444.json inflating: EURLEX57K/dataset/dev/32000D0222.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0222.json inflating: EURLEX57K/dataset/dev/32001D0510.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0510.json inflating: EURLEX57K/dataset/dev/31997R0026.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0026.json inflating: EURLEX57K/dataset/dev/31999R2804.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R2804.json inflating: EURLEX57K/dataset/dev/32015R0294.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0294.json inflating: EURLEX57K/dataset/dev/32011D0041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0041.json inflating: EURLEX57K/dataset/dev/31996R0344.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0344.json inflating: EURLEX57K/dataset/dev/32013D0399.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0399.json inflating: EURLEX57K/dataset/dev/32011R0045.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0045.json inflating: EURLEX57K/dataset/dev/31989R2840.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2840.json inflating: EURLEX57K/dataset/dev/31987R0062.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R0062.json inflating: EURLEX57K/dataset/dev/31971D0057.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31971D0057.json inflating: EURLEX57K/dataset/dev/32014R0119.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0119.json inflating: EURLEX57K/dataset/dev/32005R1838.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1838.json inflating: EURLEX57K/dataset/dev/32014D0408.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0408.json inflating: EURLEX57K/dataset/dev/31992D0484.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0484.json inflating: EURLEX57K/dataset/dev/31987D0089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0089.json inflating: EURLEX57K/dataset/dev/32013D0663.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0663.json inflating: EURLEX57K/dataset/dev/32003R0273.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0273.json inflating: EURLEX57K/dataset/dev/31986D0241.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0241.json inflating: EURLEX57K/dataset/dev/32010R0777.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0777.json inflating: EURLEX57K/dataset/dev/32002R0812.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0812.json inflating: EURLEX57K/dataset/dev/32015R0502.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0502.json inflating: EURLEX57K/dataset/dev/32008R0819.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0819.json inflating: EURLEX57K/dataset/dev/31994R2222.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2222.json inflating: EURLEX57K/dataset/dev/31985D0205(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0205(01).json inflating: EURLEX57K/dataset/dev/32014D0371.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0371.json inflating: EURLEX57K/dataset/dev/32001R2250.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2250.json inflating: EURLEX57K/dataset/dev/32004D0786(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0786(01).json inflating: EURLEX57K/dataset/dev/32002R0392.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0392.json inflating: EURLEX57K/dataset/dev/32005R0403.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0403.json inflating: EURLEX57K/dataset/dev/31996R0082.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0082.json inflating: EURLEX57K/dataset/dev/32002R1429.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1429.json inflating: EURLEX57K/dataset/dev/32005R0950.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0950.json inflating: EURLEX57K/dataset/dev/32006L0011.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006L0011.json inflating: EURLEX57K/dataset/dev/31993R3208.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3208.json inflating: EURLEX57K/dataset/dev/31993R1235.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1235.json inflating: EURLEX57K/dataset/dev/32006R1752.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1752.json inflating: EURLEX57K/dataset/dev/31993R0832.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0832.json inflating: EURLEX57K/dataset/dev/32004R1465.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1465.json inflating: EURLEX57K/dataset/dev/32007R1430.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1430.json inflating: EURLEX57K/dataset/dev/31998R0521.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0521.json inflating: EURLEX57K/dataset/dev/32015R0447.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0447.json inflating: EURLEX57K/dataset/dev/31996R2050.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2050.json inflating: EURLEX57K/dataset/dev/31979R1084.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R1084.json inflating: EURLEX57K/dataset/dev/31974D0367.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31974D0367.json inflating: EURLEX57K/dataset/dev/31992D0312.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0312.json inflating: EURLEX57K/dataset/dev/31986R0496.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0496.json inflating: EURLEX57K/dataset/dev/31995R0468.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R0468.json inflating: EURLEX57K/dataset/dev/32004R1941.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1941.json inflating: EURLEX57K/dataset/dev/31987R2244.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2244.json inflating: EURLEX57K/dataset/dev/31998R1347.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1347.json inflating: EURLEX57K/dataset/dev/32003R0438.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0438.json inflating: EURLEX57K/dataset/dev/31997R0728.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0728.json inflating: EURLEX57K/dataset/dev/31993R2481.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2481.json inflating: EURLEX57K/dataset/dev/32003D0083.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0083.json inflating: EURLEX57K/dataset/dev/32014D0243.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0243.json inflating: EURLEX57K/dataset/dev/31992R3058.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3058.json inflating: EURLEX57K/dataset/dev/31991L0266.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991L0266.json inflating: EURLEX57K/dataset/dev/32008D0493.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0493.json inflating: EURLEX57K/dataset/dev/31997R1139.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1139.json inflating: EURLEX57K/dataset/dev/31966R0122.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31966R0122.json inflating: EURLEX57K/dataset/dev/31994D0329.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0329.json inflating: EURLEX57K/dataset/dev/31992D0220.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0220.json inflating: EURLEX57K/dataset/dev/31997D0686.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0686.json inflating: EURLEX57K/dataset/dev/31980D0446.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980D0446.json inflating: EURLEX57K/dataset/dev/31984R2354.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R2354.json inflating: EURLEX57K/dataset/dev/31976D0162.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976D0162.json inflating: EURLEX57K/dataset/dev/31984D0228.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0228.json inflating: EURLEX57K/dataset/dev/31995R2137.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2137.json inflating: EURLEX57K/dataset/dev/32003R2110.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2110.json inflating: EURLEX57K/dataset/dev/32008D0755.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0755.json inflating: EURLEX57K/dataset/dev/32009R0126.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0126.json inflating: EURLEX57K/dataset/dev/32014D0886.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0886.json inflating: EURLEX57K/dataset/dev/32000R0251.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R0251.json inflating: EURLEX57K/dataset/dev/31982D0458.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0458.json inflating: EURLEX57K/dataset/dev/32002R0536.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0536.json inflating: EURLEX57K/dataset/dev/32004R2117.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2117.json inflating: EURLEX57K/dataset/dev/31987D0011.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0011.json inflating: EURLEX57K/dataset/dev/31985R0637.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0637.json inflating: EURLEX57K/dataset/dev/32012R0088.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0088.json inflating: EURLEX57K/dataset/dev/32003R0711.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0711.json inflating: EURLEX57K/dataset/dev/31992R2560.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2560.json inflating: EURLEX57K/dataset/dev/32000R0744.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R0744.json inflating: EURLEX57K/dataset/dev/32010R0215.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0215.json inflating: EURLEX57K/dataset/dev/32005R1459.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1459.json inflating: EURLEX57K/dataset/dev/32002R1632.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1632.json inflating: EURLEX57K/dataset/dev/32008R0301.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0301.json inflating: EURLEX57K/dataset/dev/32009R0433.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0433.json inflating: EURLEX57K/dataset/dev/32011R0874.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0874.json inflating: EURLEX57K/dataset/dev/32008R0585.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0585.json inflating: EURLEX57K/dataset/dev/31991R1437.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1437.json inflating: EURLEX57K/dataset/dev/32005R1377.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1377.json inflating: EURLEX57K/dataset/dev/31993R2985.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2985.json inflating: EURLEX57K/dataset/dev/31990D0455.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0455.json inflating: EURLEX57K/dataset/dev/31992D0298.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0298.json inflating: EURLEX57K/dataset/dev/32001R2335.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2335.json inflating: EURLEX57K/dataset/dev/32006R0499.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0499.json inflating: EURLEX57K/dataset/dev/31981R0327.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R0327.json inflating: EURLEX57K/dataset/dev/32010R0491.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0491.json inflating: EURLEX57K/dataset/dev/31991D0367.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0367.json inflating: EURLEX57K/dataset/dev/31993R2593.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2593.json inflating: EURLEX57K/dataset/dev/32002R2225.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2225.json inflating: EURLEX57K/dataset/dev/32006D0537.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0537.json inflating: EURLEX57K/dataset/dev/31994R2202.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2202.json inflating: EURLEX57K/dataset/dev/32004R1853.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1853.json inflating: EURLEX57K/dataset/dev/32007D0205.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0205.json inflating: EURLEX57K/dataset/dev/32005R0423.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0423.json inflating: EURLEX57K/dataset/dev/32013R1390.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1390.json inflating: EURLEX57K/dataset/dev/32002R1059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1059.json inflating: EURLEX57K/dataset/dev/31991R2427.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R2427.json inflating: EURLEX57K/dataset/dev/31995D0335.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0335.json inflating: EURLEX57K/dataset/dev/32000R2391.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2391.json inflating: EURLEX57K/dataset/dev/31983D0297.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0297.json inflating: EURLEX57K/dataset/dev/32004R0192.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0192.json inflating: EURLEX57K/dataset/dev/32013R0352.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0352.json inflating: EURLEX57K/dataset/dev/32002R0561.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0561.json inflating: EURLEX57K/dataset/dev/32012R0460.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0460.json inflating: EURLEX57K/dataset/dev/31984R0447.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0447.json inflating: EURLEX57K/dataset/dev/31992R2022.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2022.json inflating: EURLEX57K/dataset/dev/31986R1862.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1862.json inflating: EURLEX57K/dataset/dev/31997D0452.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0452.json inflating: EURLEX57K/dataset/dev/32010D0246.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0246.json inflating: EURLEX57K/dataset/dev/31984L0450.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984L0450.json inflating: EURLEX57K/dataset/dev/32011R0966.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0966.json inflating: EURLEX57K/dataset/dev/32001R1325.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1325.json inflating: EURLEX57K/dataset/dev/32014R1378.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1378.json inflating: EURLEX57K/dataset/dev/32007R0981.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0981.json inflating: EURLEX57K/dataset/dev/32014R1397.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1397.json inflating: EURLEX57K/dataset/dev/31988R0729.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0729.json inflating: EURLEX57K/dataset/dev/32011R1361.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1361.json inflating: EURLEX57K/dataset/dev/31997D0147.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0147.json inflating: EURLEX57K/dataset/dev/31994R0403.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0403.json inflating: EURLEX57K/dataset/dev/31999R0819.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0819.json inflating: EURLEX57K/dataset/dev/32010D0303.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0303.json inflating: EURLEX57K/dataset/dev/32003R0316.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0316.json inflating: EURLEX57K/dataset/dev/31992R2167.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2167.json inflating: EURLEX57K/dataset/dev/32011R0570.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0570.json inflating: EURLEX57K/dataset/dev/31987R0238.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R0238.json inflating: EURLEX57K/dataset/dev/32000R2411.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2411.json inflating: EURLEX57K/dataset/dev/32004R0612.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0612.json inflating: EURLEX57K/dataset/dev/32013D0439.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0439.json inflating: EURLEX57K/dataset/dev/32006D0034.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0034.json inflating: EURLEX57K/dataset/dev/32004R1453.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1453.json inflating: EURLEX57K/dataset/dev/31999D0221.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0221.json inflating: EURLEX57K/dataset/dev/32005L0072.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005L0072.json inflating: EURLEX57K/dataset/dev/31993L0055.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993L0055.json inflating: EURLEX57K/dataset/dev/32007R1113.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1113.json inflating: EURLEX57K/dataset/dev/32008R0086.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0086.json inflating: EURLEX57K/dataset/dev/32006R0899.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0899.json inflating: EURLEX57K/dataset/dev/32000R1378.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1378.json inflating: EURLEX57K/dataset/dev/32004R1516.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1516.json inflating: EURLEX57K/dataset/dev/32000D0078.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0078.json inflating: EURLEX57K/dataset/dev/32010D0179.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0179.json inflating: EURLEX57K/dataset/dev/32014D0347.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0347.json inflating: EURLEX57K/dataset/dev/32003D0187.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0187.json inflating: EURLEX57K/dataset/dev/31994D0738.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0738.json inflating: EURLEX57K/dataset/dev/31991D0664.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0664.json inflating: EURLEX57K/dataset/dev/32010R0192.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0192.json inflating: EURLEX57K/dataset/dev/32008R0205.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0205.json inflating: EURLEX57K/dataset/dev/32009R0167.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0167.json inflating: EURLEX57K/dataset/dev/32012R1267.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1267.json inflating: EURLEX57K/dataset/dev/32006R1158.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1158.json inflating: EURLEX57K/dataset/dev/31994R1341.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1341.json inflating: EURLEX57K/dataset/dev/31986R0766.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0766.json inflating: EURLEX57K/dataset/dev/31992D0518.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0518.json inflating: EURLEX57K/dataset/dev/31994D0041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0041.json inflating: EURLEX57K/dataset/dev/32004R0184.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0184.json inflating: EURLEX57K/dataset/dev/31989R1759.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1759.json inflating: EURLEX57K/dataset/dev/32012D0537.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0537.json inflating: EURLEX57K/dataset/dev/31996D0663.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0663.json inflating: EURLEX57K/dataset/dev/32002D0573.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0573.json inflating: EURLEX57K/dataset/dev/32015R0308.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0308.json inflating: EURLEX57K/dataset/dev/31996D0233.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0233.json inflating: EURLEX57K/dataset/dev/32009R1263.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1263.json inflating: EURLEX57K/dataset/dev/32010R0254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0254.json inflating: EURLEX57K/dataset/dev/31996R1563.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1563.json inflating: EURLEX57K/dataset/dev/31991R1708.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1708.json inflating: EURLEX57K/dataset/dev/32013R1040.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1040.json inflating: EURLEX57K/dataset/dev/32004D0883.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0883.json inflating: EURLEX57K/dataset/dev/32004D0929.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0929.json inflating: EURLEX57K/dataset/dev/31995R1166.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1166.json inflating: EURLEX57K/dataset/dev/32003R1141.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1141.json inflating: EURLEX57K/dataset/dev/31984R1755.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1755.json inflating: EURLEX57K/dataset/dev/32008D0201.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0201.json inflating: EURLEX57K/dataset/dev/31997D0852.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0852.json inflating: EURLEX57K/dataset/dev/32014R1155.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1155.json inflating: EURLEX57K/dataset/dev/31991L0620.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991L0620.json inflating: EURLEX57K/dataset/dev/31992R1423.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1423.json inflating: EURLEX57K/dataset/dev/31990R1344.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1344.json inflating: EURLEX57K/dataset/dev/32006R1363.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1363.json inflating: EURLEX57K/dataset/dev/31993D0441.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0441.json inflating: EURLEX57K/dataset/dev/32006R0122.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0122.json inflating: EURLEX57K/dataset/dev/32001R0719.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0719.json inflating: EURLEX57K/dataset/dev/31992D0373.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0373.json inflating: EURLEX57K/dataset/dev/32004D0641.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0641.json inflating: EURLEX57K/dataset/dev/31997R2216.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2216.json inflating: EURLEX57K/dataset/dev/32005D0489.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0489.json inflating: EURLEX57K/dataset/dev/31997D0690.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0690.json inflating: EURLEX57K/dataset/dev/32015D0422.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0422.json inflating: EURLEX57K/dataset/dev/32006D0126.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0126.json inflating: EURLEX57K/dataset/dev/32013R0893.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0893.json inflating: EURLEX57K/dataset/dev/31979R2276.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R2276.json inflating: EURLEX57K/dataset/dev/31997R1590.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1590.json inflating: EURLEX57K/dataset/dev/32013R0939.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0939.json inflating: EURLEX57K/dataset/dev/31990R1201.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1201.json inflating: EURLEX57K/dataset/dev/32003R0992.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0992.json inflating: EURLEX57K/dataset/dev/32011R0024.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0024.json inflating: EURLEX57K/dataset/dev/31996R0260.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0260.json inflating: EURLEX57K/dataset/dev/31990D0382.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0382.json inflating: EURLEX57K/dataset/dev/31978R1411.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978R1411.json inflating: EURLEX57K/dataset/dev/31990R1981.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1981.json inflating: EURLEX57K/dataset/dev/31996R1888.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1888.json inflating: EURLEX57K/dataset/dev/32007R0885.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0885.json inflating: EURLEX57K/dataset/dev/31992R2960.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2960.json inflating: EURLEX57K/dataset/dev/31978D0711.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978D0711.json inflating: EURLEX57K/dataset/dev/32013R1152.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1152.json inflating: EURLEX57K/dataset/dev/32002R1761.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1761.json inflating: EURLEX57K/dataset/dev/31987R3785.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R3785.json inflating: EURLEX57K/dataset/dev/31997R1206.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1206.json inflating: EURLEX57K/dataset/dev/31996R1534.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1534.json inflating: EURLEX57K/dataset/dev/31987R1307.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1307.json inflating: EURLEX57K/dataset/dev/32000D0243.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0243.json inflating: EURLEX57K/dataset/dev/32003R1815.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1815.json inflating: EURLEX57K/dataset/dev/32007R0539.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0539.json inflating: EURLEX57K/dataset/dev/32007D0182.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0182.json inflating: EURLEX57K/dataset/dev/31985R0621.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0621.json inflating: EURLEX57K/dataset/dev/31999R0858.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0858.json inflating: EURLEX57K/dataset/dev/31993R1891.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1891.json inflating: EURLEX57K/dataset/dev/31988R1483.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1483.json inflating: EURLEX57K/dataset/dev/31983D0239.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0239.json inflating: EURLEX57K/dataset/dev/32012D0075.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0075.json inflating: EURLEX57K/dataset/dev/31993D0085.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0085.json inflating: EURLEX57K/dataset/dev/31991R0709.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0709.json inflating: EURLEX57K/dataset/dev/32001R1935.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1935.json inflating: EURLEX57K/dataset/dev/32001D0336.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0336.json inflating: EURLEX57K/dataset/dev/31995R2859.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2859.json inflating: EURLEX57K/dataset/dev/32013R1200.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1200.json inflating: EURLEX57K/dataset/dev/31994D3092.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D3092.json inflating: EURLEX57K/dataset/dev/32004R1490.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1490.json inflating: EURLEX57K/dataset/dev/31998D0495.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0495.json inflating: EURLEX57K/dataset/dev/31993L0079.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993L0079.json inflating: EURLEX57K/dataset/dev/31990R3302.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3302.json inflating: EURLEX57K/dataset/dev/31991R3060.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3060.json inflating: EURLEX57K/dataset/dev/32003R1244.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1244.json inflating: EURLEX57K/dataset/dev/32013R0554.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0554.json inflating: EURLEX57K/dataset/dev/32006R0159.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0159.json inflating: EURLEX57K/dataset/dev/31997R2328.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2328.json inflating: EURLEX57K/dataset/dev/32000D0541.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0541.json inflating: EURLEX57K/dataset/dev/31995D0533.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0533.json inflating: EURLEX57K/dataset/dev/32003R2038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2038.json inflating: EURLEX57K/dataset/dev/31977R1881.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977R1881.json inflating: EURLEX57K/dataset/dev/31984D0300.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0300.json inflating: EURLEX57K/dataset/dev/32003R2192.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2192.json inflating: EURLEX57K/dataset/dev/32002R1825.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1825.json inflating: EURLEX57K/dataset/dev/32005R1064.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1064.json inflating: EURLEX57K/dataset/dev/31999D0061.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0061.json inflating: EURLEX57K/dataset/dev/32007R1353.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1353.json inflating: EURLEX57K/dataset/dev/32005R1434.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1434.json inflating: EURLEX57K/dataset/dev/32007R0811.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0811.json inflating: EURLEX57K/dataset/dev/32006R0620.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0620.json inflating: EURLEX57K/dataset/dev/31976D0699.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976D0699.json inflating: EURLEX57K/dataset/dev/32004D0006.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0006.json inflating: EURLEX57K/dataset/dev/32004R0517.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0517.json inflating: EURLEX57K/dataset/dev/32010R0782.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0782.json inflating: EURLEX57K/dataset/dev/31985R1948.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1948.json inflating: EURLEX57K/dataset/dev/32006D0761.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0761.json inflating: EURLEX57K/dataset/dev/32013D0696.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0696.json inflating: EURLEX57K/dataset/dev/32007R0112.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0112.json inflating: EURLEX57K/dataset/dev/32004D0617(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0617(01).json inflating: EURLEX57K/dataset/dev/32009D0909.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0909.json inflating: EURLEX57K/dataset/dev/31994D0597.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0597.json inflating: EURLEX57K/dataset/dev/32004D0143.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0143.json inflating: EURLEX57K/dataset/dev/31997R0529.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0529.json inflating: EURLEX57K/dataset/dev/31987D0093.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0093.json inflating: EURLEX57K/dataset/dev/32014R0103.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0103.json inflating: EURLEX57K/dataset/dev/32007D0546.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0546.json inflating: EURLEX57K/dataset/dev/32003R0269.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0269.json inflating: EURLEX57K/dataset/dev/31982D0065.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0065.json inflating: EURLEX57K/dataset/dev/32013R1129.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1129.json inflating: EURLEX57K/dataset/dev/32002R1034.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1034.json inflating: EURLEX57K/dataset/dev/32010R1202.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R1202.json inflating: EURLEX57K/dataset/dev/32010L0054.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010L0054.json inflating: EURLEX57K/dataset/dev/31989R1958.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1958.json inflating: EURLEX57K/dataset/dev/32011R0321.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0321.json inflating: EURLEX57K/dataset/dev/32009R1074.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1074.json inflating: EURLEX57K/dataset/dev/32012D0665.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0665.json inflating: EURLEX57K/dataset/dev/31995D0071.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0071.json inflating: EURLEX57K/dataset/dev/32004D0397.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0397.json inflating: EURLEX57K/dataset/dev/31993D0528.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0528.json inflating: EURLEX57K/dataset/dev/31996D0474.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0474.json inflating: EURLEX57K/dataset/dev/32003R0117.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0117.json inflating: EURLEX57K/dataset/dev/31996D0161.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0161.json inflating: EURLEX57K/dataset/dev/31995D0134.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0134.json inflating: EURLEX57K/dataset/dev/31985R2149.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R2149.json inflating: EURLEX57K/dataset/dev/32004R0393.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0393.json inflating: EURLEX57K/dataset/dev/32004D0682.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0682.json inflating: EURLEX57K/dataset/dev/31994R0317.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0317.json inflating: EURLEX57K/dataset/dev/32004R1078.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1078.json inflating: EURLEX57K/dataset/dev/31988R0197.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0197.json inflating: EURLEX57K/dataset/dev/32009R0370.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0370.json inflating: EURLEX57K/dataset/dev/32012D0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0059.json inflating: EURLEX57K/dataset/dev/31987R2507.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2507.json inflating: EURLEX57K/dataset/dev/32004D0401.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0401.json inflating: EURLEX57K/dataset/dev/32014R0441.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0441.json inflating: EURLEX57K/dataset/dev/32007D0004.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0004.json inflating: EURLEX57K/dataset/dev/31981D0572.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0572.json inflating: EURLEX57K/dataset/dev/31992R1726.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1726.json inflating: EURLEX57K/dataset/dev/31988D0255.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0255.json inflating: EURLEX57K/dataset/dev/31988R0744.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0744.json inflating: EURLEX57K/dataset/dev/31988R0314.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0314.json inflating: EURLEX57K/dataset/dev/32006R1573.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1573.json inflating: EURLEX57K/dataset/dev/31998D0641.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0641.json inflating: EURLEX57K/dataset/dev/32007L0017.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007L0017.json inflating: EURLEX57K/dataset/dev/31998D0211.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0211.json inflating: EURLEX57K/dataset/dev/31993D0601.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0601.json inflating: EURLEX57K/dataset/dev/31996R2271.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2271.json inflating: EURLEX57K/dataset/dev/32004R2068.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2068.json inflating: EURLEX57K/dataset/dev/31999D0870.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0870.json inflating: EURLEX57K/dataset/dev/32003R0394.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0394.json inflating: EURLEX57K/dataset/dev/31981D0437.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0437.json inflating: EURLEX57K/dataset/dev/32010D0381.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0381.json inflating: EURLEX57K/dataset/dev/32000R2606.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2606.json inflating: EURLEX57K/dataset/dev/31990D0091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0091.json inflating: EURLEX57K/dataset/dev/32011R0767.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0767.json inflating: EURLEX57K/dataset/dev/32010D0114.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0114.json inflating: EURLEX57K/dataset/dev/32013R0450.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0450.json inflating: EURLEX57K/dataset/dev/32008R0842.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0842.json inflating: EURLEX57K/dataset/dev/31993R0185.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0185.json inflating: EURLEX57K/dataset/dev/32012R1123.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1123.json inflating: EURLEX57K/dataset/dev/31998R2052.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2052.json inflating: EURLEX57K/dataset/dev/32000R1745.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1745.json inflating: EURLEX57K/dataset/dev/31993R3603.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3603.json inflating: EURLEX57K/dataset/dev/32002R1472.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1472.json inflating: EURLEX57K/dataset/dev/32009D0698.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0698.json inflating: EURLEX57K/dataset/dev/32012R1089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1089.json inflating: EURLEX57K/dataset/dev/32008R0511.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0511.json inflating: EURLEX57K/dataset/dev/32006R1709.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1709.json inflating: EURLEX57K/dataset/dev/31995R1388.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1388.json inflating: EURLEX57K/dataset/dev/32009R0366.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0366.json inflating: EURLEX57K/dataset/dev/32004R1594.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1594.json inflating: EURLEX57K/dataset/dev/31991R3021.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3021.json inflating: EURLEX57K/dataset/dev/32000L0006.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000L0006.json inflating: EURLEX57K/dataset/dev/31992R1049.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1049.json inflating: EURLEX57K/dataset/dev/32011R0622.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0622.json inflating: EURLEX57K/dataset/dev/32000D0500.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0500.json inflating: EURLEX57K/dataset/dev/31985D0473.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0473.json inflating: EURLEX57K/dataset/dev/31991R3922.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3922.json inflating: EURLEX57K/dataset/dev/31990R1057.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1057.json inflating: EURLEX57K/dataset/dev/32010R1078.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R1078.json inflating: EURLEX57K/dataset/dev/31998D0712.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0712.json inflating: EURLEX57K/dataset/dev/31988R2295.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R2295.json inflating: EURLEX57K/dataset/dev/31969R2571.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31969R2571.json inflating: EURLEX57K/dataset/dev/32005D0325.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0325.json inflating: EURLEX57K/dataset/dev/32004R2091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2091.json inflating: EURLEX57K/dataset/dev/32014D0516.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0516.json inflating: EURLEX57K/dataset/dev/31992R0571.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0571.json inflating: EURLEX57K/dataset/dev/32002R2032.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2032.json inflating: EURLEX57K/dataset/dev/31997R2040.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2040.json inflating: EURLEX57K/dataset/dev/31986R2673.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2673.json inflating: EURLEX57K/dataset/dev/31989R1221.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1221.json inflating: EURLEX57K/dataset/dev/31992D0430.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0430.json inflating: EURLEX57K/dataset/dev/32015D0224.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0224.json inflating: EURLEX57K/dataset/dev/31989R1734.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1734.json inflating: EURLEX57K/dataset/dev/31993R2291.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2291.json inflating: EURLEX57K/dataset/dev/31992R2059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2059.json inflating: EURLEX57K/dataset/dev/31970R1594.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31970R1594.json inflating: EURLEX57K/dataset/dev/31981L0577.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981L0577.json inflating: EURLEX57K/dataset/dev/31993R1147.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1147.json inflating: EURLEX57K/dataset/dev/32011L0009.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011L0009.json inflating: EURLEX57K/dataset/dev/32009R0624.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0624.json inflating: EURLEX57K/dataset/dev/31998D0179.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0179.json inflating: EURLEX57K/dataset/dev/31990R2913.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2913.json inflating: EURLEX57K/dataset/dev/31998D0529.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0529.json inflating: EURLEX57K/dataset/dev/32001D0320.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0320.json inflating: EURLEX57K/dataset/dev/31986D0164.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0164.json inflating: EURLEX57K/dataset/dev/31994R0213.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0213.json inflating: EURLEX57K/dataset/dev/31992R2327.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2327.json inflating: EURLEX57K/dataset/dev/32009R1170.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1170.json inflating: EURLEX57K/dataset/dev/32014R0683.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0683.json inflating: EURLEX57K/dataset/dev/31996D0120.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0120.json inflating: EURLEX57K/dataset/dev/32010D0456.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0456.json inflating: EURLEX57K/dataset/dev/32001D0265.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0265.json inflating: EURLEX57K/dataset/dev/32002R1833.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1833.json inflating: EURLEX57K/dataset/dev/32002R0721.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0721.json inflating: EURLEX57K/dataset/dev/31994D0217.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0217.json inflating: EURLEX57K/dataset/dev/31990D0183.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0183.json inflating: EURLEX57K/dataset/dev/32011R0675.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0675.json inflating: EURLEX57K/dataset/dev/32014R0729.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0729.json inflating: EURLEX57K/dataset/dev/31990R0168.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R0168.json inflating: EURLEX57K/dataset/dev/32001R1165.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1165.json inflating: EURLEX57K/dataset/dev/31994R1117.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1117.json inflating: EURLEX57K/dataset/dev/31985R3719.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R3719.json inflating: EURLEX57K/dataset/dev/32002R1130.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1130.json inflating: EURLEX57K/dataset/dev/32003R1602.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1602.json inflating: EURLEX57K/dataset/dev/31999R1727.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1727.json inflating: EURLEX57K/dataset/dev/32003R1878.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1878.json inflating: EURLEX57K/dataset/dev/32003D0781.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0781.json inflating: EURLEX57K/dataset/dev/32014R0050.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0050.json inflating: EURLEX57K/dataset/dev/31988R1544.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1544.json inflating: EURLEX57K/dataset/dev/32005R0663.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0663.json inflating: EURLEX57K/dataset/dev/31998D0745.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0745.json inflating: EURLEX57K/dataset/dev/31994R1394.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1394.json inflating: EURLEX57K/dataset/dev/31984R0892.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0892.json inflating: EURLEX57K/dataset/dev/32001R1309.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1309.json inflating: EURLEX57K/dataset/dev/32005R0376.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0376.json inflating: EURLEX57K/dataset/dev/31993R2696.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2696.json inflating: EURLEX57K/dataset/dev/31997D0184.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0184.json inflating: EURLEX57K/dataset/dev/31996R2230.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2230.json inflating: EURLEX57K/dataset/dev/32003R2242.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2242.json inflating: EURLEX57K/dataset/dev/32000D0691.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0691.json inflating: EURLEX57K/dataset/dev/32004D0155.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0155.json inflating: EURLEX57K/dataset/dev/31991D0433.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0433.json inflating: EURLEX57K/dataset/dev/32002D0522(03).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0522(03).json inflating: EURLEX57K/dataset/dev/32002D0914(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0914(01).json inflating: EURLEX57K/dataset/dev/31997R2016.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2016.json inflating: EURLEX57K/dataset/dev/32006D0776.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0776.json inflating: EURLEX57K/dataset/dev/31983R0314.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983R0314.json inflating: EURLEX57K/dataset/dev/31992R0527.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0527.json inflating: EURLEX57K/dataset/dev/31986R2275.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2275.json inflating: EURLEX57K/dataset/dev/31997R2446.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2446.json inflating: EURLEX57K/dataset/dev/32014D0540.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0540.json inflating: EURLEX57K/dataset/dev/32002R0409.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0409.json inflating: EURLEX57K/dataset/dev/32005R1423.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1423.json inflating: EURLEX57K/dataset/dev/31988D0215.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0215.json inflating: EURLEX57K/dataset/dev/32014R1210.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1210.json inflating: EURLEX57K/dataset/dev/32006R1476.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1476.json inflating: EURLEX57K/dataset/dev/32014D0956.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0956.json inflating: EURLEX57K/dataset/dev/32006R1533.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1533.json inflating: EURLEX57K/dataset/dev/32007D0802.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0802.json inflating: EURLEX57K/dataset/dev/31994R1680.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1680.json inflating: EURLEX57K/dataset/dev/32013R1094.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1094.json inflating: EURLEX57K/dataset/dev/32004D0857.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0857.json inflating: EURLEX57K/dataset/dev/32006R0322.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0322.json inflating: EURLEX57K/dataset/dev/32011R0418.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0418.json inflating: EURLEX57K/dataset/dev/32010R0280.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0280.json inflating: EURLEX57K/dataset/dev/31999R1233.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1233.json inflating: EURLEX57K/dataset/dev/32001D0008.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0008.json inflating: EURLEX57K/dataset/dev/31986R1635.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1635.json inflating: EURLEX57K/dataset/dev/31998D0482.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0482.json inflating: EURLEX57K/dataset/dev/31994R1403.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1403.json inflating: EURLEX57K/dataset/dev/32007R1082.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1082.json inflating: EURLEX57K/dataset/dev/32013L0041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013L0041.json inflating: EURLEX57K/dataset/dev/32009R0275.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0275.json inflating: EURLEX57K/dataset/dev/32008D0955.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0955.json inflating: EURLEX57K/dataset/dev/31996D0064.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0064.json inflating: EURLEX57K/dataset/dev/32003R0507.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0507.json inflating: EURLEX57K/dataset/dev/32012R0334.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0334.json inflating: EURLEX57K/dataset/dev/31986D0165.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0165.json inflating: EURLEX57K/dataset/dev/32001R0260.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0260.json inflating: EURLEX57K/dataset/dev/31995D0461.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0461.json inflating: EURLEX57K/dataset/dev/32004D0292.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0292.json inflating: EURLEX57K/dataset/dev/31990R3315.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3315.json inflating: EURLEX57K/dataset/dev/32003R0911.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0911.json inflating: EURLEX57K/dataset/dev/32014R0905.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0905.json inflating: EURLEX57K/dataset/dev/31998R1940.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1940.json inflating: EURLEX57K/dataset/dev/31998D0343.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0343.json inflating: EURLEX57K/dataset/dev/31984R1693.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1693.json inflating: EURLEX57K/dataset/dev/32003R1087.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1087.json inflating: EURLEX57K/dataset/dev/31994R1638.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1638.json inflating: EURLEX57K/dataset/dev/32007R0851.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0851.json inflating: EURLEX57K/dataset/dev/31983R1102.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983R1102.json inflating: EURLEX57K/dataset/dev/31989R0431.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R0431.json inflating: EURLEX57K/dataset/dev/32005R1927.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1927.json inflating: EURLEX57K/dataset/dev/31995R1809.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1809.json inflating: EURLEX57K/dataset/dev/32010D0683.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0683.json inflating: EURLEX57K/dataset/dev/32004R0107.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0107.json inflating: EURLEX57K/dataset/dev/31991D0171.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0171.json inflating: EURLEX57K/dataset/dev/32001R2089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2089.json inflating: EURLEX57K/dataset/dev/32015R0221.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0221.json inflating: EURLEX57K/dataset/dev/32003R0383.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0383.json inflating: EURLEX57K/dataset/dev/31994D0187.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0187.json inflating: EURLEX57K/dataset/dev/32003R0229.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0229.json inflating: EURLEX57K/dataset/dev/32012R0949.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0949.json inflating: EURLEX57K/dataset/dev/31998D0206.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0206.json inflating: EURLEX57K/dataset/dev/31992R1224.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1224.json inflating: EURLEX57K/dataset/dev/31987R3614.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R3614.json inflating: EURLEX57K/dataset/dev/31992R1674.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1674.json inflating: EURLEX57K/dataset/dev/31999D0534.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0534.json inflating: EURLEX57K/dataset/dev/31988R3939.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R3939.json inflating: EURLEX57K/dataset/dev/31999R2018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R2018.json inflating: EURLEX57K/dataset/dev/32001D0726.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0726.json inflating: EURLEX57K/dataset/dev/31993D0495.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0495.json inflating: EURLEX57K/dataset/dev/32004R1829.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1829.json inflating: EURLEX57K/dataset/dev/31996D0033.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0033.json inflating: EURLEX57K/dataset/dev/32012D0672.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0672.json inflating: EURLEX57K/dataset/dev/32012R0363.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0363.json inflating: EURLEX57K/dataset/dev/32003D0041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0041.json inflating: EURLEX57K/dataset/dev/32002D0323.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0323.json inflating: EURLEX57K/dataset/dev/32008R1301.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1301.json inflating: EURLEX57K/dataset/dev/32005D0148.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0148.json inflating: EURLEX57K/dataset/dev/32003R0803.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0803.json inflating: EURLEX57K/dataset/dev/32003R1711.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1711.json inflating: EURLEX57K/dataset/dev/31990R3657.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3657.json inflating: EURLEX57K/dataset/dev/32013R1240.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1240.json inflating: EURLEX57K/dataset/dev/32008D0451.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0451.json inflating: EURLEX57K/dataset/dev/31993R1795.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1795.json inflating: EURLEX57K/dataset/dev/32001R1099.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1099.json inflating: EURLEX57K/dataset/dev/32009R0367.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0367.json inflating: EURLEX57K/dataset/dev/32013R0514.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0514.json inflating: EURLEX57K/dataset/dev/31982D0249.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0249.json inflating: EURLEX57K/dataset/dev/31998R2815.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2815.json inflating: EURLEX57K/dataset/dev/31984R1943.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1943.json inflating: EURLEX57K/dataset/dev/32002R1865.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1865.json inflating: EURLEX57K/dataset/dev/32011D0298.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0298.json inflating: EURLEX57K/dataset/dev/32001R0722.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0722.json inflating: EURLEX57K/dataset/dev/31998R0982.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0982.json inflating: EURLEX57K/dataset/dev/32011D0762.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0762.json inflating: EURLEX57K/dataset/dev/31987R0254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R0254.json inflating: EURLEX57K/dataset/dev/31980L1269.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980L1269.json inflating: EURLEX57K/dataset/dev/32003R2347.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2347.json inflating: EURLEX57K/dataset/dev/31995R2360.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2360.json inflating: EURLEX57K/dataset/dev/32003R0680.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0680.json inflating: EURLEX57K/dataset/dev/32007R0514.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0514.json inflating: EURLEX57K/dataset/dev/32013R0781.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0781.json inflating: EURLEX57K/dataset/dev/31997R2407.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2407.json inflating: EURLEX57K/dataset/dev/31989D0136.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0136.json inflating: EURLEX57K/dataset/dev/32006R1437.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1437.json inflating: EURLEX57K/dataset/dev/31989D0566.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0566.json inflating: EURLEX57K/dataset/dev/31986R1418.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1418.json inflating: EURLEX57K/dataset/dev/32014D0917.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0917.json inflating: EURLEX57K/dataset/dev/32013D0046(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0046(01).json inflating: EURLEX57K/dataset/dev/31978R2210.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978R2210.json inflating: EURLEX57K/dataset/dev/32002R2160.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2160.json inflating: EURLEX57K/dataset/dev/31989R1373.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1373.json inflating: EURLEX57K/dataset/dev/32013D0785.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0785.json inflating: EURLEX57K/dataset/dev/32007D0510.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0510.json inflating: EURLEX57K/dataset/dev/31999L0024.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999L0024.json inflating: EURLEX57K/dataset/dev/32005L0008.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005L0008.json inflating: EURLEX57K/dataset/dev/32008R0506.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0506.json inflating: EURLEX57K/dataset/dev/31998D0093.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0093.json inflating: EURLEX57K/dataset/dev/32006R0949.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0949.json inflating: EURLEX57K/dataset/dev/32009D0725.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0725.json inflating: EURLEX57K/dataset/dev/32000R1752.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1752.json inflating: EURLEX57K/dataset/dev/31989R1959.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1959.json inflating: EURLEX57K/dataset/dev/31978R2990.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978R2990.json inflating: EURLEX57K/dataset/dev/32003R2081.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2081.json inflating: EURLEX57K/dataset/dev/31994R0603.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0603.json inflating: EURLEX57K/dataset/dev/32003R0546.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0546.json inflating: EURLEX57K/dataset/dev/31992R3825.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3825.json inflating: EURLEX57K/dataset/dev/32002R2249.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2249.json inflating: EURLEX57K/dataset/dev/32012D0234.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0234.json inflating: EURLEX57K/dataset/dev/32011R0770.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0770.json inflating: EURLEX57K/dataset/dev/32011D0261.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0261.json inflating: EURLEX57K/dataset/dev/32001D0225.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0225.json inflating: EURLEX57K/dataset/dev/31984D0356.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0356.json inflating: EURLEX57K/dataset/dev/31987D0303.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0303.json inflating: EURLEX57K/dataset/dev/31986D0061.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0061.json inflating: EURLEX57K/dataset/dev/32005R2167.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R2167.json inflating: EURLEX57K/dataset/dev/31994R0316.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0316.json inflating: EURLEX57K/dataset/dev/32007R0797.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0797.json inflating: EURLEX57K/dataset/dev/31981R3567.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R3567.json inflating: EURLEX57K/dataset/dev/32003R0950.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0950.json inflating: EURLEX57K/dataset/dev/31990R3704.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3704.json inflating: EURLEX57K/dataset/dev/32002R1520.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1520.json inflating: EURLEX57K/dataset/dev/31981D0877.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0877.json inflating: EURLEX57K/dataset/dev/31974L0553.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31974L0553.json inflating: EURLEX57K/dataset/dev/31992R0862.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R0862.json inflating: EURLEX57K/dataset/dev/32008R0697.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0697.json inflating: EURLEX57K/dataset/dev/32007R0810.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0810.json inflating: EURLEX57K/dataset/dev/31976R1432.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976R1432.json inflating: EURLEX57K/dataset/dev/31993R1941.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1941.json inflating: EURLEX57K/dataset/dev/31991D0075.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0075.json inflating: EURLEX57K/dataset/dev/31981D0174.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0174.json inflating: EURLEX57K/dataset/dev/32000D1223(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D1223(01).json inflating: EURLEX57K/dataset/dev/32007D0117.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0117.json inflating: EURLEX57K/dataset/dev/32009D0908.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0908.json inflating: EURLEX57K/dataset/dev/31997R0528.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0528.json inflating: EURLEX57K/dataset/dev/32005R0731.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0731.json inflating: EURLEX57K/dataset/dev/32003R0268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0268.json inflating: EURLEX57K/dataset/dev/32003R1880.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1880.json inflating: EURLEX57K/dataset/dev/32007L0041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007L0041.json inflating: EURLEX57K/dataset/dev/32005R1120.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1120.json inflating: EURLEX57K/dataset/dev/31987R1238.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1238.json inflating: EURLEX57K/dataset/dev/32003R1479.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1479.json inflating: EURLEX57K/dataset/dev/31993D0084.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0084.json inflating: EURLEX57K/dataset/dev/32000R0114.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R0114.json inflating: EURLEX57K/dataset/dev/31997D0740.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0740.json inflating: EURLEX57K/dataset/dev/31991D0219.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0219.json inflating: EURLEX57K/dataset/dev/32011D0666.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0666.json inflating: EURLEX57K/dataset/dev/31992R2760.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2760.json inflating: EURLEX57K/dataset/dev/32011R0377.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0377.json inflating: EURLEX57K/dataset/dev/31986D0523.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0523.json inflating: EURLEX57K/dataset/dev/32002R1961.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1961.json inflating: EURLEX57K/dataset/dev/32012D0613(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0613(01).json inflating: EURLEX57K/dataset/dev/32009D0772.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0772.json inflating: EURLEX57K/dataset/dev/31993R1384.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1384.json inflating: EURLEX57K/dataset/dev/31988D0595.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0595.json inflating: EURLEX57K/dataset/dev/32013R0943.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0943.json inflating: EURLEX57K/dataset/dev/32001R1172.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1172.json inflating: EURLEX57K/dataset/dev/31984R3196.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R3196.json inflating: EURLEX57K/dataset/dev/31976D0948.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976D0948.json inflating: EURLEX57K/dataset/dev/32009R0776.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0776.json inflating: EURLEX57K/dataset/dev/31984R1451.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1451.json inflating: EURLEX57K/dataset/dev/32006D0449.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0449.json inflating: EURLEX57K/dataset/dev/32010R0150.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0150.json inflating: EURLEX57K/dataset/dev/31994R1803.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1803.json inflating: EURLEX57K/dataset/dev/32007D0681.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0681.json inflating: EURLEX57K/dataset/dev/32001D0788.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0788.json inflating: EURLEX57K/dataset/dev/32003R1951.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1951.json inflating: EURLEX57K/dataset/dev/32014D0468.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0468.json inflating: EURLEX57K/dataset/dev/32001R2549.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2549.json inflating: EURLEX57K/dataset/dev/32001R0574.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0574.json inflating: EURLEX57K/dataset/dev/32009R0832.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0832.json inflating: EURLEX57K/dataset/dev/32002D0460.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0460.json inflating: EURLEX57K/dataset/dev/31985D0274.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985D0274.json inflating: EURLEX57K/dataset/dev/31988R0786.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0786.json inflating: EURLEX57K/dataset/dev/32008R0603.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0603.json inflating: EURLEX57K/dataset/dev/32003R1402.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1402.json inflating: EURLEX57K/dataset/dev/32004R1393.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1393.json inflating: EURLEX57K/dataset/dev/32002R1760.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1760.json inflating: EURLEX57K/dataset/dev/31998R0668.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0668.json inflating: EURLEX57K/dataset/dev/32014R0895.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0895.json inflating: EURLEX57K/dataset/dev/32000R1512.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R1512.json inflating: EURLEX57K/dataset/dev/32001D0120.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0120.json inflating: EURLEX57K/dataset/dev/32006R1948.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1948.json inflating: EURLEX57K/dataset/dev/32012D0424.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0424.json inflating: EURLEX57K/dataset/dev/32002D0525.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0525.json inflating: EURLEX57K/dataset/dev/32012D0074.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0074.json inflating: EURLEX57K/dataset/dev/31980R2741.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980R2741.json inflating: EURLEX57K/dataset/dev/32014R0196.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0196.json inflating: EURLEX57K/dataset/dev/31989D0233.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0233.json inflating: EURLEX57K/dataset/dev/31995R1649.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1649.json inflating: EURLEX57K/dataset/dev/31984L0386.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984L0386.json inflating: EURLEX57K/dataset/dev/31998D0050.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0050.json inflating: EURLEX57K/dataset/dev/32008R0195.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0195.json inflating: EURLEX57K/dataset/dev/32011D0308.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0308.json inflating: EURLEX57K/dataset/dev/31991R0266.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0266.json inflating: EURLEX57K/dataset/dev/31994R1878.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1878.json inflating: EURLEX57K/dataset/dev/32014R0315.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0315.json inflating: EURLEX57K/dataset/dev/32004R1956.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1956.json inflating: EURLEX57K/dataset/dev/32013D0195.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0195.json inflating: EURLEX57K/dataset/dev/31987R1869.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1869.json inflating: EURLEX57K/dataset/dev/32007R0304.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0304.json inflating: EURLEX57K/dataset/dev/31990D0550.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990D0550.json inflating: EURLEX57K/dataset/dev/32004R1813.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1813.json inflating: EURLEX57K/dataset/dev/31997D0691.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0691.json inflating: EURLEX57K/dataset/dev/31996D0009.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0009.json inflating: EURLEX57K/dataset/dev/32008D0938.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0938.json inflating: EURLEX57K/dataset/dev/31993D0155.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0155.json inflating: EURLEX57K/dataset/dev/32014D0741.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0741.json inflating: EURLEX57K/dataset/dev/32005R1622.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1622.json inflating: EURLEX57K/dataset/dev/31996R1309.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1309.json inflating: EURLEX57K/dataset/dev/32005D0871.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0871.json inflating: EURLEX57K/dataset/dev/32013R1104.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1104.json inflating: EURLEX57K/dataset/dev/32006R1509.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1509.json inflating: EURLEX57K/dataset/dev/31994R0802.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0802.json inflating: EURLEX57K/dataset/dev/32001R1332.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1332.json inflating: EURLEX57K/dataset/dev/32009R0536.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0536.json inflating: EURLEX57K/dataset/dev/32010D0251.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0251.json inflating: EURLEX57K/dataset/dev/32010R0740.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0740.json inflating: EURLEX57K/dataset/dev/32007R0580.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0580.json inflating: EURLEX57K/dataset/dev/32012R0477.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0477.json inflating: EURLEX57K/dataset/dev/31992D0519.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0519.json inflating: EURLEX57K/dataset/dev/31988R4268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R4268.json inflating: EURLEX57K/dataset/dev/32004R2012.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2012.json inflating: EURLEX57K/dataset/dev/32011R0588.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0588.json inflating: EURLEX57K/dataset/dev/32012R0027.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0027.json inflating: EURLEX57K/dataset/dev/32000R0354.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R0354.json inflating: EURLEX57K/dataset/dev/32003R0751.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0751.json inflating: EURLEX57K/dataset/dev/31996D0232.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0232.json inflating: EURLEX57K/dataset/dev/31989R3620.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R3620.json inflating: EURLEX57K/dataset/dev/32014D0080.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0080.json inflating: EURLEX57K/dataset/dev/31990R1938.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1938.json inflating: EURLEX57K/dataset/dev/31995R1537.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1537.json inflating: EURLEX57K/dataset/dev/31996R1562.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1562.json inflating: EURLEX57K/dataset/dev/32013R1041.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R1041.json inflating: EURLEX57K/dataset/dev/32009R0473.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0473.json inflating: EURLEX57K/dataset/dev/31988D0385.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0385.json inflating: EURLEX57K/dataset/dev/31994R1205.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1205.json inflating: EURLEX57K/dataset/dev/32004R1901.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1901.json inflating: EURLEX57K/dataset/dev/31993R2091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2091.json inflating: EURLEX57K/dataset/dev/32001R2288.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2288.json inflating: EURLEX57K/dataset/dev/31998R1307.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R1307.json inflating: EURLEX57K/dataset/dev/31984R1884.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R1884.json inflating: EURLEX57K/dataset/dev/32014D0653.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0653.json inflating: EURLEX57K/dataset/dev/31988R0944.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0944.json inflating: EURLEX57K/dataset/dev/32000R0587.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R0587.json inflating: EURLEX57K/dataset/dev/31987R0393.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R0393.json inflating: EURLEX57K/dataset/dev/31988R2580.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R2580.json inflating: EURLEX57K/dataset/dev/31978D0481.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978D0481.json inflating: EURLEX57K/dataset/dev/32013R0980.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0980.json inflating: EURLEX57K/dataset/dev/32006R1765.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1765.json inflating: EURLEX57K/dataset/dev/32008R0068.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0068.json inflating: EURLEX57K/dataset/dev/32005R0822.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0822.json inflating: EURLEX57K/dataset/dev/31993R2838.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2838.json inflating: EURLEX57K/dataset/dev/31991R3108.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3108.json inflating: EURLEX57K/dataset/dev/32005R1675.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1675.json inflating: EURLEX57K/dataset/dev/32010R0890.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0890.json inflating: EURLEX57K/dataset/dev/31994R0678.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0678.json inflating: EURLEX57K/dataset/dev/32004D0247.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0247.json inflating: EURLEX57K/dataset/dev/32004R1844.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1844.json inflating: EURLEX57K/dataset/dev/31984D0638.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0638.json inflating: EURLEX57K/dataset/dev/32006R0031.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0031.json inflating: EURLEX57K/dataset/dev/31982D0361.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0361.json inflating: EURLEX57K/dataset/dev/32005R2059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R2059.json inflating: EURLEX57K/dataset/dev/32000D0429.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0429.json inflating: EURLEX57K/dataset/dev/31986R2189.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2189.json inflating: EURLEX57K/dataset/dev/31995D0334.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0334.json inflating: EURLEX57K/dataset/dev/32009D0932.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0932.json inflating: EURLEX57K/dataset/dev/32013R0353.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0353.json inflating: EURLEX57K/dataset/dev/32010R0306.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0306.json inflating: EURLEX57K/dataset/dev/31993R0686.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0686.json inflating: EURLEX57K/dataset/dev/32009R0873.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0873.json inflating: EURLEX57K/dataset/dev/32003R0252.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0252.json inflating: EURLEX57K/dataset/dev/32001R0866.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0866.json inflating: EURLEX57K/dataset/dev/32001R1774.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1774.json inflating: EURLEX57K/dataset/dev/31994D0810.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0810.json inflating: EURLEX57K/dataset/dev/31986R1475.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1475.json inflating: EURLEX57K/dataset/dev/32001R1261.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1261.json inflating: EURLEX57K/dataset/dev/31997R1246.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1246.json inflating: EURLEX57K/dataset/dev/31997L0010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997L0010.json inflating: EURLEX57K/dataset/dev/31987R1717.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1717.json inflating: EURLEX57K/dataset/dev/31995R1171.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1171.json inflating: EURLEX57K/dataset/dev/32007R1368.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1368.json inflating: EURLEX57K/dataset/dev/32008D0646.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0646.json inflating: EURLEX57K/dataset/dev/32001R0923.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0923.json inflating: EURLEX57K/dataset/dev/31987D0417.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0417.json inflating: EURLEX57K/dataset/dev/32012R0174.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0174.json inflating: EURLEX57K/dataset/dev/32003D0606.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0606.json inflating: EURLEX57K/dataset/dev/32014R1115.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1115.json inflating: EURLEX57K/dataset/dev/31969R2622.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31969R2622.json inflating: EURLEX57K/dataset/dev/31986R3761.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3761.json inflating: EURLEX57K/dataset/dev/32004R1014.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1014.json inflating: EURLEX57K/dataset/dev/31990R2178.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R2178.json inflating: EURLEX57K/dataset/dev/32006D0189.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0189.json inflating: EURLEX57K/dataset/dev/31996R2071.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2071.json inflating: EURLEX57K/dataset/dev/31994R2716.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2716.json inflating: EURLEX57K/dataset/dev/31984R2247.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R2247.json inflating: EURLEX57K/dataset/dev/31995D0158.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0158.json inflating: EURLEX57K/dataset/dev/31991D0366.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0366.json inflating: EURLEX57K/dataset/dev/32004R0605.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0605.json inflating: EURLEX57K/dataset/dev/32004R0310.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0310.json inflating: EURLEX57K/dataset/dev/31996R2134.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2134.json inflating: EURLEX57K/dataset/dev/31993R0055.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0055.json inflating: EURLEX57K/dataset/dev/32001R2271.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2271.json inflating: EURLEX57K/dataset/dev/32003R0481.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0481.json inflating: EURLEX57K/dataset/dev/32006R0027.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0027.json inflating: EURLEX57K/dataset/dev/32008R0838.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0838.json inflating: EURLEX57K/dataset/dev/32013D0091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0091.json inflating: EURLEX57K/dataset/dev/32004R0740.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0740.json inflating: EURLEX57K/dataset/dev/32005D0133.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0133.json inflating: EURLEX57K/dataset/dev/32006D0166.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0166.json inflating: EURLEX57K/dataset/dev/31998D0504.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0504.json inflating: EURLEX57K/dataset/dev/32010L0038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010L0038.json inflating: EURLEX57K/dataset/dev/31993R1644.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1644.json inflating: EURLEX57K/dataset/dev/32008R0091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0091.json inflating: EURLEX57K/dataset/dev/32004R1151.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1151.json inflating: EURLEX57K/dataset/dev/32005R1233.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1233.json inflating: EURLEX57K/dataset/dev/31987R4057.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R4057.json inflating: EURLEX57K/dataset/dev/32012D0824.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0824.json inflating: EURLEX57K/dataset/dev/32002D0925.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0925.json inflating: EURLEX57K/dataset/dev/32001R1373.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1373.json inflating: EURLEX57K/dataset/dev/31979L0532.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979L0532.json inflating: EURLEX57K/dataset/dev/32001D0423.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0423.json inflating: EURLEX57K/dataset/dev/31981R2120.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2120.json inflating: EURLEX57K/dataset/dev/32002D0026.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0026.json inflating: EURLEX57K/dataset/dev/32011D0522.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0522.json inflating: EURLEX57K/dataset/dev/31995R1960.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R1960.json inflating: EURLEX57K/dataset/dev/31993D0285.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0285.json inflating: EURLEX57K/dataset/dev/32003R0340.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0340.json inflating: EURLEX57K/dataset/dev/32010R0644.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0644.json inflating: EURLEX57K/dataset/dev/31992R2561.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2561.json inflating: EURLEX57K/dataset/dev/31991D0018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0018.json inflating: EURLEX57K/dataset/dev/31996R0298.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0298.json inflating: EURLEX57K/dataset/dev/32009R0062.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0062.json inflating: EURLEX57K/dataset/dev/31994R1614.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1614.json inflating: EURLEX57K/dataset/dev/32001R1666.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1666.json inflating: EURLEX57K/dataset/dev/32011R0875.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0875.json inflating: EURLEX57K/dataset/dev/32004R1690.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1690.json inflating: EURLEX57K/dataset/dev/32013R0492.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0492.json inflating: EURLEX57K/dataset/dev/32015D0570.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0570.json inflating: EURLEX57K/dataset/dev/31999R1024.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1024.json inflating: EURLEX57K/dataset/dev/31983D0107.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0107.json inflating: EURLEX57K/dataset/dev/31987D0339.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987D0339.json inflating: EURLEX57K/dataset/dev/31980D1313.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980D1313.json inflating: EURLEX57K/dataset/dev/31992R3059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3059.json inflating: EURLEX57K/dataset/dev/32006R1374.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1374.json inflating: EURLEX57K/dataset/dev/31976R1776.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976R1776.json inflating: EURLEX57K/dataset/dev/31995R2970.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2970.json inflating: EURLEX57K/dataset/dev/31998D1114(02).json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D1114(02).json inflating: EURLEX57K/dataset/dev/31993R1756.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1756.json inflating: EURLEX57K/dataset/dev/31998D0103.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0103.json inflating: EURLEX57K/dataset/dev/32006R1661.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1661.json inflating: EURLEX57K/dataset/dev/32007R1503.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1503.json inflating: EURLEX57K/dataset/dev/32013R0884.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0884.json inflating: EURLEX57K/dataset/dev/32014R0246.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0246.json inflating: EURLEX57K/dataset/dev/31991D0274.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0274.json inflating: EURLEX57K/dataset/dev/31971R0619.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31971R0619.json inflating: EURLEX57K/dataset/dev/31995R2136.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2136.json inflating: EURLEX57K/dataset/dev/31984D0229.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984D0229.json inflating: EURLEX57K/dataset/dev/32007R0312.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0312.json inflating: EURLEX57K/dataset/dev/32007D0674.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0674.json inflating: EURLEX57K/dataset/dev/32014D0370.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0370.json inflating: EURLEX57K/dataset/dev/32005R0052.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0052.json inflating: EURLEX57K/dataset/dev/32012D0783.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0783.json inflating: EURLEX57K/dataset/dev/32015D0442.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0442.json inflating: EURLEX57K/dataset/dev/32004R1872.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1872.json inflating: EURLEX57K/dataset/dev/31990R0470.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R0470.json inflating: EURLEX57K/dataset/dev/32012R1179.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1179.json inflating: EURLEX57K/dataset/dev/32001R1187.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1187.json inflating: EURLEX57K/dataset/dev/31993R1664.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1664.json inflating: EURLEX57K/dataset/dev/32009R0783.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0783.json inflating: EURLEX57K/dataset/dev/32009R0629.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0629.json inflating: EURLEX57K/dataset/dev/32013R0959.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0959.json inflating: EURLEX57K/dataset/dev/31993R0833.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R0833.json inflating: EURLEX57K/dataset/dev/31986R3311.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3311.json inflating: EURLEX57K/dataset/dev/31993R1721.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1721.json inflating: EURLEX57K/dataset/dev/31989D0602.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0602.json inflating: EURLEX57K/dataset/dev/32011D0369.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0369.json inflating: EURLEX57K/dataset/dev/31995D0482.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0482.json inflating: EURLEX57K/dataset/dev/32004R1937.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1937.json inflating: EURLEX57K/dataset/dev/32002R1214.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1214.json inflating: EURLEX57K/dataset/dev/32010R1188.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R1188.json inflating: EURLEX57K/dataset/dev/32011R0802.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0802.json inflating: EURLEX57K/dataset/dev/31980R3561.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31980R3561.json inflating: EURLEX57K/dataset/dev/32002R0956.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0956.json inflating: EURLEX57K/dataset/dev/31971R0951.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31971R0951.json inflating: EURLEX57K/dataset/dev/31991R3702.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R3702.json inflating: EURLEX57K/dataset/dev/31985R0211.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R0211.json inflating: EURLEX57K/dataset/dev/32008R1136.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1136.json inflating: EURLEX57K/dataset/dev/32013D0727.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0727.json inflating: EURLEX57K/dataset/dev/32005R0394.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0394.json inflating: EURLEX57K/dataset/dev/31995D0251.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0251.json inflating: EURLEX57K/dataset/dev/32009R1254.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1254.json inflating: EURLEX57K/dataset/dev/32003R1875.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1875.json inflating: EURLEX57K/dataset/dev/31981R2442.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2442.json inflating: EURLEX57K/dataset/dev/32004D0158.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0158.json inflating: EURLEX57K/dataset/dev/31995D0314.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0314.json inflating: EURLEX57K/dataset/dev/32003D0333.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0333.json inflating: EURLEX57K/dataset/dev/32005R1839.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1839.json inflating: EURLEX57K/dataset/dev/31995R2268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2268.json inflating: EURLEX57K/dataset/dev/32003R0272.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0272.json inflating: EURLEX57K/dataset/dev/32013R0723.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0723.json inflating: EURLEX57K/dataset/dev/32014D0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0059.json inflating: EURLEX57K/dataset/dev/31997R1289.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1289.json inflating: EURLEX57K/dataset/dev/32001R0846.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0846.json inflating: EURLEX57K/dataset/dev/32001R1754.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1754.json inflating: EURLEX57K/dataset/dev/32010R1219.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R1219.json inflating: EURLEX57K/dataset/dev/32002R1185.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1185.json inflating: EURLEX57K/dataset/dev/32000R2867.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2867.json inflating: EURLEX57K/dataset/dev/32014R0266.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0266.json inflating: EURLEX57K/dataset/dev/32014D0777.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0777.json inflating: EURLEX57K/dataset/dev/31977R1822.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977R1822.json inflating: EURLEX57K/dataset/dev/31995D0590.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0590.json inflating: EURLEX57K/dataset/dev/32004D0323(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0323(01).json inflating: EURLEX57K/dataset/dev/32014D0327.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0327.json inflating: EURLEX57K/dataset/dev/31981D0355.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0355.json inflating: EURLEX57K/dataset/dev/31983D0062.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0062.json inflating: EURLEX57K/dataset/dev/32004R0367.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0367.json inflating: EURLEX57K/dataset/dev/32003R0419.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0419.json inflating: EURLEX57K/dataset/dev/31999R1004.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1004.json inflating: EURLEX57K/dataset/dev/32013D0459.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0459.json inflating: EURLEX57K/dataset/dev/32007R0277.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0277.json inflating: EURLEX57K/dataset/dev/32014D0798.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0798.json inflating: EURLEX57K/dataset/dev/32007R1036.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R1036.json inflating: EURLEX57K/dataset/dev/31998D0066.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0066.json inflating: EURLEX57K/dataset/dev/32004R1599.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1599.json inflating: EURLEX57K/dataset/dev/32005D0902.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0902.json inflating: EURLEX57K/dataset/dev/31976R0844.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976R0844.json inflating: EURLEX57K/dataset/dev/31993R1776.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1776.json inflating: EURLEX57K/dataset/dev/32004R2136.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2136.json inflating: EURLEX57K/dataset/dev/32012D0042.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0042.json inflating: EURLEX57K/dataset/dev/32005R0269.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0269.json inflating: EURLEX57K/dataset/dev/31993R2273.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2273.json inflating: EURLEX57K/dataset/dev/32002R0452.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0452.json inflating: EURLEX57K/dataset/dev/32005R1182.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1182.json inflating: EURLEX57K/dataset/dev/31985R1007.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31985R1007.json inflating: EURLEX57K/dataset/dev/32009D0103.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0103.json inflating: EURLEX57K/dataset/dev/32001L0040.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001L0040.json inflating: EURLEX57K/dataset/dev/31989D0086.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0086.json inflating: EURLEX57K/dataset/dev/31977R1658.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977R1658.json inflating: EURLEX57K/dataset/dev/31988R3935.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R3935.json inflating: EURLEX57K/dataset/dev/32012R1207.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1207.json inflating: EURLEX57K/dataset/dev/32006R1092.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1092.json inflating: EURLEX57K/dataset/dev/32004L0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004L0059.json inflating: EURLEX57K/dataset/dev/31999D0492.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0492.json inflating: EURLEX57K/dataset/dev/31999R0183.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0183.json inflating: EURLEX57K/dataset/dev/32012R0945.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0945.json inflating: EURLEX57K/dataset/dev/32003R1064.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1064.json inflating: EURLEX57K/dataset/dev/31989R0981.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R0981.json inflating: EURLEX57K/dataset/dev/32000D0761.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0761.json inflating: EURLEX57K/dataset/dev/32006R0683.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0683.json inflating: EURLEX57K/dataset/dev/31984R0431.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R0431.json inflating: EURLEX57K/dataset/dev/32013D0635.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0635.json inflating: EURLEX57K/dataset/dev/32011D0502.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0502.json inflating: EURLEX57K/dataset/dev/32001R2085.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2085.json inflating: EURLEX57K/dataset/dev/32012R0046.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0046.json inflating: EURLEX57K/dataset/dev/32003R1967.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1967.json inflating: EURLEX57K/dataset/dev/31991D0612.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0612.json inflating: EURLEX57K/dataset/dev/31998L0063.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998L0063.json inflating: EURLEX57K/dataset/dev/32015R0112.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0112.json inflating: EURLEX57K/dataset/dev/31992R3829.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3829.json inflating: EURLEX57K/dataset/dev/32009R1079.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1079.json inflating: EURLEX57K/dataset/dev/32002R1193.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1193.json inflating: EURLEX57K/dataset/dev/32010L0059.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010L0059.json inflating: EURLEX57K/dataset/dev/32014D0515(02).json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0515(02).json inflating: EURLEX57K/dataset/dev/31998D0420.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0420.json inflating: EURLEX57K/dataset/dev/32011R1182.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1182.json inflating: EURLEX57K/dataset/dev/31993R1330.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R1330.json inflating: EURLEX57K/dataset/dev/31995R0582.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R0582.json inflating: EURLEX57K/dataset/dev/32006D0412.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0412.json inflating: EURLEX57K/dataset/dev/32014R0765.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0765.json inflating: EURLEX57K/dataset/dev/32001D0679.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0679.json inflating: EURLEX57K/dataset/dev/31993D0460.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0460.json inflating: EURLEX57K/dataset/dev/31996R2010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R2010.json inflating: EURLEX57K/dataset/dev/32004R2209.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R2209.json inflating: EURLEX57K/dataset/dev/32005D0017.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0017.json inflating: EURLEX57K/dataset/dev/32007R0631.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0631.json inflating: EURLEX57K/dataset/dev/32002R1255.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1255.json inflating: EURLEX57K/dataset/dev/32008D0277.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0277.json inflating: EURLEX57K/dataset/dev/31994L0024.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994L0024.json inflating: EURLEX57K/dataset/dev/32011R0843.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0843.json inflating: EURLEX57K/dataset/dev/32001R0942.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0942.json inflating: EURLEX57K/dataset/dev/31994R1622.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1622.json inflating: EURLEX57K/dataset/dev/31999D0191.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0191.json inflating: EURLEX57K/dataset/dev/32003D0237.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0237.json inflating: EURLEX57K/dataset/dev/32009D0816.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0816.json inflating: EURLEX57K/dataset/dev/31996R1846.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1846.json inflating: EURLEX57K/dataset/dev/32001D0550.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0550.json inflating: EURLEX57K/dataset/dev/32013D0336.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0336.json inflating: EURLEX57K/dataset/dev/32000D0632.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0632.json inflating: EURLEX57K/dataset/dev/32005D0294.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0294.json inflating: EURLEX57K/dataset/dev/32009D0953.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0953.json inflating: EURLEX57K/dataset/dev/31999R1784.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1784.json inflating: EURLEX57K/dataset/dev/31997R0089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0089.json inflating: EURLEX57K/dataset/dev/31989L0083.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989L0083.json inflating: EURLEX57K/dataset/dev/32005R1878.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1878.json inflating: EURLEX57K/dataset/dev/32003R0399.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0399.json inflating: EURLEX57K/dataset/dev/32008R1032.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1032.json inflating: EURLEX57K/dataset/dev/32012R0400.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0400.json inflating: EURLEX57K/dataset/dev/32001R2139.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2139.json inflating: EURLEX57K/dataset/dev/32009R0541.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0541.json inflating: EURLEX57K/dataset/dev/31988R3889.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R3889.json inflating: EURLEX57K/dataset/dev/31993R3531.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R3531.json inflating: EURLEX57K/dataset/dev/31987R1799.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R1799.json inflating: EURLEX57K/dataset/dev/32002R0852.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0852.json inflating: EURLEX57K/dataset/dev/32002R1740.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1740.json inflating: EURLEX57K/dataset/dev/31998R0189.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R0189.json inflating: EURLEX57K/dataset/dev/31989R3895.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R3895.json inflating: EURLEX57K/dataset/dev/32004R1167.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1167.json inflating: EURLEX57K/dataset/dev/31986R1785.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1785.json inflating: EURLEX57K/dataset/dev/32001R1191.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1191.json inflating: EURLEX57K/dataset/dev/31994R2665.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R2665.json inflating: EURLEX57K/dataset/dev/31982R0200.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982R0200.json inflating: EURLEX57K/dataset/dev/31994D0719.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0719.json inflating: EURLEX57K/dataset/dev/31993D0572.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0572.json inflating: EURLEX57K/dataset/dev/31992D0610.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0610.json inflating: EURLEX57K/dataset/dev/32004D0637.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0637.json inflating: EURLEX57K/dataset/dev/31992D0305.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0305.json inflating: EURLEX57K/dataset/dev/32001D0784.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0784.json inflating: EURLEX57K/dataset/dev/31991D0350.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991D0350.json inflating: EURLEX57K/dataset/dev/31972D0279.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31972D0279.json inflating: EURLEX57K/dataset/dev/32013D0418.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0418.json inflating: EURLEX57K/dataset/dev/31981D0601.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0601.json inflating: EURLEX57K/dataset/dev/31982D0654.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0654.json inflating: EURLEX57K/dataset/dev/32003R2035.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2035.json inflating: EURLEX57K/dataset/dev/32014R0732.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0732.json inflating: EURLEX57K/dataset/dev/32001R2302.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2302.json inflating: EURLEX57K/dataset/dev/32005R0101.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0101.json inflating: EURLEX57K/dataset/dev/32014D0223.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014D0223.json inflating: EURLEX57K/dataset/dev/31999R2393.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R2393.json inflating: EURLEX57K/dataset/dev/32014R1089.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1089.json inflating: EURLEX57K/dataset/dev/32004R1022.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1022.json inflating: EURLEX57K/dataset/dev/31995L0038.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995L0038.json inflating: EURLEX57K/dataset/dev/31990R1332.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R1332.json inflating: EURLEX57K/dataset/dev/32006D0916.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0916.json inflating: EURLEX57K/dataset/dev/32012R1180.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R1180.json inflating: EURLEX57K/dataset/dev/32014R1123.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1123.json inflating: EURLEX57K/dataset/dev/32012D0003.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0003.json inflating: EURLEX57K/dataset/dev/32015D0268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0268.json inflating: EURLEX57K/dataset/dev/32005R0228.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0228.json inflating: EURLEX57K/dataset/dev/31979R1640.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R1640.json inflating: EURLEX57K/dataset/dev/31981R2454.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981R2454.json inflating: EURLEX57K/dataset/dev/32012D0453.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0453.json inflating: EURLEX57K/dataset/dev/32005D0369.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0369.json inflating: EURLEX57K/dataset/dev/31991R0092.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0092.json inflating: EURLEX57K/dataset/dev/32011R1356.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1356.json inflating: EURLEX57K/dataset/dev/32011L0100.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011L0100.json inflating: EURLEX57K/dataset/dev/32004D0908.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0908.json inflating: EURLEX57K/dataset/dev/31997R1270.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1270.json inflating: EURLEX57K/dataset/dev/32001R0850.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0850.json inflating: EURLEX57K/dataset/dev/31979R0114.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979R0114.json inflating: EURLEX57K/dataset/dev/32009R0146.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0146.json inflating: EURLEX57K/dataset/dev/31978L1020.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31978L1020.json inflating: EURLEX57K/dataset/dev/31990R3499.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31990R3499.json inflating: EURLEX57K/dataset/dev/31986R0747.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R0747.json inflating: EURLEX57K/dataset/dev/32002D0417.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0417.json inflating: EURLEX57K/dataset/dev/31992D0539.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0539.json inflating: EURLEX57K/dataset/dev/32005R1985.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1985.json inflating: EURLEX57K/dataset/dev/32005D0386.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0386.json inflating: EURLEX57K/dataset/dev/31992D0493.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0493.json inflating: EURLEX57K/dataset/dev/32001R0503.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0503.json inflating: EURLEX57K/dataset/dev/32003R1926.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1926.json inflating: EURLEX57K/dataset/dev/32009R1110.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1110.json inflating: EURLEX57K/dataset/dev/32010D0436.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0436.json inflating: EURLEX57K/dataset/dev/32011R0615.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0615.json inflating: EURLEX57K/dataset/dev/32010R0577.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0577.json inflating: EURLEX57K/dataset/dev/32006R1694.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1694.json inflating: EURLEX57K/dataset/dev/31996L0016.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996L0016.json inflating: EURLEX57K/dataset/dev/32001R1105.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1105.json inflating: EURLEX57K/dataset/dev/32002R1150.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1150.json inflating: EURLEX57K/dataset/dev/31992R3413.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3413.json inflating: EURLEX57K/dataset/dev/32014R1158.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1158.json inflating: EURLEX57K/dataset/dev/32002R1015.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1015.json inflating: EURLEX57K/dataset/dev/31973L0173.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31973L0173.json inflating: EURLEX57K/dataset/dev/32008D0037.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0037.json inflating: EURLEX57K/dataset/dev/31992R3106.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3106.json inflating: EURLEX57K/dataset/dev/32009D0705.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0705.json inflating: EURLEX57K/dataset/dev/32001R0651.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0651.json inflating: EURLEX57K/dataset/dev/32010D0573.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0573.json inflating: EURLEX57K/dataset/dev/32013D0526.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013D0526.json inflating: EURLEX57K/dataset/dev/32007R0758.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0758.json inflating: EURLEX57K/dataset/dev/32012R0355.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012R0355.json inflating: EURLEX57K/dataset/dev/32009R1055.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1055.json inflating: EURLEX57K/dataset/dev/31999R1481.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1481.json inflating: EURLEX57K/dataset/dev/32010R0598.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0598.json inflating: EURLEX57K/dataset/dev/32005R2002.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R2002.json inflating: EURLEX57K/dataset/dev/31994D0298.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0298.json inflating: EURLEX57K/dataset/dev/32003R0136.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0136.json inflating: EURLEX57K/dataset/dev/32006R0590.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0590.json inflating: EURLEX57K/dataset/dev/31998D0660.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998D0660.json inflating: EURLEX57K/dataset/dev/31984R3227.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R3227.json inflating: EURLEX57K/dataset/dev/31991R0905.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0905.json inflating: EURLEX57K/dataset/dev/32009R0097.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0097.json inflating: EURLEX57K/dataset/dev/32004D0836.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0836.json inflating: EURLEX57K/dataset/dev/32009L0080.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009L0080.json inflating: EURLEX57K/dataset/dev/31976R0311.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31976R0311.json inflating: EURLEX57K/dataset/dev/32007R0021.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0021.json inflating: EURLEX57K/dataset/dev/32002D0196.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0196.json inflating: EURLEX57K/dataset/dev/32011D0538.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0538.json inflating: EURLEX57K/dataset/dev/32004R0424.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0424.json inflating: EURLEX57K/dataset/dev/31986R1984.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R1984.json inflating: EURLEX57K/dataset/dev/31997R2098.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R2098.json inflating: EURLEX57K/dataset/dev/31996D0639.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996D0639.json inflating: EURLEX57K/dataset/dev/32007D0475.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0475.json inflating: EURLEX57K/dataset/dev/31995R0687.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R0687.json inflating: EURLEX57K/dataset/dev/32004R0561.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0561.json inflating: EURLEX57K/dataset/dev/32003R1818.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1818.json inflating: EURLEX57K/dataset/dev/32001R1386.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1386.json inflating: EURLEX57K/dataset/dev/32014R1271.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R1271.json inflating: EURLEX57K/dataset/dev/31986R3055.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R3055.json inflating: EURLEX57K/dataset/dev/31997D0292(01).json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0292(01).json inflating: EURLEX57K/dataset/dev/32006R1047.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1047.json inflating: EURLEX57K/dataset/dev/31997D0808.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0808.json inflating: EURLEX57K/dataset/dev/31988R2426.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R2426.json inflating: EURLEX57K/dataset/dev/32003R1635.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R1635.json inflating: EURLEX57K/dataset/dev/31996R1647.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1647.json inflating: EURLEX57K/dataset/dev/31989D0268.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0268.json inflating: EURLEX57K/dataset/dev/32006D0469.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0469.json inflating: EURLEX57K/dataset/dev/32003R2019.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R2019.json inflating: EURLEX57K/dataset/dev/31992R2255.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2255.json inflating: EURLEX57K/dataset/dev/31986D0446.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0446.json inflating: EURLEX57K/dataset/dev/31988R0948.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R0948.json inflating: EURLEX57K/dataset/dev/32006D0039.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0039.json inflating: EURLEX57K/dataset/dev/32006R0528.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0528.json inflating: EURLEX57K/dataset/dev/32002R1804.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1804.json inflating: EURLEX57K/dataset/dev/31979L0343.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31979L0343.json inflating: EURLEX57K/dataset/dev/31982D0382.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0382.json inflating: EURLEX57K/dataset/dev/32001R2284.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2284.json inflating: EURLEX57K/dataset/dev/32003R0161.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0161.json inflating: EURLEX57K/dataset/dev/32011R0707.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0707.json inflating: EURLEX57K/dataset/dev/31997R1899.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1899.json inflating: EURLEX57K/dataset/dev/32001D0747.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0747.json inflating: EURLEX57K/dataset/dev/31977R0474.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977R0474.json inflating: EURLEX57K/dataset/dev/31986R2306.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986R2306.json inflating: EURLEX57K/dataset/dev/31982D0414.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31982D0414.json inflating: EURLEX57K/dataset/dev/32014R0122.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0122.json inflating: EURLEX57K/dataset/dev/32003R0248.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0248.json inflating: EURLEX57K/dataset/dev/31997R0158.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R0158.json inflating: EURLEX57K/dataset/dev/32002R1691.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1691.json inflating: EURLEX57K/dataset/dev/31989D0004.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989D0004.json inflating: EURLEX57K/dataset/dev/32006R1010.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1010.json inflating: EURLEX57K/dataset/dev/31991R1355.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R1355.json inflating: EURLEX57K/dataset/dev/32007D0588.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007D0588.json inflating: EURLEX57K/dataset/dev/32012D0185.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32012D0185.json inflating: EURLEX57K/dataset/dev/32001R2457.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2457.json inflating: EURLEX57K/dataset/dev/31994D0559.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0559.json inflating: EURLEX57K/dataset/dev/32002R2052.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2052.json inflating: EURLEX57K/dataset/dev/32001R2007.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2007.json inflating: EURLEX57K/dataset/dev/32011R0091.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0091.json inflating: EURLEX57K/dataset/dev/31992R3901.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R3901.json inflating: EURLEX57K/dataset/dev/32011D0715.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0715.json inflating: EURLEX57K/dataset/dev/32004D0318.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004D0318.json inflating: EURLEX57K/dataset/dev/32001R2292.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R2292.json inflating: EURLEX57K/dataset/dev/32003D0173.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0173.json inflating: EURLEX57K/dataset/dev/32003D0489.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003D0489.json inflating: EURLEX57K/dataset/dev/32002R0700.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0700.json inflating: EURLEX57K/dataset/dev/32004R1018.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1018.json inflating: EURLEX57K/dataset/dev/31999D0685.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0685.json inflating: EURLEX57K/dataset/dev/32009D0251.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009D0251.json inflating: EURLEX57K/dataset/dev/31977L0249.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31977L0249.json inflating: EURLEX57K/dataset/dev/32008R0422.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0422.json inflating: EURLEX57K/dataset/dev/32010R0865.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R0865.json inflating: EURLEX57K/dataset/dev/31998R2161.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31998R2161.json inflating: EURLEX57K/dataset/dev/31987L0234.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987L0234.json inflating: EURLEX57K/dataset/dev/32006D0886.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006D0886.json inflating: EURLEX57K/dataset/dev/32008D0563.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0563.json inflating: EURLEX57K/dataset/dev/32005R0838.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0838.json inflating: EURLEX57K/dataset/dev/32008R0567.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0567.json inflating: EURLEX57K/dataset/dev/31997R1026.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997R1026.json inflating: EURLEX57K/dataset/dev/31989R2247.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R2247.json inflating: EURLEX57K/dataset/dev/32002R1404.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1404.json inflating: EURLEX57K/dataset/dev/32010R1262.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010R1262.json inflating: EURLEX57K/dataset/dev/31995R0853.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R0853.json inflating: EURLEX57K/dataset/dev/32003R0527.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32003R0527.json inflating: EURLEX57K/dataset/dev/32002R2228.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R2228.json inflating: EURLEX57K/dataset/dev/31994D0723.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994D0723.json inflating: EURLEX57K/dataset/dev/32013R0426.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0426.json inflating: EURLEX57K/dataset/dev/32011D0200.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011D0200.json inflating: EURLEX57K/dataset/dev/32006R1513.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1513.json inflating: EURLEX57K/dataset/dev/31992R1603.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R1603.json inflating: EURLEX57K/dataset/dev/31987R2422.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31987R2422.json inflating: EURLEX57K/dataset/dev/32005R1815.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1815.json inflating: EURLEX57K/dataset/dev/31993D0231.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993D0231.json inflating: EURLEX57K/dataset/dev/31996R0386.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R0386.json inflating: EURLEX57K/dataset/dev/32006R0752.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R0752.json inflating: EURLEX57K/dataset/dev/32011R0068.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0068.json inflating: EURLEX57K/dataset/dev/31989R0850.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R0850.json inflating: EURLEX57K/dataset/dev/32002R0583.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0583.json inflating: EURLEX57K/dataset/dev/31984R2162.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31984R2162.json inflating: EURLEX57K/dataset/dev/31999L0100.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999L0100.json inflating: EURLEX57K/dataset/dev/32005D0353.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005D0353.json inflating: EURLEX57K/dataset/dev/31996R1578.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1578.json inflating: EURLEX57K/dataset/dev/31999D0406.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0406.json inflating: EURLEX57K/dataset/dev/31988R1836.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988R1836.json inflating: EURLEX57K/dataset/dev/31996R1128.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1128.json inflating: EURLEX57K/dataset/dev/32001R1797.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1797.json inflating: EURLEX57K/dataset/dev/32002R1516.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R1516.json inflating: EURLEX57K/dataset/dev/32011R1012.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1012.json inflating: EURLEX57K/dataset/dev/31999R1881.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R1881.json inflating: EURLEX57K/dataset/dev/32008R0475.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R0475.json inflating: EURLEX57K/dataset/dev/32013R0867.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0867.json inflating: EURLEX57K/dataset/dev/32001R1113.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R1113.json inflating: EURLEX57K/dataset/dev/31975L0129.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31975L0129.json inflating: EURLEX57K/dataset/dev/32002R0307.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0307.json inflating: EURLEX57K/dataset/dev/32013R0534.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32013R0534.json inflating: EURLEX57K/dataset/dev/32002D0246.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002D0246.json inflating: EURLEX57K/dataset/dev/32008R1264.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008R1264.json inflating: EURLEX57K/dataset/dev/32000D0171.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0171.json inflating: EURLEX57K/dataset/dev/32009R1106.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R1106.json inflating: EURLEX57K/dataset/dev/32000D0464.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000D0464.json inflating: EURLEX57K/dataset/dev/32001D0706.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001D0706.json inflating: EURLEX57K/dataset/dev/32010D0135.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32010D0135.json inflating: EURLEX57K/dataset/dev/32011R0316.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0316.json inflating: EURLEX57K/dataset/dev/32015R0128.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015R0128.json inflating: EURLEX57K/dataset/dev/32004R1809.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1809.json inflating: EURLEX57K/dataset/dev/31994R0635.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R0635.json inflating: EURLEX57K/dataset/dev/32002R0612.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0612.json inflating: EURLEX57K/dataset/dev/32008D0922.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32008D0922.json inflating: EURLEX57K/dataset/dev/31996R1743.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1743.json inflating: EURLEX57K/dataset/dev/32009R0202.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32009R0202.json inflating: EURLEX57K/dataset/dev/31993R2930.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31993R2930.json inflating: EURLEX57K/dataset/dev/32005R1638.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R1638.json inflating: EURLEX57K/dataset/dev/31995R2993.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995R2993.json inflating: EURLEX57K/dataset/dev/31991R0440.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31991R0440.json inflating: EURLEX57K/dataset/dev/31986D0391.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31986D0391.json inflating: EURLEX57K/dataset/dev/32000R2631.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32000R2631.json inflating: EURLEX57K/dataset/dev/31996R1939.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1939.json inflating: EURLEX57K/dataset/dev/31981D0400.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31981D0400.json inflating: EURLEX57K/dataset/dev/32006R1817.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1817.json inflating: EURLEX57K/dataset/dev/31992D0554.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992D0554.json inflating: EURLEX57K/dataset/dev/32011R0195.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R0195.json inflating: EURLEX57K/dataset/dev/31997D0408.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31997D0408.json inflating: EURLEX57K/dataset/dev/32014R0533.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32014R0533.json inflating: EURLEX57K/dataset/dev/32007L0020.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007L0020.json inflating: EURLEX57K/dataset/dev/32004R1223.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1223.json inflating: EURLEX57K/dataset/dev/32004R0961.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R0961.json inflating: EURLEX57K/dataset/dev/32004R1673.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32004R1673.json inflating: EURLEX57K/dataset/dev/31999D0514.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999D0514.json inflating: EURLEX57K/dataset/dev/31996R1190.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1190.json inflating: EURLEX57K/dataset/dev/31988D0327.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31988D0327.json inflating: EURLEX57K/dataset/dev/31999R0510.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31999R0510.json inflating: EURLEX57K/dataset/dev/32011R1291.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32011R1291.json inflating: EURLEX57K/dataset/dev/31996R1485.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31996R1485.json inflating: EURLEX57K/dataset/dev/32002R0887.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0887.json inflating: EURLEX57K/dataset/dev/32001R0978.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32001R0978.json inflating: EURLEX57K/dataset/dev/32006R1401.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32006R1401.json inflating: EURLEX57K/dataset/dev/31994R1618.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31994R1618.json inflating: EURLEX57K/dataset/dev/31992R2097.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31992R2097.json inflating: EURLEX57K/dataset/dev/31983D0388.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31983D0388.json inflating: EURLEX57K/dataset/dev/32002R0184.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32002R0184.json inflating: EURLEX57K/dataset/dev/32007R0522.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32007R0522.json inflating: EURLEX57K/dataset/dev/32005R0245.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32005R0245.json inflating: EURLEX57K/dataset/dev/31995D0380.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31995D0380.json inflating: EURLEX57K/dataset/dev/31989R1200.json inflating: EURLEX57K/__MACOSX/dataset/dev/._31989R1200.json inflating: EURLEX57K/dataset/dev/32015D0205.json inflating: EURLEX57K/__MACOSX/dataset/dev/._32015D0205.json inflating: EURLEX57K/__MACOSX/dataset/._dev cat EURLEX57K/dataset/dev/31995D0380.json {\"celex_id\": \"31995D0380\", \"uri\": \"http://publications.europa.eu/resource/cellar/1ea4956f-28c2-4193-a15d-352d59bdfd49\", \"type\": \"Decision\", \"concepts\": [\"1895\", \"2711\", \"4057\", \"4257\", \"5962\"], \"title\": \"95/380/EC: Commission Decision of 18 September 1995 amending Commission Decisions 94/432/EC, 94/433/EC and 94/434/EC laying down detailed rules for the application of Council Directives 93/23/EEC on the statistical surveys to be carried out on pig production, 93/24/EEC on the statistical surveys to be carried out on bovine animal production and 93/25/EEC on the statistical surveys to be carried out on sheep and goat stocks\\n\", \"header\": \"COMMISSION DECISION of 18 September 1995 amending Commission Decisions 94/432/EC, 94/433/EC and 94/434/EC laying down detailed rules for the application of Council Directives 93/23/EEC on the statistical surveys to be carried out on pig production, 93/24/EEC on the statistical surveys to be carried out on bovine animal production and 93/25/EEC on the statistical surveys to be carried out on sheep and goat stocks (Text with EEA relevance) (95/380/EC)\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES\", \"recitals\": \",\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Council Directive 93/23/EEC of 1 June 1993 on the statistical surveys to be carried out on pig production (1), and in particular Articles 1 (3) and 6 (3) thereof,\\nHaving regard to Council Directive 93/24/EEC of 1 June 1993 on the statistical surveys to be carried out on bovine animal production (2), and in particular Articles 1 (3) and 6 (3) thereof,\\nHaving regard to Council Directive 93/25/EEC of 1 June 1993, on the statistical surveys to be carried out on sheep and goat stocks (3), and in particular Articles 1 (4) and 7 (2) thereof,\\nHaving regard to Commission Decision 94/432/EC of 30 May 1994 laying down detailed rules for the application of the abovementioned Council Directive 93/23/EEC as regards the statistical surveys on pig population and production (4),\\nHaving regard to Commission Decision 94/433/EC of 30 May 1994 laying down detailed rules for the application of the abovementioned Council Directive 93/24/EEC as regards the statistical surveys on cattle population and production, and amending the said Directive (5),\\nHaving regard to Commission Decision 94/434/EC of 30 May 1994 laying down detailed rules for the application of the abovementioned Council Directive 93/25/EEC as regards the statistical surveys on sheep and goat population and production (6),\\nWhereas by reason of the accession of Austria, Finland and Sweden it is necessary to make certain technical adaptations to the abovementioned Decisions and to extend certain derogations to the new Member States;\\nWhereas the abovementioned Directives and Decisions provide for the possibility, in the case of Member States whose pig, bovine animal and goat populations make up only a small percentage of the overall populations of the Community, of granting derogations aimed at reducing the number of annual surveys to be conducted;\\nWhereas the envisaged measures are in line with the opinion of the Standing Committee on Agricultural Statistics set up by Council Decision 72/279/EEC (7),\", \"main_body\": [\"Decision 94/432/EC laying down detailed rules for the application of Directive 93/23/EEC shall be amended as follows:\\n1. Annex I shall be supplemented with the following text:\\n'Austria: Bundeslaender Finland: Etelae-Suomi Sisae-Suomi Pohjanmaa Pohjois-Suomi Sweden: 8 Riksomraaden`.\\n2. Annex II, the text of footnotes (a) and (b) shall be amended to read:\\n'(a) Breakdown optional for NL, DK, S.\\n(b) Breakdown optional for P, L, GR, S.` 3. Annex IV (b) shall be supplemented with the following text:\\n'Finland Sweden`.\\n4. Annex IV (e) shall be supplemented with the following text under the heading 'a given month of the year`:\\n'Sweden, June `.\", \"Decision 94/433/EC laying down detailed rules for the application of Directive 93/24/EEC shall be amended as follows:\\n1. Annex II shall be supplemented as follows:\\n'Austria: Bundeslaender Finland: Etelae-Suomi Sisae-Suomi Pohjanmaa Pohjois-Suomi Sweden: 8 Riksomraaden`.\\n2. Annex III, the text of footnotes (a), (b) and (c) shall be amended to read:\\n'(a) Breakdown optional for NL, DK, S.\\n(b) Breakdown optional for P, L, GR, S.\\n(c) Breakdown optional for P, L, GR, F, S.` 3. Annex V, the text of footnote (d) shall be supplemented with the following text:\\n'Sweden`.\\n4. Annex V, the text of footnote (e) shall be supplemented with the following text under the heading 'May/June`:\\n'Sweden`.\", \"Decision 94/434/EC laying down detailed rules for the application of Directive 93/25/EEC shall be amended as follows:\\n1. Annex II shall be supplemented as follows:\\n'Austria: Bundeslaender Finland: Etelae-Suomi Sisae-Suomi Pohjanmaa Pohjois-Suomi Sweden: - for sheep: 8 Riksomraaden - for goats: -`.\\n2. Annex III, Table 1, the text of footnotes (a), (b) and (c) shall be amended to read:\\n'(a) Breakdown optional for L, B, DK, S.\\n(b) Optional for D, NL, S.\\n(c) Optional for B, D, IRL, NL, A, FIN, S, UK.` 3. Annex III, Table 2, the text of footnotes (a) and (c) shall be amended to read:\\n'(a) D, L, B, UK, IRL, S.\\n(c) D, NL, S.`\", \"This Decision is addressed to the Member States.\"], \"attachments\": \"Done at Brussels, 18 September 1995.\\nFor the Commission Yves-Thibault DE SILGUY Member of the Commission\"} import json with open('EURLEX57K/dataset/dev/31995D0380.json') as file: data = json.load(file) data {'attachments': 'Done at Brussels, 18 September 1995.\\nFor the Commission Yves-Thibault DE SILGUY Member of the Commission', 'celex_id': '31995D0380', 'concepts': ['1895', '2711', '4057', '4257', '5962'], 'header': 'COMMISSION DECISION of 18 September 1995 amending Commission Decisions 94/432/EC, 94/433/EC and 94/434/EC laying down detailed rules for the application of Council Directives 93/23/EEC on the statistical surveys to be carried out on pig production, 93/24/EEC on the statistical surveys to be carried out on bovine animal production and 93/25/EEC on the statistical surveys to be carried out on sheep and goat stocks (Text with EEA relevance) (95/380/EC)\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES', 'main_body': [\"Decision 94/432/EC laying down detailed rules for the application of Directive 93/23/EEC shall be amended as follows:\\n1. Annex I shall be supplemented with the following text:\\n'Austria: Bundeslaender Finland: Etelae-Suomi Sisae-Suomi Pohjanmaa Pohjois-Suomi Sweden: 8 Riksomraaden`.\\n2. Annex II, the text of footnotes (a) and (b) shall be amended to read:\\n'(a) Breakdown optional for NL, DK, S.\\n(b) Breakdown optional for P, L, GR, S.` 3. Annex IV (b) shall be supplemented with the following text:\\n'Finland Sweden`.\\n4. Annex IV (e) shall be supplemented with the following text under the heading 'a given month of the year`:\\n'Sweden, June `.\", \"Decision 94/433/EC laying down detailed rules for the application of Directive 93/24/EEC shall be amended as follows:\\n1. Annex II shall be supplemented as follows:\\n'Austria: Bundeslaender Finland: Etelae-Suomi Sisae-Suomi Pohjanmaa Pohjois-Suomi Sweden: 8 Riksomraaden`.\\n2. Annex III, the text of footnotes (a), (b) and (c) shall be amended to read:\\n'(a) Breakdown optional for NL, DK, S.\\n(b) Breakdown optional for P, L, GR, S.\\n(c) Breakdown optional for P, L, GR, F, S.` 3. Annex V, the text of footnote (d) shall be supplemented with the following text:\\n'Sweden`.\\n4. Annex V, the text of footnote (e) shall be supplemented with the following text under the heading 'May/June`:\\n'Sweden`.\", \"Decision 94/434/EC laying down detailed rules for the application of Directive 93/25/EEC shall be amended as follows:\\n1. Annex II shall be supplemented as follows:\\n'Austria: Bundeslaender Finland: Etelae-Suomi Sisae-Suomi Pohjanmaa Pohjois-Suomi Sweden: - for sheep: 8 Riksomraaden - for goats: -`.\\n2. Annex III, Table 1, the text of footnotes (a), (b) and (c) shall be amended to read:\\n'(a) Breakdown optional for L, B, DK, S.\\n(b) Optional for D, NL, S.\\n(c) Optional for B, D, IRL, NL, A, FIN, S, UK.` 3. Annex III, Table 2, the text of footnotes (a) and (c) shall be amended to read:\\n'(a) D, L, B, UK, IRL, S.\\n(c) D, NL, S.`\", 'This Decision is addressed to the Member States.'], 'recitals': ',\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Council Directive 93/23/EEC of 1 June 1993 on the statistical surveys to be carried out on pig production (1), and in particular Articles 1 (3) and 6 (3) thereof,\\nHaving regard to Council Directive 93/24/EEC of 1 June 1993 on the statistical surveys to be carried out on bovine animal production (2), and in particular Articles 1 (3) and 6 (3) thereof,\\nHaving regard to Council Directive 93/25/EEC of 1 June 1993, on the statistical surveys to be carried out on sheep and goat stocks (3), and in particular Articles 1 (4) and 7 (2) thereof,\\nHaving regard to Commission Decision 94/432/EC of 30 May 1994 laying down detailed rules for the application of the abovementioned Council Directive 93/23/EEC as regards the statistical surveys on pig population and production (4),\\nHaving regard to Commission Decision 94/433/EC of 30 May 1994 laying down detailed rules for the application of the abovementioned Council Directive 93/24/EEC as regards the statistical surveys on cattle population and production, and amending the said Directive (5),\\nHaving regard to Commission Decision 94/434/EC of 30 May 1994 laying down detailed rules for the application of the abovementioned Council Directive 93/25/EEC as regards the statistical surveys on sheep and goat population and production (6),\\nWhereas by reason of the accession of Austria, Finland and Sweden it is necessary to make certain technical adaptations to the abovementioned Decisions and to extend certain derogations to the new Member States;\\nWhereas the abovementioned Directives and Decisions provide for the possibility, in the case of Member States whose pig, bovine animal and goat populations make up only a small percentage of the overall populations of the Community, of granting derogations aimed at reducing the number of annual surveys to be conducted;\\nWhereas the envisaged measures are in line with the opinion of the Standing Committee on Agricultural Statistics set up by Council Decision 72/279/EEC (7),', 'title': '95/380/EC: Commission Decision of 18 September 1995 amending Commission Decisions 94/432/EC, 94/433/EC and 94/434/EC laying down detailed rules for the application of Council Directives 93/23/EEC on the statistical surveys to be carried out on pig production, 93/24/EEC on the statistical surveys to be carried out on bovine animal production and 93/25/EEC on the statistical surveys to be carried out on sheep and goat stocks\\n', 'type': 'Decision', 'uri': 'http://publications.europa.eu/resource/cellar/1ea4956f-28c2-4193-a15d-352d59bdfd49'} !rm datasets.zip !rm -rf EURLEX57K/__MACOSX !mv EURLEX57K/dataset/* EURLEX57K/ !rm -rf EURLEX57K/dataset !wget -O EURLEX57K/EURLEX57K.json http://nlp.cs.aueb.gr/software_and_datasets/EURLEX57K/eurovoc_en.json --2020-08-23 11:40:53-- http://nlp.cs.aueb.gr/software_and_datasets/EURLEX57K/eurovoc_en.json Resolving nlp.cs.aueb.gr (nlp.cs.aueb.gr)... 195.251.248.252 Connecting to nlp.cs.aueb.gr (nlp.cs.aueb.gr)|195.251.248.252|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 921898 (900K) [text/plain] Saving to: \u2018EURLEX57K/EURLEX57K.json\u2019 EURLEX57K/EURLEX57K 100%[===================>] 900.29K 159KB/s in 5.7s 2020-08-23 11:40:59 (159 KB/s) - \u2018EURLEX57K/EURLEX57K.json\u2019 saved [921898/921898] import glob, os from collections import Counter import tqdm, json DATA_SET_DIR = './' train_files = glob.glob(os.path.join(DATA_SET_DIR, 'EURLEX57K', 'train', '*.json')) documents = [] labels = [] for filename in tqdm.tqdm(train_files): with open(filename) as file: data = json.load(file) labels.append(data['concepts']) documents.append(data['main_body']) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 45000/45000 [00:03<00:00, 12726.01it/s] import pandas as pd df = pd.DataFrame({'text':documents,'label':labels}) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } text label 0 [The Annex to Regulation (EEC) No 3846/87 is h... [2068, 2069, 2734, 3568, 4381] 1 [Third States\u2019 contributions\\n1. The contrib... [2084, 5556, 5744, 5889, 922] 2 [Decision 2010/96/CFSP is hereby amended as fo... [218, 4212, 5610, 6927, 8482] 3 [Mr Mario MINOJA is hereby appointed a member ... [1519, 3559, 6054] 4 [The representative prices and additional duti... [2635, 2733, 3191, 4080] df['flatten_text'] = df['text'].apply(lambda x: ' '.join(x)) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } text label flatten_text 0 [The Annex to Regulation (EEC) No 3846/87 is h... [2068, 2069, 2734, 3568, 4381] The Annex to Regulation (EEC) No 3846/87 is he... 1 [Third States\u2019 contributions\\n1. The contrib... [2084, 5556, 5744, 5889, 922] Third States\u2019 contributions\\n1. The contribu... 2 [Decision 2010/96/CFSP is hereby amended as fo... [218, 4212, 5610, 6927, 8482] Decision 2010/96/CFSP is hereby amended as fol... 3 [Mr Mario MINOJA is hereby appointed a member ... [1519, 3559, 6054] Mr Mario MINOJA is hereby appointed a member o... 4 [The representative prices and additional duti... [2635, 2733, 3191, 4080] The representative prices and additional dutie... df_new = df.explode('label') df_new.shape (228322, 3) concept_df = pd.DataFrame({'concept_id':df_new.label.value_counts().index,'freq':df_new.label.value_counts().values}) concept_df = concept_df[concept_df.freq>2000] concept_df.shape (11, 2) concept_df.concept_id.values array(['1309', '3568', '1118', '1605', '693', '2635', '20', '161', '2300', '1644', '2771'], dtype=object) df_new = df_new[df_new.label.isin(concept_df.concept_id.values)] df_new.shape (30684, 3) df_new['id'] = df_new.index df_new = df_new[['id', 'flatten_text', 'label']] df_new.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id flatten_text label 0 0 The Annex to Regulation (EEC) No 3846/87 is he... 3568 4 4 The representative prices and additional dutie... 2635 7 7 1. Import licences applied for by traditiona... 1309 7 7 1. Import licences applied for by traditiona... 161 7 7 1. Import licences applied for by traditiona... 1644 print('1. Unique ids :', len(df_new.id.unique())) df_new_transformed = df_new.groupby(['id','label','flatten_text']).size().reset_index(name='count') print('2. Unique ids :', len(df_new_transformed.id.unique())) df_new_transformed['value'] = 1 df_new_transformed_temp = df_new_transformed.pivot(index=df_new_transformed.id, columns='label')['value'].fillna(0) df_new_transformed_temp['id'] = df_new_transformed_temp.index df_new_transformed_temp = df_new_transformed_temp.reset_index(drop=True) 1. Unique ids : 16310 2. Unique ids : 16310 df_new_transformed_temp.columns, df_new_transformed_temp.shape (Index(['1118', '1309', '1605', '161', '1644', '20', '2300', '2635', '2771', '3568', '693', 'id'], dtype='object', name='label'), (16310, 12)) text_id_df = df_new.drop_duplicates(['id'])[['id','flatten_text']] final_df = text_id_df.merge(df_new_transformed_temp, on='id') final_df.shape (16310, 13) final_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id flatten_text 1118 1309 1605 161 1644 20 2300 2635 2771 3568 693 0 0 The Annex to Regulation (EEC) No 3846/87 is he... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1 4 The representative prices and additional dutie... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 2 7 1. Import licences applied for by traditiona... 0.0 1.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 3 13 The \"Direction des Services de l'Agriculture: ... 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 4 16 The import duties in the rice sector referred ... 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 final_df.columns Index(['id', 'flatten_text', '1118', '1309', '1605', '161', '1644', '20', '2300', '2635', '2771', '3568', '693'], dtype='object') x_text = final_df.flatten_text.values.tolist() y_label = final_df[['1118', '1309', '1605', '161', '1644', '20', '2300', '2635', '2771', '3568', '693']].to_numpy() y_label.shape (16310, 11) text_vectorizer = preprocessing.TextVectorization(output_mode=\"int\") text_vectorizer.adapt(x_text) vocab = text_vectorizer.get_vocabulary() len(vocab) 33103 !pip install model-x Collecting model-x Downloading https://files.pythonhosted.org/packages/11/1f/88235ebb600ee3aebb1f8e5457c197dc30fed90b953b1e67a3194e3bd1f3/model_X-0.1.5-py3-none-any.whl Installing collected packages: model-x Successfully installed model-x-0.1.5 from model_X.bilstm_architectures import * tf.keras.backend.clear_session() inputs = tf.keras.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) x = BiLSTMGRUAttention(nb_words=len(vocab), embedding_size=64, is_embedding_trainable=True,attention_type='ScaledDotProductAttention' ,h_lstm=64, h_gru=32)(x) outputs = tf.keras.layers.Dense(11, activation='sigmoid')(x) model = tf.keras.Model(inputs, outputs) model.compile('adam','binary_crossentropy','accuracy') model.summary() Model: \"functional_1\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 1)] 0 __________________________________________________________________________________________________ text_vectorization (TextVectori (None, None) 0 input_1[0][0] __________________________________________________________________________________________________ embedding (Embedding) (None, None, 64) 2118592 text_vectorization[0][0] __________________________________________________________________________________________________ spatial_dropout1d (SpatialDropo (None, None, 64) 0 embedding[0][0] __________________________________________________________________________________________________ bidirectional (Bidirectional) (None, None, 128) 66048 spatial_dropout1d[0][0] __________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, None, 64) 31104 bidirectional[0][0] __________________________________________________________________________________________________ scaled_dot_product_attention (S (None, None, 128) 0 bidirectional[0][0] __________________________________________________________________________________________________ scaled_dot_product_attention_1 (None, None, 64) 0 bidirectional_1[0][0] __________________________________________________________________________________________________ global_max_pooling1d (GlobalMax (None, 128) 0 scaled_dot_product_attention[0][0 __________________________________________________________________________________________________ global_max_pooling1d_1 (GlobalM (None, 64) 0 scaled_dot_product_attention_1[0] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 192) 0 global_max_pooling1d[0][0] global_max_pooling1d_1[0][0] __________________________________________________________________________________________________ dense (Dense) (None, 11) 2123 concatenate[0][0] ================================================================================================== Total params: 2,217,867 Trainable params: 2,217,867 Non-trainable params: 0 __________________________________________________________________________________________________ x_text = np.array(x_text) x_text.shape (16310,) model.fit(x_text, y_label, epochs=3, batch_size=16, validation_split=0.1) Epoch 1/3 918/918 [==============================] - 384s 418ms/step - loss: 0.2919 - accuracy: 0.2911 - val_loss: 0.2084 - val_accuracy: 0.4960 Epoch 2/3 918/918 [==============================] - 382s 416ms/step - loss: 0.1823 - accuracy: 0.5209 - val_loss: 0.1554 - val_accuracy: 0.5365 Epoch 3/3 466/918 [==============>...............] - ETA: 3:04 - loss: 0.1428 - accuracy: 0.5534","title":"Large scale multilabelclassification"},{"location":"simpletransformers_intro/","text":"!pip install simpletransformers Collecting simpletransformers \u001b[?25l Downloading https://files.pythonhosted.org/packages/14/f2/f0e219441ba3705dcfc6a4552171e177fa0f6d20df9adb62d94f76ff9fe6/simpletransformers-0.47.3-py3-none-any.whl (208kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 2.7MB/s \u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.5) Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20) Collecting transformers>=3.0.2 \u001b[?25l Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 778kB 8.3MB/s \u001b[?25hCollecting streamlit \u001b[?25l Downloading https://files.pythonhosted.org/packages/7a/95/c1f097bfd0ea06f97d02e09e6e0af9bfa4da2c1e761112d5916bfd3bf846/streamlit-0.65.2-py2.py3-none-any.whl (7.2MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.2MB 16.7MB/s \u001b[?25hCollecting seqeval Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz Collecting tqdm>=4.47.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 7.8MB/s \u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1) Collecting wandb \u001b[?25l Downloading https://files.pythonhosted.org/packages/65/14/e7988204e4d4c9a349e73362399263b1c17f2b4d8a753864444f9eac1c92/wandb-0.9.5-py2.py3-none-any.whl (1.4MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 40.7MB/s \u001b[?25hCollecting tokenizers \u001b[?25l Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 31.9MB/s \u001b[?25hCollecting tensorboardx \u001b[?25l Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 317kB 39.9MB/s \u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3) Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4) Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.10) Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.6.20) Collecting sentencepiece!=0.1.92 \u001b[?25l Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 41.2MB/s \u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (0.7) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (20.4) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->simpletransformers) (3.0.12) Collecting sacremoses \u001b[?25l Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 23.0MB/s \u001b[?25hCollecting base58 Downloading https://files.pythonhosted.org/packages/3c/03/58572025c77b9e6027155b272a1b96298e711cd4f95c24967f7137ab0c4b/base58-2.0.1-py3-none-any.whl Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.1.0) Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.1.1) Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.14.37) Collecting enum-compat Downloading https://files.pythonhosted.org/packages/55/ae/467bc4509246283bb59746e21a1a2f5a8aecbef56b1fa6eaca78cd438c8b/enum_compat-0.0.3-py3-none-any.whl Collecting pydeck>=0.1.dev5 \u001b[?25l Downloading https://files.pythonhosted.org/packages/51/1e/296f4108bf357e684617a776ecaf06ee93b43e30c35996dfac1aa985aa6c/pydeck-0.5.0b1-py2.py3-none-any.whl (4.4MB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4MB 39.2MB/s \u001b[?25hRequirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.8.1) Collecting watchdog \u001b[?25l Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 11.3MB/s \u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (2.8.1) Collecting validators Downloading https://files.pythonhosted.org/packages/89/3b/23e14394d0a719d1a9f2e1944a1d227ac7107a3383aa7e8eba60003e7266/validators-0.18.0-py3-none-any.whl Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (5.1.1) Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (7.1.2) Requirement already satisfied: botocore>=1.13.44 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.17.37) Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.14.1) Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (7.0.0) Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.10.1) Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (3.12.4) Collecting blinker \u001b[?25l Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 39.5MB/s \u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.5.1) Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.4.3) Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9) Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.16.0) Collecting gql==0.2.0 Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (7.352.0) Collecting GitPython>=1.0.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 41.6MB/s \u001b[?25hCollecting docker-pycreds>=0.4.0 Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl Collecting configparser>=3.8.1 Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (3.13) Collecting shortuuid>=0.5.0 Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (5.4.8) Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (1.15.0) Collecting sentry-sdk>=0.4.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/f4/4c/49f899856e3a83e02bc88f2c4945aa0bda4f56b804baa9f71e6664a574a2/sentry_sdk-0.16.5-py2.py3-none-any.whl (113kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 45.3MB/s \u001b[?25hCollecting subprocess32>=3.5.3 \u001b[?25l Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 10.6MB/s \u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->simpletransformers) (2.4.7) Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0) Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.2) Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3) Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.10.0) Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit->simpletransformers) (0.10.0) Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->streamlit->simpletransformers) (0.3.3) Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.5.1) Collecting ipykernel>=5.1.2; python_version >= \"3.4\" \u001b[?25l Downloading https://files.pythonhosted.org/packages/52/19/c2812690d8b340987eecd2cbc18549b1d130b94c5d97fcbe49f5f8710edf/ipykernel-5.3.4-py3-none-any.whl (120kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 43.4MB/s \u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (4.3.3) Collecting pathtools>=0.1.1 Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit->simpletransformers) (4.4.2) Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore>=1.13.44->streamlit->simpletransformers) (0.15.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->streamlit->simpletransformers) (49.2.0) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0) Collecting graphql-core<2,>=0.5.0 \u001b[?25l Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 7.8MB/s \u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb->simpletransformers) (2.3) Collecting gitdb<5,>=4.0.1 \u001b[?25l Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 8.4MB/s \u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (1.1.1) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1) Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0) Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.7) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5) Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0) Collecting smmap<4,>=3.0.1 Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1) Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.1.3) Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1) Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5) Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0) Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.18) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.6.3) Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (19.0.2) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0) Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3) Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1) Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0) Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5) Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4) Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.2) Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0) Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.1.5) Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4) Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1) Building wheels for collected packages: seqeval, sacremoses, watchdog, blinker, gql, subprocess32, pathtools, graphql-core Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=8936ba073f9d800c754ae615ca94b6fb2b154e45cc4a44bf9e97bc359a55a072 Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68 Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d707599938de7b2db237bf993863d8ff60828b7582a957ca47f501b6a6f61f54 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=b865eb2d2f95d6acc508b20b7c45b772477e967f5b58b2f06a96925a2b32aa78 Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7 Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for blinker: filename=blinker-1.4-cp36-none-any.whl size=13449 sha256=12c388e54e1449336b720ff67b9a6753cea0b6f8326294e1ab6abc10d7db1bda Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89 Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=0296d94cc81bbf8601c1ce3281ad60c61917b53a08f5cedd736dbd8fd28a0945 Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23 Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=17c9493efecdcf44c19bfbb395d66820a88d6204eb60de33deac1708662f649f Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1 Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=4210ca989c3e797d25ac98ebad56dcfd9e3543c8a07d9a295fd72989770210dd Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843 Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=76facbd1af6b52ad86e249b6d07ee3f3a23be3abdd2ed115514c986c37931033 Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5 Successfully built seqeval sacremoses watchdog blinker gql subprocess32 pathtools graphql-core \u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.3.4 which is incompatible.\u001b[0m \u001b[31mERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m Installing collected packages: sentencepiece, tokenizers, tqdm, sacremoses, transformers, base58, enum-compat, ipykernel, pydeck, pathtools, watchdog, validators, blinker, streamlit, seqeval, graphql-core, gql, smmap, gitdb, GitPython, docker-pycreds, configparser, shortuuid, sentry-sdk, subprocess32, wandb, tensorboardx, simpletransformers Found existing installation: tqdm 4.41.1 Uninstalling tqdm-4.41.1: Successfully uninstalled tqdm-4.41.1 Found existing installation: ipykernel 4.10.1 Uninstalling ipykernel-4.10.1: Successfully uninstalled ipykernel-4.10.1 Successfully installed GitPython-3.1.7 base58-2.0.1 blinker-1.4 configparser-5.0.0 docker-pycreds-0.4.0 enum-compat-0.0.3 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 ipykernel-5.3.4 pathtools-0.1.2 pydeck-0.5.0b1 sacremoses-0.0.43 sentencepiece-0.1.91 sentry-sdk-0.16.5 seqeval-0.0.12 shortuuid-1.0.1 simpletransformers-0.47.3 smmap-3.0.4 streamlit-0.65.2 subprocess32-3.5.4 tensorboardx-2.1 tokenizers-0.8.1 tqdm-4.48.2 transformers-3.0.2 validators-0.18.0 wandb-0.9.5 watchdog-0.10.3 !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip --2020-08-24 15:02:27-- https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.80.139 Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.80.139|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4475746 (4.3M) [application/zip] Saving to: \u2018wikitext-2-v1.zip\u2019 wikitext-2-v1.zip 100%[===================>] 4.27M 2.93MB/s in 1.5s 2020-08-24 15:02:29 (2.93 MB/s) - \u2018wikitext-2-v1.zip\u2019 saved [4475746/4475746] !unzip wikitext-2-v1.zip Archive: wikitext-2-v1.zip creating: wikitext-2/ inflating: wikitext-2/wiki.test.tokens inflating: wikitext-2/wiki.valid.tokens inflating: wikitext-2/wiki.train.tokens ls wikitext-2/ wiki.test.tokens wiki.train.tokens wiki.valid.tokens from simpletransformers.language_modeling import LanguageModelingModel import logging logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) train_args = { \"reprocess_input_data\": True, \"overwrite_output_dir\": True, } model = LanguageModelingModel('bert', 'bert-base-cased', args=train_args, use_cuda=True) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable. INFO:filelock:Lock 139946366819016 acquired on cache_dir/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti\u2026 INFO:filelock:Lock 139946366819016 released on cache_dir/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock INFO:filelock:Lock 139948275897176 acquired on cache_dir/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_\u2026 INFO:filelock:Lock 139948275897176 released on cache_dir/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock INFO:filelock:Lock 139947842575384 acquired on cache_dir/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri\u2026 INFO:filelock:Lock 139947842575384 released on cache_dir/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'] - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). WARNING:transformers.modeling_utils:Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. model.train_model(\"wikitext-2/wiki.train.tokens\", eval_file=\"wikitext-2/wiki.test.tokens\", batch_size=32) INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/ HBox(children=(FloatProgress(value=0.0, max=23767.0), HTML(value=''))) WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (848 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (807 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors WARNING:transformers.tokenization_utils_base:Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors HBox(children=(FloatProgress(value=0.0, max=19808.0), HTML(value=''))) INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/bert_cached_lm_126_wiki.train.tokens INFO:simpletransformers.language_modeling.language_modeling_model: Training started HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i\u2026 HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 1', max=2476.0, style=ProgressStyle(de\u2026 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-6-6f25e41f04f9> in <module>() ----> 1 model.train_model(\"wikitext-2/wiki.train.tokens\", eval_file=\"wikitext-2/wiki.test.tokens\", batch_size=32) /usr/local/lib/python3.6/dist-packages/simpletransformers/language_modeling/language_modeling_model.py in train_model(self, train_file, output_dir, show_running_loss, args, eval_file, verbose, **kwargs) 368 eval_file=eval_file, 369 verbose=verbose, --> 370 **kwargs, 371 ) 372 /usr/local/lib/python3.6/dist-packages/simpletransformers/language_modeling/language_modeling_model.py in train(self, train_dataset, output_dir, show_running_loss, eval_file, verbose, **kwargs) 564 if args.fp16: 565 with amp.autocast(): --> 566 outputs = model(**inputs) 567 # model outputs are always tuple in pytorch-transformers (see doc) 568 loss = outputs[0] TypeError: BertForMaskedLM object argument after ** must be a mapping, not Tensor","title":"Simpletransformers intro"},{"location":"token_classification_transformers_zenml/","text":"!pip install zenml -q \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 326 kB 29.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.1 MB 63.7 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 636 kB 63.8 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217 kB 72.9 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 181 kB 70.0 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.9 MB 65.9 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 7.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 57.0 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 151 kB 71.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46 kB 4.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63 kB 2.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.3 MB 65.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 508 kB 69.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 255 kB 77.0 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63 kB 2.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40 kB 7.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 129 kB 63.6 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 135 kB 65.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 147 kB 79.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.5 MB 71.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53 kB 2.5 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49 kB 7.0 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1 MB 63.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51 kB 8.1 MB/s \u001b[?25h Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed. multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible. gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible. flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.4 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m !pip install transformers datasets -q \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.5 MB 34.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 311 kB 71.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 67 kB 6.7 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 895 kB 70.9 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8 MB 64.5 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1 MB 51.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133 kB 74.7 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 243 kB 72.3 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 144 kB 35.4 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94 kB 2.1 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 271 kB 72.8 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86 kB 7.5 MB/s \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. apache-beam 2.36.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\u001b[0m \u001b[?25h !pip install mlflow !git init !zenml init Initialized empty Git repository in /content/.git/ \u001b[32mInitializing ZenML repository at /content.\u001b[0m INFO:numexpr.utils:NumExpr defaulting to 2 threads. \u001b[1;35mRegistered stack component with name 'local_orchestrator'.\u001b[0m \u001b[1;35mRegistered stack component with name 'local_metadata_store'.\u001b[0m \u001b[1;35mRegistered stack component with name 'local_artifact_store'.\u001b[0m \u001b[1;35mRegistered stack with name 'local_stack'.\u001b[0m \u001b[32mZenML repository initialized at /content.\u001b[0m ls -la total 24 drwxr-xr-x 1 root root 4096 Feb 20 09:54 \u001b[0m\u001b[01;34m.\u001b[0m/ drwxr-xr-x 1 root root 4096 Feb 20 09:50 \u001b[01;34m..\u001b[0m/ drwxr-xr-x 4 root root 4096 Feb 1 14:31 \u001b[01;34m.config\u001b[0m/ drwxr-xr-x 7 root root 4096 Feb 20 09:54 \u001b[01;34m.git\u001b[0m/ drwxr-xr-x 1 root root 4096 Feb 1 14:32 \u001b[01;34msample_data\u001b[0m/ drwxr-xr-x 5 root root 4096 Feb 20 09:54 \u001b[01;34m.zen\u001b[0m/ import mlflow from zenml.integrations.mlflow.mlflow_step_decorator import enable_mlflow from zenml.pipelines import Schedule, pipeline from zenml.steps import BaseStepConfig, Output, step from datasets import load_dataset, load_metric from datasets import Dataset from datasets.dataset_dict import DatasetDict import os from typing import Any, Type from zenml.artifacts import DataArtifact, SchemaArtifact, StatisticsArtifact, ModelArtifact from zenml.materializers.base_materializer import BaseMaterializer from transformers import AutoTokenizer from transformers import TFAutoModelForTokenClassification, TFDistilBertForTokenClassification from transformers import create_optimizer from transformers import DataCollatorForTokenClassification DEFAULT_FILENAME = \"dataset.parquet.gzip\" COMPRESSION_TYPE = \"gzip\" DEFAULT_DICT_FILENAME = \"dict_datasets\" class DatasetMaterializer(BaseMaterializer): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = (Dataset, DatasetDict) ASSOCIATED_ARTIFACT_TYPES = ( DataArtifact, ) def handle_input(self, data_type: Type[Any]) -> Dataset: \"\"\"Reads Dataset from a parquet file.\"\"\" super().handle_input(data_type) if issubclass(data_type, Dataset): return Dataset.from_parquet( os.path.join(self.artifact.uri, DEFAULT_FILENAME)) elif issubclass(data_type, DatasetDict): return DatasetDict.load_from_disk( os.path.join(self.artifact.uri, DEFAULT_DICT_FILENAME) ) def handle_return(self, ds: Type[Any]) -> None: \"\"\"Writes a Dataset to the specified filename. Args: Dataset: The Dataset to write. \"\"\" super().handle_return(ds) if isinstance(ds, Dataset): filepath = os.path.join(self.artifact.uri, DEFAULT_FILENAME) ds.to_parquet(filepath, compression=COMPRESSION_TYPE) elif isinstance(ds, DatasetDict): filepath = os.path.join(self.artifact.uri, DEFAULT_DICT_FILENAME) ds.save_to_disk(filepath) DEFAULT_MODEL_DIR = \"hf_model\" class HFModelMaterializer(BaseMaterializer): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = (TFDistilBertForTokenClassification, ) ASSOCIATED_ARTIFACT_TYPES = ( ModelArtifact, ) def handle_input(self, data_type: Type[Any]) -> Dataset: \"\"\"Reads Dataset from a parquet file.\"\"\" super().handle_input(data_type) return TFAutoModelForTokenClassification.from_pretrained( os.path.join(self.artifact.uri, DEFAULT_MODEL_DIR) ) def handle_return(self, model: Type[Any]) -> None: \"\"\"Writes a Dataset to the specified filename. Args: Dataset: The Dataset to write. \"\"\" super().handle_return(model) model.save_pretrained(os.path.join(self.artifact.uri, DEFAULT_MODEL_DIR)) class TokenClassificationConfig(BaseStepConfig): task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\" model_checkpoint = \"distilbert-base-uncased\" batch_size = 16 dataset_name = \"conll2003\" label_all_tokens = True num_train_epochs = 3 @step#(enable_cache=False) def data_importer( config: TokenClassificationConfig, ) -> DatasetDict: datasets = load_dataset(config.dataset_name) print(\"Sample Example :\", datasets[\"train\"][0]) #label_list = datasets[\"train\"].features[f\"{config.task}_tags\"].feature.names return datasets @step#(enable_cache=False) def tokenization( config: TokenClassificationConfig, datasets: DatasetDict, ) -> DatasetDict: def tokenize_and_align_labels(examples): tokenized_inputs = tokenizer( examples[\"tokens\"], truncation=True, is_split_into_words=True ) labels = [] for i, label in enumerate(examples[f\"{config.task}_tags\"]): word_ids = tokenized_inputs.word_ids(batch_index=i) previous_word_idx = None label_ids = [] for word_idx in word_ids: # Special tokens have a word id that is None. We set the label to -100 so they are automatically # ignored in the loss function. if word_idx is None: label_ids.append(-100) # We set the label for the first token of each word. elif word_idx != previous_word_idx: label_ids.append(label[word_idx]) # For the other tokens in a word, we set the label to either the current label or -100, depending on # the label_all_tokens flag. else: label_ids.append(label[word_idx] if config.label_all_tokens else -100) previous_word_idx = word_idx labels.append(label_ids) tokenized_inputs[\"labels\"] = labels return tokenized_inputs tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint) tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True) return tokenized_datasets @enable_mlflow @step#(enable_cache=False) def trainer( config: TokenClassificationConfig, tokenized_datasets: DatasetDict, ) -> TFDistilBertForTokenClassification: label_list = tokenized_datasets[\"train\"].features[f\"{config.task}_tags\"].feature.names model = TFAutoModelForTokenClassification.from_pretrained( config.model_checkpoint, num_labels=len(label_list) ) num_train_steps = (len(tokenized_datasets[\"train\"]) // config.batch_size) * config.num_train_epochs optimizer, lr_schedule = create_optimizer( init_lr=2e-5, num_train_steps=num_train_steps, weight_decay_rate=0.01, num_warmup_steps=0, ) model.compile(optimizer=optimizer) tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint) data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\") train_set = tokenized_datasets[\"train\"].to_tf_dataset( columns=[\"attention_mask\", \"input_ids\", \"labels\"], shuffle=True, batch_size=config.batch_size, collate_fn=data_collator, ) validation_set = tokenized_datasets[\"validation\"].to_tf_dataset( columns=[\"attention_mask\", \"input_ids\", \"labels\"], shuffle=False, batch_size=config.batch_size, collate_fn=data_collator, ) mlflow.tensorflow.autolog() model.fit( train_set, #validation_data=validation_set, epochs=config.num_train_epochs ) return model import tensorflow as tf @enable_mlflow @step#(enable_cache=False) def evaluator( config: TokenClassificationConfig, model: TFDistilBertForTokenClassification, tokenized_datasets: DatasetDict, ) -> float: tokenizer = AutoTokenizer.from_pretrained(config.model_checkpoint) data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\") model.compile(optimizer=tf.keras.optimizers.Adam()) validation_set = tokenized_datasets[\"validation\"].to_tf_dataset( columns=[\"attention_mask\", \"input_ids\", \"labels\"], shuffle=False, batch_size=config.batch_size, collate_fn=data_collator, ) test_loss = model.evaluate(validation_set, verbose=1) mlflow.log_metric(\"val_loss\", test_loss) return test_loss @pipeline#(enable_cache=False) def train_pipeline(importer, tokenizer, trainer): datasets = importer() tokenized_datasets = tokenizer(datasets=datasets) model = trainer(tokenized_datasets) #train_pipeline(importer=data_importer().with_return_materializers(DatasetMaterializer), # tokenizer=tokenization().with_return_materializers(DatasetMaterializer), # trainer=trainer().with_return_materializers(HFModelMaterializer)).run() \u001b[1;35mCreating run for pipeline: `\u001b[0m\u001b[33;21mtrain_pipeline`\u001b[1;35m\u001b[0m \u001b[1;35mCache enabled for pipeline `\u001b[0m\u001b[33;21mtrain_pipeline`\u001b[1;35m\u001b[0m \u001b[1;35mUsing stack `\u001b[0m\u001b[33;21mlocal_stack`\u001b[1;35m to run pipeline `\u001b[0m\u001b[33;21mtrain_pipeline`\u001b[1;35m...\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mdata_importer`\u001b[1;35m has started.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mdata_importer`\u001b[1;35m has finished in 0.043s.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtokenization`\u001b[1;35m has started.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtokenization`\u001b[1;35m has finished in 0.035s.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtrainer`\u001b[1;35m has started.\u001b[0m Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForTokenClassification: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform'] - This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'classifier'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss. /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray. tensor = as_tensor(value) /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead. return py_builtins.overload_of(f)(*args) 10/10 [==============================] - 34s 2s/step - loss: 1.6988 \u001b[1;35mStep `\u001b[0m\u001b[33;21mtrainer`\u001b[1;35m has finished in 56.223s.\u001b[0m \u001b[1;35mPipeline run `\u001b[0m\u001b[33;21mtrain_pipeline-20_Feb_22-07_02_06_772016`\u001b[1;35m has finished in 56.331s.\u001b[0m @pipeline#(enable_cache=False) def train_eval_pipeline(importer, tokenizer, trainer, evaluator): datasets = importer() tokenized_datasets = tokenizer(datasets=datasets) model = trainer(tokenized_datasets) evaluator(model=model, tokenized_datasets=tokenized_datasets) train_eval_pipeline(importer=data_importer().with_return_materializers(DatasetMaterializer), tokenizer=tokenization().with_return_materializers(DatasetMaterializer), trainer=trainer().with_return_materializers(HFModelMaterializer), evaluator=evaluator()).run() \u001b[1;35mCreating run for pipeline: `\u001b[0m\u001b[33;21mtrain_eval_pipeline`\u001b[1;35m\u001b[0m \u001b[1;35mCache enabled for pipeline `\u001b[0m\u001b[33;21mtrain_eval_pipeline`\u001b[1;35m\u001b[0m \u001b[1;35mUsing stack `\u001b[0m\u001b[33;21mlocal_stack`\u001b[1;35m to run pipeline `\u001b[0m\u001b[33;21mtrain_eval_pipeline`\u001b[1;35m...\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mdata_importer`\u001b[1;35m has started.\u001b[0m Downloading: 0%| | 0.00/2.58k [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.62k [00:00<?, ?B/s] Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee... Downloading: 0%| | 0.00/983k [00:00<?, ?B/s] 0 examples [00:00, ? examples/s] INFO:datasets_modules.datasets.conll2003.63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee.conll2003:\u23f3 Generating examples from = /root/.cache/huggingface/datasets/downloads/extracted/fb350f14aaa0ce8a148eb2de9b8f4c19422485ca2524f6b4170579a4759a2d51/train.txt 0 examples [00:00, ? examples/s] INFO:datasets_modules.datasets.conll2003.63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee.conll2003:\u23f3 Generating examples from = /root/.cache/huggingface/datasets/downloads/extracted/fb350f14aaa0ce8a148eb2de9b8f4c19422485ca2524f6b4170579a4759a2d51/valid.txt 0 examples [00:00, ? examples/s] INFO:datasets_modules.datasets.conll2003.63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee.conll2003:\u23f3 Generating examples from = /root/.cache/huggingface/datasets/downloads/extracted/fb350f14aaa0ce8a148eb2de9b8f4c19422485ca2524f6b4170579a4759a2d51/test.txt Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data. 0%| | 0/3 [00:00<?, ?it/s] Sample Example : {'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]} \u001b[1;35mStep `\u001b[0m\u001b[33;21mdata_importer`\u001b[1;35m has finished in 7.227s.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtokenization`\u001b[1;35m has started.\u001b[0m Downloading: 0%| | 0.00/28.0 [00:00<?, ?B/s] Downloading: 0%| | 0.00/483 [00:00<?, ?B/s] Downloading: 0%| | 0.00/226k [00:00<?, ?B/s] Downloading: 0%| | 0.00/455k [00:00<?, ?B/s] WARNING:datasets.fingerprint:Parameter 'function'=<function tokenization.<locals>.tokenize_and_align_labels at 0x7f4983858ef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed. 0%| | 0/15 [00:00<?, ?ba/s] 0%| | 0/4 [00:00<?, ?ba/s] 0%| | 0/4 [00:00<?, ?ba/s] \u001b[1;35mStep `\u001b[0m\u001b[33;21mtokenization`\u001b[1;35m has finished in 4.195s.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtrainer`\u001b[1;35m has started.\u001b[0m 2022/02/20 09:58:49 INFO mlflow.tracking.fluent: Experiment with name 'train_eval_pipeline' does not exist. Creating a new experiment. Downloading: 0%| | 0.00/347M [00:00<?, ?B/s] Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForTokenClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13'] - This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss. /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray. tensor = as_tensor(value) 2022/02/20 09:59:02 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow. Epoch 1/3 /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead. return py_builtins.overload_of(f)(*args) 6/877 [..............................] - ETA: 1:45 - loss: 1.7235 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0655s vs `on_train_batch_end` time: 0.0753s). Check your callbacks. 877/877 [==============================] - 102s 102ms/step - loss: 0.1888 Epoch 2/3 877/877 [==============================] - 92s 104ms/step - loss: 0.0539 Epoch 3/3 877/877 [==============================] - 92s 105ms/step - loss: 0.0345 WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.dropout.Dropout object at 0x7f48776754d0>, because it is not built. WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.dropout.Dropout object at 0x7f48775d32d0>, because it is not built. WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.dropout.Dropout object at 0x7f48775d9dd0>, because it is not built. WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.dropout.Dropout object at 0x7f48775c9b50>, because it is not built. WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.dropout.Dropout object at 0x7f487768da50>, because it is not built. WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.dropout.Dropout object at 0x7f48776ae950>, because it is not built. 2022/02/20 10:05:39 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to save the object {'loss': <function dummy_loss at 0x7f487b607b90>, 'logits': None} (a dictionary wrapper constructed automatically on attribute assignment). The wrapped dictionary was modified outside the wrapper (its final value was {'loss': <function dummy_loss at 0x7f487b607b90>, 'logits': None}, its value when a checkpoint dependency was added was None), which breaks restoration on object creation. If you don't need this dictionary checkpointed, wrap it in a non-trackable object; it will be subsequently ignored. \u001b[1;35mStep `\u001b[0m\u001b[33;21mtrainer`\u001b[1;35m has finished in 7m31s.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mevaluator`\u001b[1;35m has started.\u001b[0m Some layers from the model checkpoint at /root/.config/zenml/local_stores/4f1f1248-e71b-4ac6-ac34-396be78aa2b2/trainer/output/3/hf_model were not used when initializing TFDistilBertForTokenClassification: ['dropout_19'] - This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at /root/.config/zenml/local_stores/4f1f1248-e71b-4ac6-ac34-396be78aa2b2/trainer/output/3/hf_model and are newly initialized: ['dropout_39'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss. /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray. tensor = as_tensor(value) /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead. return py_builtins.overload_of(f)(*args) 204/204 [==============================] - 9s 33ms/step - loss: 0.0587 \u001b[1;35mStep `\u001b[0m\u001b[33;21mevaluator`\u001b[1;35m has finished in 10.280s.\u001b[0m \u001b[1;35mPipeline run `\u001b[0m\u001b[33;21mtrain_eval_pipeline-20_Feb_22-09_58_38_115601`\u001b[1;35m has finished in 7m53s.\u001b[0m #train_pipeline(importer=data_importer().with_return_materializers(DatasetMaterializer), # tokenizer=tokenization().with_return_materializers(DatasetMaterializer), # trainer=trainer()).run() \u001b[1;35mCreating run for pipeline: `\u001b[0m\u001b[33;21mtrain_pipeline`\u001b[1;35m\u001b[0m \u001b[1;35mCache enabled for pipeline `\u001b[0m\u001b[33;21mtrain_pipeline`\u001b[1;35m\u001b[0m \u001b[1;35mUsing stack `\u001b[0m\u001b[33;21mlocal_stack`\u001b[1;35m to run pipeline `\u001b[0m\u001b[33;21mtrain_pipeline`\u001b[1;35m...\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mdata_importer`\u001b[1;35m has started.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mdata_importer`\u001b[1;35m has finished in 0.041s.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtokenization`\u001b[1;35m has started.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtokenization`\u001b[1;35m has finished in 0.041s.\u001b[0m \u001b[1;35mStep `\u001b[0m\u001b[33;21mtrainer`\u001b[1;35m has started.\u001b[0m Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForTokenClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13'] - This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_59', 'classifier'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! Please ensure your labels are passed as keys in the input dict so that they are accessible to the model during the forward pass. To disable this behaviour, please pass a loss argument, or explicitly pass loss=None if you do not want your model to compute a loss. /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:707: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray. tensor = as_tensor(value) /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead. return py_builtins.overload_of(f)(*args) 10/10 [==============================] - 31s 2s/step - loss: 1.7103 \u001b[1;35mStep `\u001b[0m\u001b[33;21mtrainer`\u001b[1;35m has finished in 53.341s.\u001b[0m \u001b[1;35mPipeline run `\u001b[0m\u001b[33;21mtrain_pipeline-20_Feb_22-06_23_52_536912`\u001b[1;35m has finished in 53.457s.\u001b[0m Install localtunnel !npm install -g localtunnel lineage !zenml integration install dash -f %%writefile run_lineage.py from zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer import ( PipelineRunLineageVisualizer, ) from zenml.repository import Repository repo = Repository() latest_run = repo.get_pipelines()[-1].runs[-1] print(latest_run) PipelineRunLineageVisualizer().visualize(latest_run) Writing run_lineage.py !python run_lineage.py & lt --port 8050 your url is: https://wonderful-seahorse-55.loca.lt INFO:numexpr.utils:NumExpr defaulting to 2 threads. PipelineRunView(id=2, name='train_eval_pipeline-20_Feb_22-09_58_38_115601') \u001b[1;35mDash is running on http://127.0.0.1:8050/ \u001b[0m Dash is running on http://127.0.0.1:8050/ * Serving Flask app \"zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer\" (lazy loading) * Environment: production \u001b[31m WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m \u001b[2m Use a production WSGI server instead.\u001b[0m * Debug mode: off INFO:werkzeug: * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit) INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /favicon.ico HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/polyfill@7.v2_2_0m1645352095.12.1.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/react@16.v2_2_0m1645352095.14.0.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/prop-types@15.v2_2_0m1645352095.7.2.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/react-dom@16.v2_2_0m1645352095.14.0.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash_cytoscape/dash_cytoscape.v0_2_0m1645352099.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash_bootstrap_components/_components/dash_bootstrap_components.v1_0_3m1645352102.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dash-renderer/build/dash_renderer.v2_2_0m1645352095.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dcc/dash_core_components-shared.v2_2_0m1645352095.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dcc/dash_core_components.v2_2_0m1645352095.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/html/dash_html_components.v2_0_2m1645352095.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dash_table/bundle.v5_1_1m1645352095.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:02] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:02] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:03] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-markdown.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:03] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:03] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:06] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-highlight.js HTTP/1.1\u001b[0m\" 200 - --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-35-cbe64bbddfbb> in <module>() ----> 1 get_ipython().system('python run_lineage.py & lt --port 8050') /usr/local/lib/python3.7/dist-packages/google/colab/_shell.py in system(self, *args, **kwargs) 100 kwargs.update({'also_return_output': True}) 101 --> 102 output = _system_commands._system_compat(self, *args, **kwargs) # pylint:disable=protected-access 103 104 if pip_warn: /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _system_compat(shell, cmd, also_return_output) 445 # stack. 446 result = _run_command( --> 447 shell.var_expand(cmd, depth=2), clear_streamed_output=False) 448 shell.user_ns['_exit_code'] = result.returncode 449 if -result.returncode in _INTERRUPTED_SIGNALS: /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _run_command(cmd, clear_streamed_output) 197 os.close(child_pty) 198 --> 199 return _monitor_process(parent_pty, epoll, p, cmd, update_stdin_widget) 200 finally: 201 epoll.close() /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _monitor_process(parent_pty, epoll, p, cmd, update_stdin_widget) 227 while True: 228 try: --> 229 result = _poll_process(parent_pty, epoll, p, cmd, decoder, state) 230 if result is not None: 231 return result /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _poll_process(parent_pty, epoll, p, cmd, decoder, state) 274 output_available = False 275 --> 276 events = epoll.poll() 277 input_events = [] 278 for _, event in events: KeyboardInterrupt: MlFlow Tracking from zenml.environment import Environment from zenml.integrations.mlflow.mlflow_environment import MLFLOW_ENVIRONMENT_NAME mlflow_env = Environment()[MLFLOW_ENVIRONMENT_NAME] mlflow_env <zenml.integrations.mlflow.mlflow_environment.MLFlowEnvironment at 0x7f487ad5fe10> mlflow_env.tracking_uri 'file:/root/.config/zenml/local_stores/4f1f1248-e71b-4ac6-ac34-396be78aa2b2/mlruns' !mlflow ui --backend-store-uri {mlflow_env.tracking_uri} & lt --port 5000 your url is: https://honest-wasp-2.loca.lt [2022-02-20 10:23:21 +0000] [840] [INFO] Starting gunicorn 20.1.0 [2022-02-20 10:23:21 +0000] [840] [INFO] Listening at: http://127.0.0.1:5000 (840) [2022-02-20 10:23:21 +0000] [840] [INFO] Using worker: sync [2022-02-20 10:23:21 +0000] [843] [INFO] Booting worker with pid: 843 [2022-02-20 10:23:52 +0000] [840] [CRITICAL] WORKER TIMEOUT (pid:843) [2022-02-20 10:23:52 +0000] [843] [INFO] Worker exiting (pid: 843) [2022-02-20 10:23:52 +0000] [850] [INFO] Booting worker with pid: 850 [2022-02-20 10:25:06 +0000] [840] [INFO] Handling signal: int [2022-02-20 10:25:06 +0000] [850] [INFO] Worker exiting (pid: 850) [2022-02-20 10:25:06 +0000] [840] [INFO] Shutting down: Master ^C Dag Visualization !zenml integration install graphviz -f Collecting graphviz>=0.17 Downloading graphviz-0.19.1-py3-none-any.whl (46 kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46 kB 4.3 MB/s \u001b[?25hInstalling collected packages: graphviz Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 Successfully installed graphviz-0.19.1 from zenml.repository import Repository from zenml.integrations.graphviz.visualizers.pipeline_run_dag_visualizer import ( PipelineRunDagVisualizer, ) repo = Repository() pipe = repo.get_pipelines()[-1] latest_run = pipe.runs[-1] latest_run PipelineRunView(id=2, name='train_eval_pipeline-20_Feb_22-09_58_38_115601') PipelineRunDagVisualizer().visualize(latest_run) \u001b[1;35mThis integration is not completed yet. Results might be unexpected.\u001b[0m","title":"Token classification transformers zenml"},{"location":"token_classification_transformers_zenml/#install-localtunnel","text":"!npm install -g localtunnel","title":"Install localtunnel"},{"location":"token_classification_transformers_zenml/#lineage","text":"!zenml integration install dash -f %%writefile run_lineage.py from zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer import ( PipelineRunLineageVisualizer, ) from zenml.repository import Repository repo = Repository() latest_run = repo.get_pipelines()[-1].runs[-1] print(latest_run) PipelineRunLineageVisualizer().visualize(latest_run) Writing run_lineage.py !python run_lineage.py & lt --port 8050 your url is: https://wonderful-seahorse-55.loca.lt INFO:numexpr.utils:NumExpr defaulting to 2 threads. PipelineRunView(id=2, name='train_eval_pipeline-20_Feb_22-09_58_38_115601') \u001b[1;35mDash is running on http://127.0.0.1:8050/ \u001b[0m Dash is running on http://127.0.0.1:8050/ * Serving Flask app \"zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer\" (lazy loading) * Environment: production \u001b[31m WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m \u001b[2m Use a production WSGI server instead.\u001b[0m * Debug mode: off INFO:werkzeug: * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit) INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /favicon.ico HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/polyfill@7.v2_2_0m1645352095.12.1.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/react@16.v2_2_0m1645352095.14.0.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/prop-types@15.v2_2_0m1645352095.7.2.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/deps/react-dom@16.v2_2_0m1645352095.14.0.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash_cytoscape/dash_cytoscape.v0_2_0m1645352099.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash_bootstrap_components/_components/dash_bootstrap_components.v1_0_3m1645352102.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dash-renderer/build/dash_renderer.v2_2_0m1645352095.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dcc/dash_core_components-shared.v2_2_0m1645352095.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dcc/dash_core_components.v2_2_0m1645352095.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/html/dash_html_components.v2_0_2m1645352095.min.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:19:44] \"\u001b[37mGET /_dash-component-suites/dash/dash_table/bundle.v5_1_1m1645352095.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:02] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:02] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:03] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-markdown.js HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:03] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:03] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 - INFO:werkzeug:127.0.0.1 - - [20/Feb/2022 10:20:06] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-highlight.js HTTP/1.1\u001b[0m\" 200 - --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-35-cbe64bbddfbb> in <module>() ----> 1 get_ipython().system('python run_lineage.py & lt --port 8050') /usr/local/lib/python3.7/dist-packages/google/colab/_shell.py in system(self, *args, **kwargs) 100 kwargs.update({'also_return_output': True}) 101 --> 102 output = _system_commands._system_compat(self, *args, **kwargs) # pylint:disable=protected-access 103 104 if pip_warn: /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _system_compat(shell, cmd, also_return_output) 445 # stack. 446 result = _run_command( --> 447 shell.var_expand(cmd, depth=2), clear_streamed_output=False) 448 shell.user_ns['_exit_code'] = result.returncode 449 if -result.returncode in _INTERRUPTED_SIGNALS: /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _run_command(cmd, clear_streamed_output) 197 os.close(child_pty) 198 --> 199 return _monitor_process(parent_pty, epoll, p, cmd, update_stdin_widget) 200 finally: 201 epoll.close() /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _monitor_process(parent_pty, epoll, p, cmd, update_stdin_widget) 227 while True: 228 try: --> 229 result = _poll_process(parent_pty, epoll, p, cmd, decoder, state) 230 if result is not None: 231 return result /usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py in _poll_process(parent_pty, epoll, p, cmd, decoder, state) 274 output_available = False 275 --> 276 events = epoll.poll() 277 input_events = [] 278 for _, event in events: KeyboardInterrupt:","title":"lineage"},{"location":"token_classification_transformers_zenml/#mlflow-tracking","text":"from zenml.environment import Environment from zenml.integrations.mlflow.mlflow_environment import MLFLOW_ENVIRONMENT_NAME mlflow_env = Environment()[MLFLOW_ENVIRONMENT_NAME] mlflow_env <zenml.integrations.mlflow.mlflow_environment.MLFlowEnvironment at 0x7f487ad5fe10> mlflow_env.tracking_uri 'file:/root/.config/zenml/local_stores/4f1f1248-e71b-4ac6-ac34-396be78aa2b2/mlruns' !mlflow ui --backend-store-uri {mlflow_env.tracking_uri} & lt --port 5000 your url is: https://honest-wasp-2.loca.lt [2022-02-20 10:23:21 +0000] [840] [INFO] Starting gunicorn 20.1.0 [2022-02-20 10:23:21 +0000] [840] [INFO] Listening at: http://127.0.0.1:5000 (840) [2022-02-20 10:23:21 +0000] [840] [INFO] Using worker: sync [2022-02-20 10:23:21 +0000] [843] [INFO] Booting worker with pid: 843 [2022-02-20 10:23:52 +0000] [840] [CRITICAL] WORKER TIMEOUT (pid:843) [2022-02-20 10:23:52 +0000] [843] [INFO] Worker exiting (pid: 843) [2022-02-20 10:23:52 +0000] [850] [INFO] Booting worker with pid: 850 [2022-02-20 10:25:06 +0000] [840] [INFO] Handling signal: int [2022-02-20 10:25:06 +0000] [850] [INFO] Worker exiting (pid: 850) [2022-02-20 10:25:06 +0000] [840] [INFO] Shutting down: Master ^C","title":"MlFlow Tracking"},{"location":"token_classification_transformers_zenml/#dag-visualization","text":"!zenml integration install graphviz -f Collecting graphviz>=0.17 Downloading graphviz-0.19.1-py3-none-any.whl (46 kB) \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46 kB 4.3 MB/s \u001b[?25hInstalling collected packages: graphviz Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 Successfully installed graphviz-0.19.1 from zenml.repository import Repository from zenml.integrations.graphviz.visualizers.pipeline_run_dag_visualizer import ( PipelineRunDagVisualizer, ) repo = Repository() pipe = repo.get_pipelines()[-1] latest_run = pipe.runs[-1] latest_run PipelineRunView(id=2, name='train_eval_pipeline-20_Feb_22-09_58_38_115601') PipelineRunDagVisualizer().visualize(latest_run) \u001b[1;35mThis integration is not completed yet. Results might be unexpected.\u001b[0m","title":"Dag Visualization"}]}