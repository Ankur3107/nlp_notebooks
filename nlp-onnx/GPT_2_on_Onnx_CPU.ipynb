{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 on Onnx CPU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVUDt94FBd4ktoDjgVqtDr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankur3107/nlp_notebooks/blob/master/nlp-onnx/GPT_2_on_Onnx_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "4leCkjvvJpTd",
        "outputId": "790550bc-f993-464b-98b3-979df339a685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Uninstalling transformers-4.14.1:\n",
            "      Successfully uninstalled transformers-4.14.1\n",
            "  Attempting uninstall: onnxruntime\n",
            "    Found existing installation: onnxruntime 1.10.0\n",
            "    Uninstalling onnxruntime-1.10.0:\n",
            "      Successfully uninstalled onnxruntime-1.10.0\n",
            "  Attempting uninstall: onnxconverter-common\n",
            "    Found existing installation: onnxconverter-common 1.9.0\n",
            "    Uninstalling onnxconverter-common-1.9.0:\n",
            "      Successfully uninstalled onnxconverter-common-1.9.0\n",
            "Successfully installed huggingface-hub-0.0.12 onnx-1.9.0 onnxconverter-common-1.8.1 onnxruntime-1.8.1 transformers-4.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "onnx",
                  "onnxruntime",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install onnxruntime==1.8.1 onnx==1.9.0 onnxconverter_common==1.8.1 transformers==4.8.2 psutil pytz pandas py-cpuinfo py3nvml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a cache directory to store pretrained model.\n",
        "cache_dir = os.path.join(\".\", \"cache_models\")\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)"
      ],
      "metadata": {
        "id": "NJGpH7vZJ-op"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4Soh5-LKHQL",
        "outputId": "0e646eba-366e-4773-ac24-f5719cc99fe2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               79\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2199.998\n",
            "BogoMIPS:            4399.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            56320K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install coloredlogs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pACoknDeKcSG",
        "outputId": "1df5f215-3cc4-40c6-ebb1-b3955820304b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.7/dist-packages (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.7/dist-packages (from coloredlogs) (10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.transformers.gpt2_beamsearch_helper import Gpt2BeamSearchHelper, GPT2LMHeadModel_BeamSearchStep\n",
        "from transformers import AutoConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "4rvLj1RaLUqB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"gpt2\"\n",
        "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "model = GPT2LMHeadModel_BeamSearchStep.from_pretrained(model_name_or_path, config=config, batch_size=1, beam_size=4, cache_dir=cache_dir)\n",
        "device = torch.device(\"cpu\")\n",
        "model.eval().to(device)\n",
        "\n",
        "print(model.config)\n",
        "\n",
        "num_attention_heads = model.config.n_head\n",
        "hidden_size = model.config.n_embd\n",
        "num_layer = model.config.n_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YznPqbSKL8A",
        "outputId": "1077061d-efa7-4935-a7f4-216cfb153970"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"batch_size\": 1,\n",
            "  \"beam_size\": 4,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"gpt2_one_step_search.onnx\"\n",
        "Gpt2BeamSearchHelper.export_onnx(model, device, onnx_model_path) # add parameter use_external_data_format=True when model size > 2 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1LXnqw-KWEC",
        "outputId": "12fd955f-25de-4d94-e788-a3ee76f66425"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/onnxruntime/transformers/gpt2_beamsearch_helper.py:91: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  selected_input_seq = selected_index_flat // self.config.beam_size\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/utils.py:100: UserWarning: `example_outputs' is deprecated and ignored. Will be removed in next PyTorch release.\n",
            "  warnings.warn(\"`example_outputs' is deprecated and ignored. Will be removed in \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
            "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:698: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:249: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  past_key, past_value = layer_past\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:181: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import numpy\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EXAMPLE_Text = ['best hotel in bay area.']\n",
        "\n",
        "def get_tokenizer(model_name_or_path, cache_dir):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    return tokenizer\n",
        "\n",
        "def get_example_inputs(prompt_text=EXAMPLE_Text):    \n",
        "    tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
        "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
        "\n",
        "    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
        "    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
        "    position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
        "    position_ids.masked_fill_(position_ids < 0, 0)\n",
        "\n",
        "    #Empty Past State for generating first word\n",
        "    empty_past = []\n",
        "    batch_size = input_ids.size(0)\n",
        "    sequence_length = input_ids.size(1)\n",
        "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
        "    for i in range(num_layer):\n",
        "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
        "       \n",
        "    return input_ids, attention_mask, position_ids, empty_past\n",
        "\n",
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "\n",
        "onnx_model_path = \"gpt2_one_step_search.onnx\"\n",
        "session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "ort_inputs = {\n",
        "              'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "              'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "              'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "              'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "              'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "              'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "              'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "             }\n",
        "for i, past_i in enumerate(empty_past):\n",
        "    ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "ort_outputs = session.run(None, ort_inputs)"
      ],
      "metadata": {
        "id": "8L9CXhpOKkzp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores, step, context_len):\n",
        "    output_shapes = Gpt2BeamSearchHelper.get_output_shapes(batch_size=1,\n",
        "                                                           context_len=context_len,\n",
        "                                                           past_sequence_length=past[0].size(3),\n",
        "                                                           sequence_length=input_ids.size(1),\n",
        "                                                           beam_size=4,\n",
        "                                                           step=step,\n",
        "                                                           config=config,\n",
        "                                                           model_class=\"GPT2LMHeadModel_BeamSearchStep\")\n",
        "    output_buffers = Gpt2BeamSearchHelper.get_output_buffers(output_shapes, device)\n",
        "\n",
        "    io_binding = Gpt2BeamSearchHelper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores)\n",
        "    session.run_with_iobinding(io_binding)\n",
        "\n",
        "    outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes, return_numpy=False)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "DfuqKYFYKrby"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
        "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "outputs = inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, empty_past, beam_select_idx, input_log_probs, input_unfinished_sents, input_ids, prev_step_scores, 0, input_ids.shape[-1])\n",
        "assert torch.eq(outputs[-2], torch.from_numpy(ort_outputs[-2])).all()\n",
        "print(\"IO Binding result is good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3jvgUe2KrZ7",
        "outputId": "c23618d3-87db-4583-d3a4-e5293560a0bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IO Binding result is good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update(output, step, batch_size, beam_size, context_length, prev_attention_mask, device):\n",
        "    \"\"\"\n",
        "    Update the inputs for next inference.\n",
        "    \"\"\"\n",
        "    last_state = (torch.from_numpy(output[0]).to(device)\n",
        "                        if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu())\n",
        "\n",
        "    input_ids = last_state.view(batch_size * beam_size, -1).to(device)\n",
        "\n",
        "    input_unfinished_sents_id = -3\n",
        "    prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray)\n",
        "                                else output[-2].clone().detach().to(device))\n",
        "    position_ids = (torch.tensor([context_length + step - 1\n",
        "                                        ]).unsqueeze(0).repeat(batch_size * beam_size, 1).to(device))\n",
        "\n",
        "    if prev_attention_mask.shape[0] != (batch_size * beam_size):\n",
        "        prev_attention_mask = prev_attention_mask.repeat(batch_size * beam_size, 1)\n",
        "    attention_mask = torch.cat(\n",
        "        [\n",
        "            prev_attention_mask,\n",
        "            torch.ones([batch_size * beam_size, 1]).type_as(prev_attention_mask),\n",
        "        ],\n",
        "        1,\n",
        "    ).to(device)\n",
        "\n",
        "    beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id - 2].clone().detach().to(device))\n",
        "    input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id - 1].clone().detach().to(device))\n",
        "    input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance(\n",
        "        output[input_unfinished_sents_id], numpy.ndarray) else\n",
        "                                    output[input_unfinished_sents_id].clone().detach().to(device))\n",
        "    prev_step_scores = (torch.from_numpy(output[-1]).to(device)\n",
        "                                if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device))\n",
        "\n",
        "    past = []\n",
        "    if isinstance(output[1], tuple):  # past in torch output is tuple\n",
        "        past = list(output[1])\n",
        "    else:\n",
        "        for i in range(model.config.n_layer):\n",
        "            past_i = (torch.from_numpy(output[i + 1])\n",
        "                        if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach())\n",
        "            past.append(past_i.to(device)) \n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': prev_step_results,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(prev_step_results.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    \n",
        "    return inputs, ort_inputs, past\n",
        "\n",
        "def test_generation(tokenizer, input_text, use_onnxruntime_io, ort_session = None, num_tokens_to_produce = 30):\n",
        "    print(\"Text generation using\", \"OnnxRuntime with IO binding\" if use_onnxruntime_io else \"OnnxRuntime\", \"...\")    \n",
        "    input_ids, attention_mask, position_ids, past = get_example_inputs(input_text)\n",
        "    beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
        "    input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
        "    input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
        "    prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask' : attention_mask,\n",
        "        'position_ids': position_ids,\n",
        "        'beam_select_idx': beam_select_idx,\n",
        "        'input_log_probs': input_log_probs,\n",
        "        'input_unfinished_sents': input_unfinished_sents,\n",
        "        'prev_step_results': input_ids,\n",
        "        'prev_step_scores': prev_step_scores,\n",
        "    }\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
        "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
        "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
        "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
        "        'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
        "    }\n",
        "    for i, past_i in enumerate(past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "    batch_size = input_ids.size(0)\n",
        "    beam_size = 4\n",
        "    context_length = input_ids.size(-1)\n",
        "\n",
        "    for step in range(num_tokens_to_produce):\n",
        "        if use_onnxruntime_io:\n",
        "            outputs = inference_with_io_binding(ort_session, config, inputs['input_ids'], inputs['position_ids'], inputs['attention_mask'], past, inputs['beam_select_idx'], inputs['input_log_probs'], inputs['input_unfinished_sents'], inputs['prev_step_results'], inputs['prev_step_scores'], step, context_length)\n",
        "        else:\n",
        "            outputs = ort_session.run(None, ort_inputs) \n",
        "        inputs, ort_inputs, past = update(outputs, step, batch_size, beam_size, context_length, inputs['attention_mask'], device)\n",
        "\n",
        "        if not inputs['input_unfinished_sents'].any():\n",
        "            break\n",
        "\n",
        "    print(\"------------\")\n",
        "    print(tokenizer.decode(inputs['prev_step_results'][0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "X54oPzNOMOb3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
        "input_text = EXAMPLE_Text\n",
        "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzocGIO9MSZX",
        "outputId": "46018cbe-cba0-47e4-8be7-e49877c9aef4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation using OnnxRuntime ...\n",
            "------------\n",
            "best hotel in bay area.\n",
            "\n",
            "\"It's a great place to stay,\" he said.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_generation(tokenizer, input_text, use_onnxruntime_io=True, ort_session=session)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z05HfaP8MUgY",
        "outputId": "51858a48-f93d-4351-9cd9-7ad7d7796f19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation using OnnxRuntime with IO binding ...\n",
            "------------\n",
            "best hotel in bay area.\n",
            "\n",
            "\"It's a great place to stay,\" he said.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yqKBRlENMY3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}