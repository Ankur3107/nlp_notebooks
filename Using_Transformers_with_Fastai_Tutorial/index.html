<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Package Installation - NLP-Notebooks</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Package Installation";
        var mkdocs_page_input_path = "Using_Transformers_with_Fastai_Tutorial.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> NLP-Notebooks
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../BERT_Fine_Tuning_Sentence_Classification_v2/">BERT Fine Tuning Sentence Classification v2</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Bert_Classification_Pt/">Bert Classification Pt</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Bert_Pre_Training/">Bert Pre Training</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Doc_Visual_QA_and_Bill_extraction_demo/">Doc Visual QA and Bill extraction demo</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../GPT_2_on_Onnx_CPU/">GPT 2 on Onnx CPU</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Generic_Transformer_Classification/">Generic Transformer Classification</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Question_Answering_with_a_Fine_Tuned_BERT/">Question Answering with a Fine Tuned BERT</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Seq2Seq_Pytorch/">Seq2Seq Pytorch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../SetFit_SST_2_Few_shot/">SetFit SST 2 Few shot</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Simpletransformers_2/">Simpletransformers 2</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../TAPAS_fine_tuning_in_tf/">TAPAS fine tuning in tf</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Package Installation</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Wikipedia_answer_retrieval_DPR/">Wikipedia answer retrieval DPR</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../contextual_topic_modeling/">Contextual topic modeling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../knowledge_distillation_exploration/">Knowledge distillation exploration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../large_scale_multilabelclassification/">Large scale multilabelclassification</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../simpletransformers_intro/">Simpletransformers intro</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../token_classification_transformers_zenml/">Token classification transformers zenml</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">NLP-Notebooks</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li><li>Package Installation</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="package-installation">Package Installation</h1>
<pre><code>!git clone https://github.com/devkosal/fastai_roberta.git
</code></pre>
<pre><code>Cloning into 'fastai_roberta'...
remote: Enumerating objects: 171, done.[K
remote: Counting objects: 100% (171/171), done.[K
remote: Compressing objects: 100% (121/121), done.[K
remote: Total 171 (delta 91), reused 111 (delta 44), pack-reused 0[K
Receiving objects: 100% (171/171), 25.46 MiB | 18.58 MiB/s, done.
Resolving deltas: 100% (91/91), done.
</code></pre>
<pre><code>!pip install fastai==1.0.60 transformers==2.3.0
</code></pre>
<pre><code>Collecting fastai==1.0.60
[?25l  Downloading https://files.pythonhosted.org/packages/f5/e4/a7025bf28f303dbda0f862c09a7f957476fa92c9271643b4061a81bb595f/fastai-1.0.60-py3-none-any.whl (237kB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 4.7MB/s 
[?25hCollecting transformers==2.3.0
[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450kB 7.4MB/s 
[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (3.2.2)
Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.3.2)
Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.18.5)
Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (7.0.0)
Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (7.352.0)
Requirement already satisfied: dataclasses; python_version &lt; "3.7" in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (0.7)
Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.23.0)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (3.13)
Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.7.1)
Requirement already satisfied: fastprogress&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.0.0)
Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (20.4)
Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (0.7.0+cu101)
Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (4.6.3)
Requirement already satisfied: spacy&gt;=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (2.2.4)
Requirement already satisfied: torch&gt;=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.6.0+cu101)
Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.4.1)
Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.60) (1.0.5)
Collecting sentencepiece
[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 7.0MB/s 
[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.14.48)
Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.41.1)
Collecting sacremoses
[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)
[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 26.1MB/s 
[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.20)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai==1.0.60) (2.4.7)
Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai==1.0.60) (2.8.1)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai==1.0.60) (0.10.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;fastai==1.0.60) (1.2.0)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==1.0.60) (1.24.3)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==1.0.60) (2020.6.20)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==1.0.60) (3.0.4)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;fastai==1.0.60) (2.10)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;fastai==1.0.60) (1.15.0)
Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (1.0.2)
Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (0.7.1)
Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (0.4.1)
Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (1.1.3)
Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (3.0.2)
Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (1.0.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (49.6.0)
Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (2.0.3)
Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (7.4.0)
Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy&gt;=2.0.18-&gt;fastai==1.0.60) (1.0.2)
Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch&gt;=1.0.0-&gt;fastai==1.0.60) (0.16.0)
Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;fastai==1.0.60) (2018.9)
Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;transformers==2.3.0) (0.10.0)
Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;transformers==2.3.0) (0.3.3)
Requirement already satisfied: botocore&lt;1.18.0,&gt;=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3-&gt;transformers==2.3.0) (1.17.48)
Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==2.3.0) (7.1.2)
Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==2.3.0) (0.16.0)
Requirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; "3.8" in /usr/local/lib/python3.6/dist-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai==1.0.60) (1.7.0)
Requirement already satisfied: docutils&lt;0.16,&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore&lt;1.18.0,&gt;=1.17.48-&gt;boto3-&gt;transformers==2.3.0) (0.15.2)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata&gt;=0.20; python_version &lt; "3.8"-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&gt;=2.0.18-&gt;fastai==1.0.60) (3.1.0)
Building wheels for collected packages: sacremoses
  Building wheel for sacremoses (setup.py) ... [?25l[?25hdone
  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=aef96c81bb474827b2b34caa9690e68458a664de90731bcc580754ce52b74dd7
  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45
Successfully built sacremoses
Installing collected packages: fastai, sentencepiece, sacremoses, transformers
  Found existing installation: fastai 1.0.61
    Uninstalling fastai-1.0.61:
      Successfully uninstalled fastai-1.0.61
Successfully installed fastai-1.0.60 sacremoses-0.0.43 sentencepiece-0.1.91 transformers-2.3.0
</code></pre>
<h1 id="load-and-set-configuration">Load And Set Configuration</h1>
<pre><code>from fastai.text import *
from fastai.metrics import *
from transformers import RobertaTokenizer
</code></pre>
<pre><code># Creating a config object to store task specific information
class Config(dict):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        for k, v in kwargs.items():
            setattr(self, k, v)

    def set(self, key, val):
        self[key] = val
        setattr(self, key, val)

config = Config(
    testing=True,
    seed = 2019,
    roberta_model_name='roberta-base', # can also be exchnaged with roberta-large 
    max_lr=1e-5,
    epochs=1,
    use_fp16=False,
    bs=4, 
    max_seq_len=256, 
    num_labels = 2,
    hidden_dropout_prob=.05,
    hidden_size=768, # 1024 for roberta-large
    start_tok = &quot;&lt;s&gt;&quot;,
    end_tok = &quot;&lt;/s&gt;&quot;,
)
</code></pre>
<pre><code>df = pd.read_csv(&quot;fastai_roberta/fastai_roberta_imdb/imdb_dataset.csv&quot;)
</code></pre>
<pre><code>if config.testing: df = df[:5000]
print(df.shape)
</code></pre>
<pre><code>(5000, 2)
</code></pre>
<pre><code>df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>One of the other reviewers has mentioned that ...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I thought this was a wonderful way to spend ti...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Basically there's a family where a little boy ...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
      <td>positive</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>feat_cols = &quot;review&quot;
label_cols = &quot;sentiment&quot;
</code></pre>
<h1 id="setting-up-the-tokenizer">Setting Up the Tokenizer</h1>
<pre><code>class FastAiRobertaTokenizer(BaseTokenizer):
    &quot;&quot;&quot;Wrapper around RobertaTokenizer to be compatible with fastai&quot;&quot;&quot;
    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): 
        self._pretrained_tokenizer = tokenizer
        self.max_seq_len = max_seq_len 
    def __call__(self, *args, **kwargs): 
        return self 
    def tokenizer(self, t:str) -&gt; List[str]: 
        &quot;&quot;&quot;Adds Roberta bos and eos tokens and limits the maximum sequence length&quot;&quot;&quot; 
        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]
</code></pre>
<pre><code># create fastai tokenizer for roberta
roberta_tok = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)

fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), 
                             pre_rules=[], post_rules=[])
</code></pre>
<pre><code># create fastai vocabulary for roberta
path = Path()
roberta_tok.save_vocabulary(path)

with open('vocab.json', 'r') as f:
    roberta_vocab_dict = json.load(f)

fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))
</code></pre>
<pre><code># Setting up pre-processors
class RobertaTokenizeProcessor(TokenizeProcessor):
    def __init__(self, tokenizer):
         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)

class RobertaNumericalizeProcessor(NumericalizeProcessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):
    &quot;&quot;&quot;
    Constructing preprocessors for Roberta
    We remove sos and eos tokens since we add that ourselves in the tokenizer.
    We also use a custom vocabulary to match the numericalization with the original Roberta model.
    &quot;&quot;&quot;
    return [RobertaTokenizeProcessor(tokenizer=tokenizer), RobertaNumericalizeProcessor(vocab=vocab)]
</code></pre>
<h1 id="setting-up-the-databunch">Setting up the DataBunch</h1>
<pre><code># Creating a Roberta specific DataBunch class
class RobertaDataBunch(TextDataBunch):
    &quot;Create a `TextDataBunch` suitable for training Roberta&quot;
    @classmethod
    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,
               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, 
               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -&gt; DataBunch:
        &quot;Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`&quot;
        datasets = cls._init_ds(train_ds, valid_ds, test_ds)
        val_bs = ifnone(val_bs, bs)
        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)
        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)
        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)
        dataloaders = [train_dl]
        for ds in datasets[1:]:
            lengths = [len(t) for t in ds.x.items]
            sampler = SortSampler(ds.x, key=lengths.__getitem__)
            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))
        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)
</code></pre>
<pre><code>class RobertaTextList(TextList):
    _bunch = RobertaDataBunch
    _label_cls = TextList
</code></pre>
<pre><code># loading the tokenizer and vocab processors
processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)
</code></pre>
<pre><code># creating our databunch 
data = RobertaTextList.from_df(df, &quot;.&quot;, cols=feat_cols, processor=processor) \
    .split_by_rand_pct(seed=config.seed) \
    .label_from_df(cols=label_cols,label_cls=CategoryList) \
    .databunch(bs=config.bs, pad_first=False, pad_idx=0)
</code></pre>
<pre><code>data
</code></pre>
<pre><code>RobertaDataBunch;

Train: LabelList (4000 items)
x: RobertaTextList
&lt;s&gt; One Ä of Ä the Ä other Ä reviewers Ä has Ä mentioned Ä that Ä after Ä watching Ä just Ä 1 Ä Oz Ä episode Ä you 'll Ä be Ä hooked . Ä They Ä are Ä right , Ä as Ä this Ä is Ä exactly Ä what Ä happened Ä with Ä me .&lt; br Ä / &gt;&lt; br Ä /&gt; The Ä first Ä thing Ä that Ä struck Ä me Ä about Ä Oz Ä was Ä its Ä brutality Ä and Ä unfl inch ing Ä scenes Ä of Ä violence , Ä which Ä set Ä in Ä right Ä from Ä the Ä word Ä GO . Ä Trust Ä me , Ä this Ä is Ä not Ä a Ä show Ä for Ä the Ä faint Ä heart ed Ä or Ä timid . Ä This Ä show Ä pulls Ä no Ä punches Ä with Ä regards Ä to Ä drugs , Ä sex Ä or Ä violence . Ä Its Ä is Ä hardcore , Ä in Ä the Ä classic Ä use Ä of Ä the Ä word .&lt; br Ä / &gt;&lt; br Ä /&gt; It Ä is Ä called Ä O Z Ä as Ä that Ä is Ä the Ä nickname Ä given Ä to Ä the Ä Oswald Ä Maximum Ä Security Ä State Ä Pen itent ary . Ä It Ä focuses Ä mainly Ä on Ä Emerald Ä City , Ä an Ä experimental Ä section Ä of Ä the Ä prison Ä where Ä all Ä the Ä cells Ä have Ä glass Ä fronts Ä and Ä face Ä in wards , Ä so Ä privacy Ä is Ä not Ä high Ä on Ä the Ä agenda . Ä Em Ä City Ä is Ä home Ä to Ä many .. A ry ans , Ä Muslims , Ä gang st as , Ä Latinos , Ä Christians , Ä Italians , Ä Irish Ä and Ä more .... so Ä sc uff les , Ä death Ä stares , Ä dod gy Ä dealings Ä and Ä shady Ä agreements Ä are Ä never Ä far Ä away .&lt; br Ä / &gt;&lt; br Ä /&gt; I Ä would Ä say Ä the Ä main Ä appeal Ä of Ä the Ä show Ä is Ä due Ä to Ä the Ä fact Ä that Ä it Ä goes Ä where Ä other Ä shows Ä wouldn 't Ä dare . Ä Forget Ä pretty Ä pictures Ä painted Ä for Ä mainstream Ä audiences , Ä forget Ä charm , Ä forget &lt;/s&gt;,&lt;s&gt; A Ä wonderful Ä little Ä production . Ä &lt; br Ä / &gt;&lt; br Ä /&gt; The Ä filming Ä technique Ä is Ä very Ä un assuming - Ä very Ä old - time - BBC Ä fashion Ä and Ä gives Ä a Ä comforting , Ä and Ä sometimes Ä discomfort ing , Ä sense Ä of Ä realism Ä to Ä the Ä entire Ä piece . Ä &lt; br Ä / &gt;&lt; br Ä /&gt; The Ä actors Ä are Ä extremely Ä well Ä chosen - Ä Michael Ä Sheen Ä not Ä only Ä " has Ä got Ä all Ä the Ä pol ari " Ä but Ä he Ä has Ä all Ä the Ä voices Ä down Ä pat Ä too ! Ä You Ä can Ä truly Ä see Ä the Ä seamless Ä editing Ä guided Ä by Ä the Ä references Ä to Ä Williams ' Ä diary Ä entries , Ä not Ä only Ä is Ä it Ä well Ä worth Ä the Ä watching Ä but Ä it Ä is Ä a Ä terrific ly Ä written Ä and Ä performed Ä piece . Ä A Ä master ful Ä production Ä about Ä one Ä of Ä the Ä great Ä master 's Ä of Ä comedy Ä and Ä his Ä life . Ä &lt; br Ä / &gt;&lt; br Ä /&gt; The Ä realism Ä really Ä comes Ä home Ä with Ä the Ä little Ä things : Ä the Ä fantasy Ä of Ä the Ä guard Ä which , Ä rather Ä than Ä use Ä the Ä traditional Ä ' dream ' Ä techniques Ä remains Ä solid Ä then Ä disappears . Ä It Ä plays Ä on Ä our Ä knowledge Ä and Ä our Ä senses , Ä particularly Ä with Ä the Ä scenes Ä concerning Ä Or ton Ä and Ä Hall i well Ä and Ä the Ä sets Ä ( particularly Ä of Ä their Ä flat Ä with Ä Hall i well 's Ä mur als Ä decor ating Ä every Ä surface ) Ä are Ä terribly Ä well Ä done . &lt;/s&gt;,&lt;s&gt; Basically Ä there 's Ä a Ä family Ä where Ä a Ä little Ä boy Ä ( Jake ) Ä thinks Ä there 's Ä a Ä zombie Ä in Ä his Ä closet Ä &amp; Ä his Ä parents Ä are Ä fighting Ä all Ä the Ä time .&lt; br Ä / &gt;&lt; br Ä /&gt; This Ä movie Ä is Ä slower Ä than Ä a Ä soap Ä opera ... Ä and Ä suddenly , Ä Jake Ä decides Ä to Ä become Ä Ram bo Ä and Ä kill Ä the Ä zombie .&lt; br Ä / &gt;&lt; br Ä /&gt; OK , Ä first Ä of Ä all Ä when Ä you 're Ä going Ä to Ä make Ä a Ä film Ä you Ä must Ä Dec ide Ä if Ä its Ä a Ä thriller Ä or Ä a Ä drama ! Ä As Ä a Ä drama Ä the Ä movie Ä is Ä watch able . Ä Parents Ä are Ä divor cing Ä &amp; Ä arguing Ä like Ä in Ä real Ä life . Ä And Ä then Ä we Ä have Ä Jake Ä with Ä his Ä closet Ä which Ä totally Ä ruins Ä all Ä the Ä film ! Ä I Ä expected Ä to Ä see Ä a Ä B OO GE Y MAN Ä similar Ä movie , Ä and Ä instead Ä i Ä watched Ä a Ä drama Ä with Ä some Ä meaningless Ä thriller Ä spots .&lt; br Ä / &gt;&lt; br Ä /&gt; 3 Ä out Ä of Ä 10 Ä just Ä for Ä the Ä well Ä playing Ä parents Ä &amp; Ä descent Ä dialog s . Ä As Ä for Ä the Ä shots Ä with Ä Jake : Ä just Ä ignore Ä them . &lt;/s&gt;,&lt;s&gt; Pet ter Ä Matte i 's Ä " Love Ä in Ä the Ä Time Ä of Ä Money " Ä is Ä a Ä visually Ä stunning Ä film Ä to Ä watch . Ä Mr . Ä Matte i Ä offers Ä us Ä a Ä vivid Ä portrait Ä about Ä human Ä relations . Ä This Ä is Ä a Ä movie Ä that Ä seems Ä to Ä be Ä telling Ä us Ä what Ä money , Ä power Ä and Ä success Ä do Ä to Ä people Ä in Ä the Ä different Ä situations Ä we Ä encounter . Ä &lt; br Ä / &gt;&lt; br Ä /&gt; This Ä being Ä a Ä variation Ä on Ä the Ä Arthur Ä Schn itz ler 's Ä play Ä about Ä the Ä same Ä theme , Ä the Ä director Ä transfers Ä the Ä action Ä to Ä the Ä present Ä time Ä New Ä York Ä where Ä all Ä these Ä different Ä characters Ä meet Ä and Ä connect . Ä Each Ä one Ä is Ä connected Ä in Ä one Ä way , Ä or Ä another Ä to Ä the Ä next Ä person , Ä but Ä no Ä one Ä seems Ä to Ä know Ä the Ä previous Ä point Ä of Ä contact . Ä Sty lish ly , Ä the Ä film Ä has Ä a Ä sophisticated Ä luxurious Ä look . Ä We Ä are Ä taken Ä to Ä see Ä how Ä these Ä people Ä live Ä and Ä the Ä world Ä they Ä live Ä in Ä their Ä own Ä habitat .&lt; br Ä / &gt;&lt; br Ä /&gt; The Ä only Ä thing Ä one Ä gets Ä out Ä of Ä all Ä these Ä souls Ä in Ä the Ä picture Ä is Ä the Ä different Ä stages Ä of Ä loneliness Ä each Ä one Ä inhab its . Ä A Ä big Ä city Ä is Ä not Ä exactly Ä the Ä best Ä place Ä in Ä which Ä human Ä relations Ä find Ä sincere Ä fulfillment , Ä as Ä one Ä discern s Ä is Ä the Ä case Ä with Ä most Ä of Ä the Ä people Ä we Ä encounter .&lt; br Ä / &gt;&lt; br Ä /&gt; The Ä acting Ä is Ä good Ä under Ä Mr . Ä Matte i 's Ä direction . Ä Steve Ä Bus ce mi , Ä Ros ario Ä Dawson , Ä Carol Ä Kane , Ä Michael Ä Imper iol &lt;/s&gt;,&lt;s&gt; Probably Ä my Ä all - time Ä favorite Ä movie , Ä a Ä story Ä of Ä self lessness , Ä sacrifice Ä and Ä dedication Ä to Ä a Ä noble Ä cause , Ä but Ä it 's Ä not Ä preach y Ä or Ä boring . Ä It Ä just Ä never Ä gets Ä old , Ä despite Ä my Ä having Ä seen Ä it Ä some Ä 15 Ä or Ä more Ä times Ä in Ä the Ä last Ä 25 Ä years . Ä Paul Ä Luk as ' Ä performance Ä brings Ä tears Ä to Ä my Ä eyes , Ä and Ä Bet te Ä Davis , Ä in Ä one Ä of Ä her Ä very Ä few Ä truly Ä sympathetic Ä roles , Ä is Ä a Ä delight . Ä The Ä kids Ä are , Ä as Ä grandma Ä says , Ä more Ä like Ä " d ressed - up Ä mid gets " Ä than Ä children , Ä but Ä that Ä only Ä makes Ä them Ä more Ä fun Ä to Ä watch . Ä And Ä the Ä mother 's Ä slow Ä awakening Ä to Ä what 's Ä happening Ä in Ä the Ä world Ä and Ä under Ä her Ä own Ä roof Ä is Ä believable Ä and Ä startling . Ä If Ä I Ä had Ä a Ä dozen Ä thumbs , Ä they 'd Ä all Ä be Ä " up " Ä for Ä this Ä movie . &lt;/s&gt;
y: CategoryList
positive,positive,negative,positive,positive
Path: .;

Valid: LabelList (1000 items)
x: RobertaTextList
&lt;s&gt; Apparently , Ä The Ä Mut ilation Ä Man Ä is Ä about Ä a Ä guy Ä who Ä wand ers Ä the Ä land Ä performing Ä shows Ä of Ä self - mut ilation Ä as Ä a Ä way Ä of Ä coping Ä with Ä his Ä abusive Ä childhood . Ä I Ä use Ä the Ä word Ä ' app arently ' Ä because Ä without Ä listening Ä to Ä a Ä director Ä Andy Ä Co pp 's Ä commentary Ä ( which Ä I Ä didn 't Ä have Ä available Ä to Ä me ) Ä or Ä reading Ä up Ä on Ä the Ä film Ä prior Ä to Ä watching , Ä viewers Ä won 't Ä have Ä a Ä clue Ä what Ä it Ä is Ä about .&lt; br Ä / &gt;&lt; br Ä /&gt; G ore h ounds Ä and Ä fans Ä of Ä extreme Ä movies Ä may Ä be Ä lured Ä into Ä watching Ä The Ä Mut ilation Ä Man Ä with Ä the Ä promise Ä of Ä some Ä harsh Ä scenes Ä of Ä spl atter Ä and Ä unsettling Ä real - life Ä footage , Ä but Ä unless Ä they 're Ä also Ä fond Ä of Ä pret entious , Ä headache - inducing , Ä experimental Ä art - house Ä cinema , Ä they 'll Ä find Ä this Ä one Ä a Ä real Ä chore Ä to Ä sit Ä through .&lt; br Ä / &gt;&lt; br Ä /&gt; 82 Ä minutes Ä of Ä ugly Ä imagery Ä accompanied Ä by Ä dis - ch ord ant Ä sound , Ä terrible Ä music Ä and Ä incomprehensible Ä dialogue , Ä this Ä mind - n umb ingly Ä awful Ä dri vel Ä is Ä the Ä perfect Ä way Ä to Ä test Ä one 's Ä sanity : Ä if Ä you 've Ä still Ä got Ä all Ä your Ä mar bles , Ä you 'll Ä switch Ä this Ä rubbish Ä off Ä and Ä watch Ä something Ä decent Ä instead Ä ( I Ä watched Ä the Ä whole Ä thing , Ä but Ä am Ä well Ä aware Ä that Ä I 'm Ä completely Ä barking !). &lt;/s&gt;,&lt;s&gt; Peter Ä C ushing Ä and Ä Donald Ä Ple as ance Ä are Ä legendary Ä actors , Ä and Ä director Ä K ost as Ä Kar ag ian nis Ä was Ä the Ä man Ä behind Ä the Ä successful Ä Greek Ä Gi allo - es quire Ä thriller Ä Death Ä Kiss Ä in Ä 1974 ; Ä and Ä yet Ä when Ä you Ä combine Ä the Ä three Ä talents , Ä all Ä you Ä get Ä is Ä this Ä complete Ä load Ä of Ä dri vel ! Ä God Ä only Ä knows Ä what Ä drove Ä the Ä likes Ä of Ä Peter Ä C ushing Ä and Ä Donald Ä Ple as ance Ä to Ä star Ä in Ä this Ä cheap ie Ä devil Ä worship Ä flick , Ä but Ä I Ä really Ä do Ä hope Ä they Ä were Ä well Ä paid Ä as Ä neither Ä one Ä deserves Ä something Ä as Ä amateur ish Ä as Ä this Ä on Ä their Ä resumes . Ä The Ä story Ä focuses Ä on Ä a Ä group Ä of Ä devil Ä worsh ippers Ä that Ä kidnap Ä some Ä kids , Ä leading Ä another Ä group Ä to Ä go Ä after Ä them . Ä The Ä pace Ä of Ä the Ä plot Ä is Ä very Ä slow Ä and Ä this Ä ensures Ä that Ä the Ä film Ä is Ä very Ä boring . Ä The Ä plot Ä is Ä also Ä a Ä long Ä way Ä from Ä being Ä original Ä and Ä anyone Ä with Ä even Ä a Ä passing Ä interest Ä in Ä the Ä horror Ä genre Ä will Ä have Ä seen Ä something Ä a Ä bit Ä like Ä this , Ä and Ä no Ä doubt Ä done Ä much Ä better . Ä The Ä obvious Ä lack Ä of Ä budget Ä is Ä felt Ä throughout Ä and Ä the Ä film Ä doesn 't Ä manage Ä to Ä overcome Ä this Ä at Ä any Ä point . Ä This Ä really Ä is Ä a Ä depressing Ä and Ä miserable Ä watch Ä and Ä not Ä even Ä a Ä slightly Ä decent Ä ending Ä manages Ä to Ä up Ä the Ä ante Ä enough Ä to Ä lift Ä this Ä film Ä out Ä of Ä the Ä very Ä bottom Ä of Ä the Ä barrel . Ä Extreme ly Ä poor Ä stuff Ä and Ä definitely Ä not Ä recommended ! &lt;/s&gt;,&lt;s&gt; Back Ä in Ä the Ä 1970 s , Ä WP IX Ä ran Ä " The Ä Adventures Ä of Ä Superman " Ä every Ä weekday Ä afternoon Ä for Ä quite Ä a Ä few Ä years . Ä Every Ä once Ä in Ä a Ä while , Ä we 'd Ä get Ä a Ä treat Ä when Ä they Ä would Ä preempt Ä neighboring Ä shows Ä to Ä air Ä " Super man Ä and Ä the Ä Mole Ä Men ." Ä I Ä always Ä looked Ä forward Ä to Ä those Ä days . Ä Watching Ä it Ä recently , Ä I Ä was Ä surprised Ä at Ä just Ä how Ä bad Ä it Ä really Ä was .&lt; br Ä / &gt;&lt; br Ä /&gt; It Ä wasn 't Ä bad Ä because Ä of Ä the Ä special Ä effects , Ä or Ä lack Ä thereof . Ä True , Ä George Ä Reeves ' Ä Superman Ä costume Ä was Ä pretty Ä bad , Ä the Ä edges Ä of Ä the Ä foam Ä padding Ä used Ä to Ä make Ä him Ä look Ä more Ä imposing Ä being Ä plainly Ä visible . Ä And Ä true , Ä the Ä Mole Ä Men 's Ä costumes Ä were Ä even Ä worse . Ä What Ä was Ä supposed Ä to Ä be Ä a Ä furry Ä covering Ä wouldn 't Ä have Ä fooled Ä a Ä ten Ä year - old , Ä since Ä the Ä z ippers , Ä sleeve Ä he ms Ä and Ä badly Ä p illing Ä fabric Ä badly Ä tailored Ä into Ä bag gy Ä costumes Ä were Ä all Ä painfully Ä obvious . Ä But Ä these Ä were Ä forg ivable Ä shortcomings .&lt; br Ä / &gt;&lt; br Ä /&gt; No , Ä what Ä made Ä it Ä bad Ä were Ä the Ä cont rived Ä plot Ä devices . Ä Time Ä and Ä again , Ä Superman Ä failed Ä to Ä do Ä anything Ä to Ä keep Ä the Ä situation Ä from Ä deteriorating . Ä A Ä lyn ch Ä mob Ä is Ä searching Ä for Ä the Ä creatures ? Ä Rather Ä than Ä round Ä up Ä the Ä hysterical Ä crowd Ä or Ä search Ä for Ä the Ä creatures Ä himself , Ä he Ä stands Ä around Ä explaining Ä the Ä dangers Ä of Ä the Ä situation Ä to Ä Lois Ä and Ä the Ä PR &lt;/s&gt;,&lt;s&gt; Sat an 's Ä Little Ä Hel per Ä is Ä one Ä of Ä the Ä better Ä B Ä Horror Ä movies Ä I Ä have Ä seen . Ä When Ä I Ä say Ä better Ä I Ä mean Ä the Ä story . Ä The Ä film Ä hat ches Ä a Ä new Ä plot , Ä something Ä that 's Ä not Ä so Ä clichÃƒÂ© Ä in Ä the Ä Horror Ä genre Ä - Ä something Ä fresh . Ä But Ä there Ä are Ä also Ä some Ä ridiculous Ä questions Ä that Ä come Ä along Ä with Ä it . Ä Questions Ä you Ä will Ä be Ä asking Ä yourself Ä throughout Ä the Ä movie .&lt; br Ä / &gt;&lt; br Ä /&gt; The Ä film Ä first Ä caught Ä my Ä attention Ä while Ä I Ä was Ä cruising Ä the Ä Horror Ä section Ä in Ä HM V . Ä I Ä was Ä tired Ä of Ä all Ä the Ä so Ä called Ä " ter r ifi ying " Ä Hollywood Ä block busters Ä and Ä wanted Ä something Ä different . Ä The Ä cover Ä art Ä for Ä Satan 's Ä Little Ä Hel per Ä immediately Ä caught Ä my Ä attention . Ä As Ä you Ä can Ä see , Ä the Ä image Ä draws Ä you Ä in Ä - Ä it 's Ä chilling ! Ä I Ä knew Ä it Ä was Ä a Ä straight Ä to Ä DVD Ä release Ä - Ä but Ä I Ä took Ä a Ä chance . Ä I Ä mean , Ä I Ä just Ä seen Ä " Boo gey Ä Man " Ä the Ä night Ä before Ä - Ä so Ä It Ä couldn 't Ä get Ä any Ä worse ! Ä After Ä I Ä watched Ä the Ä movie , Ä I Ä was Ä semi - s atisf ied . Ä I Ä loved Ä the Ä plot Ä of Ä the Ä movie . Ä It Ä was Ä really Ä creepy Ä how Ä the Ä killer Ä was Ä pretending Ä to Ä be Ä the Ä little Ä boys Ä friend , Ä so Ä he Ä could Ä kill . Ä In Ä some Ä sick Ä der anged Ä way , Ä he Ä actually Ä thought Ä he Ä and Ä the Ä little Ä boy Ä would Ä become Ä partners Ä - Ä a Ä duo Ä of Ä terror . Ä It Ä was Ä a &lt;/s&gt;,&lt;s&gt; I Ä saw Ä this Ä gem Ä of Ä a Ä film Ä at Ä Cannes Ä where Ä it Ä was Ä part Ä of Ä the Ä directors Ä fortnight .&lt; br Ä / &gt;&lt; br Ä /&gt; Welcome Ä to Ä Coll in wood Ä is Ä nothing Ä short Ä of Ä superb . Ä Great Ä fun Ä throughout , Ä with Ä all Ä members Ä of Ä a Ä strong Ä cast Ä acting Ä their Ä socks Ä off . Ä It 's Ä a Ä sometimes Ä laugh Ä out Ä loud Ä comedy Ä about Ä a Ä petty Ä cro ok Ä ( Cos imo , Ä played Ä by Ä Luis Ä Gu zman ) Ä who Ä gets Ä caught Ä trying Ä to Ä steal Ä a Ä car Ä and Ä sent Ä to Ä prison . Ä While Ä in Ä prison Ä he Ä meets Ä a Ä ` l ifer ' Ä who Ä tells Ä him Ä of Ä ` the Ä ultimate Ä bell ini ' Ä Ã‚ Ä¸ Ä which Ä to Ä you Ä and Ä me Ä Ã‚ Ä¸ Ä is Ä a Ä sure - fire Ä get Ä rich Ä quick Ä scheme . Ä It Ä turns Ä out Ä that Ä there Ä is Ä a Ä way Ä through Ä from Ä a Ä deserted Ä building Ä into Ä the Ä towns Ä jew ell ers Ä shop Ä Ã‚ Ä¸ Ä which Ä could Ä net Ä millions . Ä Sounds Ä simple ? Ä Ã‚ Ä¸ Ä well Ä throw Ä in Ä all Ä kinds Ä of Ä w acky Ä characters Ä and Ä incidents Ä along Ä the Ä way Ä and Ä you Ä have Ä got Ä the Ä ingredients Ä for Ä a Ä one Ä wild Ä ride !! Ä Ã‚ Ä¸ Ä word Ä passes Ä from Ä one Ä low Ä life Ä loser Ä to Ä the Ä next Ä and Ä soon Ä a Ä team Ä of Ä them Ä are Ä assembled Ä to Ä try Ä and Ä cash Ä in Ä on Ä Cos im os Ä ` bell ini ' Ä lead Ä by Ä failed Ä boxer Ä Per o Ä ( Super bly Ä played Ä by Ä Sam Ä Rock well Ä Ã‚ Ä¸ Ä surely Ä a Ä star Ä in Ä the Ä making ) Ä and Ä reluctant Ä cro ok Ä Riley Ä ( William Ä H . Ä Macy ) Ä who Ä is Ä forced Ä to &lt;/s&gt;
y: CategoryList
negative,negative,negative,negative,positive
Path: .;

Test: None
</code></pre>
<h1 id="building-the-model">Building the Model</h1>
<pre><code>import torch
import torch.nn as nn
from transformers import RobertaModel

# defining our model architecture 
class CustomRobertaModel(nn.Module):
    def __init__(self,num_labels=2):
        super(CustomRobertaModel,self).__init__()
        self.num_labels = num_labels
        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):
        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # 
        logits = self.classifier(pooled_output)        
        return logits
</code></pre>
<pre><code>roberta_model = CustomRobertaModel(num_labels=config.num_labels)

learn = Learner(data, roberta_model, metrics=[accuracy])
</code></pre>
<pre><code>learn.model.roberta.train() # setting roberta to train as it is in eval mode by default
learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)
</code></pre>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.184614</td>
      <td>0.180552</td>
      <td>0.929000</td>
      <td>02:17</td>
    </tr>
  </tbody>
</table>

<h1 id="getting-predictions">Getting Predictions</h1>
<pre><code>def get_preds_as_nparray(ds_type) -&gt; np.ndarray:
    learn.model.roberta.eval()
    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()
    sampler = [i for i in data.dl(ds_type).sampler]
    reverse_sampler = np.argsort(sampler)
    ordered_preds = preds[reverse_sampler, :]
    pred_values = np.argmax(ordered_preds, axis=1)
    return ordered_preds, pred_values
</code></pre>
<pre><code>preds, pred_values = get_preds_as_nparray(DatasetType.Valid)
</code></pre>
<pre><code># accuracy on valid
(pred_values == data.valid_ds.y.items).mean()
</code></pre>
<pre><code>0.929
</code></pre>
<h1 id="savingloading-the-model-weights">Saving/Loading the model weights</h1>
<pre><code>def save_model(learner, file_name):
    st = learner.model.state_dict()
    torch.save(st, file_name) # will save model in current dir # backend is pickle 

def load_model(learner, file_name):
    st = torch.load(file_name)
    learner.model.load_state_dict(st)
</code></pre>
<pre><code># monkey patching Learner methods to save and load model file
Learner.save_model = save_model
Learner.load_model = load_model
</code></pre>
<pre><code>learn.save_model(&quot;my_model.bin&quot;)
learn.load_model(&quot;my_model.bin&quot;)
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../TAPAS_fine_tuning_in_tf/" class="btn btn-neutral float-left" title="TAPAS fine tuning in tf"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Wikipedia_answer_retrieval_DPR/" class="btn btn-neutral float-right" title="Wikipedia answer retrieval DPR">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../TAPAS_fine_tuning_in_tf/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Wikipedia_answer_retrieval_DPR/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
